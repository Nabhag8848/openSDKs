/**
 * This file was auto-generated by openapi-typescript.
 * Do not make direct changes to the file.
 */

/** OneOf type helpers */
type Without<T, U> = {[P in Exclude<keyof T, keyof U>]?: never}
type XOR<T, U> = T | U extends object
  ? (Without<T, U> & U) | (Without<U, T> & T)
  : T | U
type OneOf<T extends any[]> = T extends [infer Only]
  ? Only
  : T extends [infer A, infer B, ...infer Rest]
    ? OneOf<[XOR<A, B>, ...Rest]>
    : never

export interface paths {
  '/destinations': {
    /** List destinations */
    get: operations['listDestinations']
    /**
     * Create a destination
     * @description Creates a destination given a name, workspace id, and a json blob containing the configuration for the source.
     */
    post: operations['createDestination']
  }
  '/destinations/{destinationId}': {
    /** Get Destination details */
    get: operations['getDestination']
    /** Update a Destination and fully overwrite it */
    put: operations['putDestination']
    /** Delete a Destination */
    delete: operations['deleteDestination']
    /** Update a Destination */
    patch: operations['patchDestination']
    parameters: {
      path: {
        destinationId: string
      }
    }
  }
}

export type webhooks = Record<string, never>

export interface components {
  schemas: {
    /**
     * Root Type for RedirectUrlResponse
     * @example {
     *   "redirectUrl": "https://airbyte.portal.speakeasyapi.dev?speakeasyAccessToken=eydas.ad45.1234"
     * }
     */
    RedirectUrlResponse: {
      /** Format: url */
      redirectUrl?: string
    }
    /**
     * Root Type for JobResponse
     * @description Provides details of a single job.
     * @example {
     *   "id": "18dccc91-0ab1-4f72-9ed7-0b8fc27c5826",
     *   "status": "running",
     *   "jobType": "sync",
     *   "startTime": "2023-03-25T01:30:50Z",
     *   "duration": "PT8H6M12S"
     * }
     */
    JobResponse: {
      /** Format: int64 */
      jobId: number
      status: components['schemas']['JobStatusEnum']
      jobType: components['schemas']['JobTypeEnum']
      startTime: string
      /** Format: UUID */
      connectionId: string
      lastUpdatedAt?: string
      /** @description Duration of a sync in ISO_8601 format */
      duration?: string
      /** Format: int64 */
      bytesSynced?: number
      /** Format: int64 */
      rowsSynced?: number
    }
    /**
     * Root Type for JobsResponse
     * @example {
     *   "next": "https://api.airbyte.com/v1/jobs?limit=5&offset=10",
     *   "previous": "https://api.airbyte.com/v1/jobs?limit=5&offset=0",
     *   "data": [
     *     {
     *       "id": "18dccc91-0ab1-4f72-9ed7-0b8fc27c5826",
     *       "status": "running",
     *       "jobType": "sync",
     *       "startTime": "2023-03-25T01:30:50Z"
     *     }
     *   ]
     * }
     */
    JobsResponse: {
      previous?: string
      next?: string
      data: components['schemas']['JobResponse'][]
    }
    ConnectionCreateRequest: {
      /** @description Optional name of the connection */
      name?: string
      /** Format: uuid */
      sourceId: string
      /** Format: uuid */
      destinationId: string
      configurations?: components['schemas']['StreamConfigurations']
      schedule?: components['schemas']['ConnectionSchedule']
      dataResidency?: components['schemas']['GeographyEnum']
      namespaceDefinition?: components['schemas']['NamespaceDefinitionEnum']
      /**
       * @description Used when namespaceDefinition is 'custom_format'. If blank then behaves like namespaceDefinition = 'destination'. If "${SOURCE_NAMESPACE}" then behaves like namespaceDefinition = 'source'.
       * @default null
       * @example ${SOURCE_NAMESPACE}
       */
      namespaceFormat?: string
      /** @description Prefix that will be prepended to the name of each stream when it is written to the destination (ex. “airbyte_” causes “projects” => “airbyte_projects”). */
      prefix?: string
      nonBreakingSchemaUpdatesBehavior?: components['schemas']['NonBreakingSchemaUpdatesBehaviorEnum']
      status?: components['schemas']['ConnectionStatusEnum']
    }
    ConnectionPatchRequest: {
      /** @description Optional name of the connection */
      name?: string
      configurations?: components['schemas']['StreamConfigurations']
      schedule?: components['schemas']['ConnectionSchedule']
      dataResidency?: components['schemas']['GeographyEnumNoDefault']
      namespaceDefinition?: components['schemas']['NamespaceDefinitionEnumNoDefault']
      /**
       * @description Used when namespaceDefinition is 'custom_format'. If blank then behaves like namespaceDefinition = 'destination'. If "${SOURCE_NAMESPACE}" then behaves like namespaceDefinition = 'source'.
       * @default null
       * @example ${SOURCE_NAMESPACE}
       */
      namespaceFormat?: string
      /** @description Prefix that will be prepended to the name of each stream when it is written to the destination (ex. “airbyte_” causes “projects” => “airbyte_projects”). */
      prefix?: string
      nonBreakingSchemaUpdatesBehavior?: components['schemas']['NonBreakingSchemaUpdatesBehaviorEnumNoDefault']
      status?: components['schemas']['ConnectionStatusEnum']
    }
    /**
     * Root Type for JobCreate
     * @description Creates a new Job from the configuration provided in the request body.
     * @example {
     *   "connectionId": "18dccc91-0ab1-4f72-9ed7-0b8fc27c5826",
     *   "jobType": "sync"
     * }
     */
    JobCreateRequest: {
      /** Format: UUID */
      connectionId: string
      jobType: components['schemas']['JobTypeEnum']
    }
    /** @enum {string} */
    JobStatusEnum:
      | 'pending'
      | 'running'
      | 'incomplete'
      | 'failed'
      | 'succeeded'
      | 'cancelled'
    /**
     * @description Enum that describes the different types of jobs that the platform runs.
     * @enum {string}
     */
    JobTypeEnum: 'sync' | 'reset'
    SourceCreateRequest: {
      /** @description Name of the source e.g. dev-mysql-instance. */
      name: string
      /**
       * Format: uuid
       * @description The UUID of the connector definition. One of configuration.sourceType or definitionId must be provided.
       */
      definitionId?: string
      /** Format: uuid */
      workspaceId: string
      configuration: components['schemas']['SourceConfiguration']
      /** @description Optional secretID obtained through the public API OAuth redirect flow. */
      secretId?: string
    }
    SourcePutRequest: {
      name: string
      configuration: components['schemas']['SourceConfiguration']
    }
    SourcePatchRequest: {
      /** @example My source */
      name?: string
      /** Format: uuid */
      workspaceId?: string
      configuration?: components['schemas']['SourceConfiguration']
      /** @description Optional secretID obtained through the public API OAuth redirect flow. */
      secretId?: string
    }
    /**
     * @description Arbitrary vars to pass for OAuth depending on what the source/destination spec requires.
     * @example {
     *   "host": "test.snowflake.com"
     * }
     */
    OAuthInputConfiguration: Record<string, never>
    /**
     * Root Type for ConnectionResponse
     * @description Provides details of a single connection.
     */
    ConnectionResponse: {
      /** Format: UUID */
      connectionId: string
      name: string
      /** Format: UUID */
      sourceId: string
      /** Format: UUID */
      destinationId: string
      /** Format: UUID */
      workspaceId: string
      status: components['schemas']['ConnectionStatusEnum']
      schedule: components['schemas']['ConnectionScheduleResponse']
      dataResidency: components['schemas']['GeographyEnum']
      nonBreakingSchemaUpdatesBehavior?: components['schemas']['NonBreakingSchemaUpdatesBehaviorEnum']
      namespaceDefinition?: components['schemas']['NamespaceDefinitionEnum']
      namespaceFormat?: string
      prefix?: string
      configurations: components['schemas']['StreamConfigurations']
    }
    /** @description schedule for when the the connection should run, per the schedule type */
    ConnectionSchedule: {
      scheduleType: components['schemas']['ScheduleTypeEnum']
      cronExpression?: string
    }
    /** @enum {string} */
    ScheduleTypeEnum: 'manual' | 'cron'
    /** @description schedule for when the the connection should run, per the schedule type */
    ConnectionScheduleResponse: {
      scheduleType: components['schemas']['ScheduleTypeWithBasicEnum']
      cronExpression?: string
      basicTiming?: string
    }
    /** @enum {string} */
    ScheduleTypeWithBasicEnum: 'manual' | 'cron' | 'basic'
    /**
     * @default auto
     * @enum {string}
     */
    GeographyEnum: 'auto' | 'us' | 'eu'
    /** @enum {string} */
    GeographyEnumNoDefault: 'auto' | 'us' | 'eu'
    /** @enum {string} */
    ConnectionStatusEnum: 'active' | 'inactive' | 'deprecated'
    /**
     * @description Define the location where the data will be stored in the destination
     * @default destination
     * @enum {string}
     */
    NamespaceDefinitionEnum: 'source' | 'destination' | 'custom_format'
    /**
     * @description Set how Airbyte handles syncs when it detects a non-breaking schema change in the source
     * @default ignore
     * @enum {string}
     */
    NonBreakingSchemaUpdatesBehaviorEnum:
      | 'ignore'
      | 'disable_connection'
      | 'propagate_columns'
      | 'propagate_fully'
    /**
     * @description Define the location where the data will be stored in the destination
     * @enum {string}
     */
    NamespaceDefinitionEnumNoDefault: 'source' | 'destination' | 'custom_format'
    /**
     * @description Set how Airbyte handles syncs when it detects a non-breaking schema change in the source
     * @enum {string}
     */
    NonBreakingSchemaUpdatesBehaviorEnumNoDefault:
      | 'ignore'
      | 'disable_connection'
      | 'propagate_columns'
      | 'propagate_fully'
    /**
     * Root Type for DestinationResponse
     * @description Provides details of a single destination.
     * @example {
     *   "destinationId": "18dccc91-0ab1-4f72-9ed7-0b8fc27c5826",
     *   "name": "Analytics Team Postgres",
     *   "destinationType": "postgres",
     *   "workspaceId": "871d9b60-11d1-44cb-8c92-c246d53bf87e"
     * }
     */
    DestinationResponse: {
      /** Format: UUID */
      destinationId: string
      name: string
      destinationType: string
      /** Format: UUID */
      workspaceId: string
      configuration: components['schemas']['DestinationConfiguration']
    }
    /**
     * Root Type for SourceResponse
     * @description Provides details of a single source.
     * @example {
     *   "sourceId": "18dccc91-0ab1-4f72-9ed7-0b8fc27c5826",
     *   "name": "Analytics Team Postgres",
     *   "sourceType": "postgres",
     *   "workspaceId": "871d9b60-11d1-44cb-8c92-c246d53bf87e"
     * }
     */
    SourceResponse: {
      /** Format: UUID */
      sourceId: string
      name: string
      sourceType: string
      /** Format: UUID */
      workspaceId: string
      configuration: components['schemas']['SourceConfiguration']
    }
    DestinationCreateRequest: {
      /** @description Name of the destination e.g. dev-mysql-instance. */
      name: string
      /**
       * Format: uuid
       * @description The UUID of the connector definition. One of configuration.destinationType or definitionId must be provided.
       */
      definitionId?: string
      /** Format: uuid */
      workspaceId: string
      configuration: components['schemas']['DestinationConfiguration']
    }
    DestinationPatchRequest: {
      name?: string
      configuration?: components['schemas']['DestinationConfiguration']
    }
    DestinationPutRequest: {
      name: string
      configuration: components['schemas']['DestinationConfiguration']
    }
    WorkspaceCreateRequest: {
      /** @description Name of the workspace */
      name: string
    }
    WorkspaceUpdateRequest: {
      /** @description Name of the workspace */
      name: string
    }
    /**
     * Root Type for WorkspaceResponse
     * @description Provides details of a single workspace.
     */
    WorkspaceResponse: {
      /** Format: UUID */
      workspaceId: string
      name: string
      dataResidency: components['schemas']['GeographyEnum']
    }
    /**
     * Root Type for ConnectionsResponse
     * @example {
     *   "next": "https://api.airbyte.com/v1/connections?limit=5&offset=10",
     *   "previous": "https://api.airbyte.com/v1/connections?limit=5&offset=0",
     *   "data": [
     *     {
     *       "name": "test-connection"
     *     },
     *     {
     *       "connection_id": "18dccc91-0ab1-4f72-9ed7-0b8fc27c5826"
     *     },
     *     {
     *       "sourceId": "49237019-645d-47d4-b45b-5eddf97775ce"
     *     },
     *     {
     *       "destinationId": "al312fs-0ab1-4f72-9ed7-0b8fc27c5826"
     *     },
     *     {
     *       "schedule": {
     *         "scheduleType": "manual"
     *       }
     *     },
     *     {
     *       "status": "active"
     *     },
     *     {
     *       "dataResidency": "auto"
     *     }
     *   ]
     * }
     */
    ConnectionsResponse: {
      previous?: string
      next?: string
      /** @default [] */
      data: components['schemas']['ConnectionResponse'][]
    }
    /**
     * Root Type for SourcesResponse
     * @example {
     *   "next": "https://api.airbyte.com/v1/sources?limit=5&offset=10",
     *   "previous": "https://api.airbyte.com/v1/sources?limit=5&offset=0",
     *   "data": {
     *     "sourceId": "18dccc91-0ab1-4f72-9ed7-0b8fc27c5826",
     *     "name": "Analytics Team Postgres",
     *     "sourceType": "postgres",
     *     "workspaceId": "871d9b60-11d1-44cb-8c92-c246d53bf87e"
     *   }
     * }
     */
    SourcesResponse: {
      previous?: string
      next?: string
      data: components['schemas']['SourceResponse'][]
    }
    /**
     * Root Type for DestinationsResponse
     * @example {
     *   "next": "https://api.airbyte.com/v1/destinations?limit=5&offset=10",
     *   "previous": "https://api.airbyte.com/v1/destinations?limit=5&offset=0",
     *   "data": {
     *     "destinationId": "18dccc91-0ab1-4f72-9ed7-0b8fc27c5826",
     *     "name": "Analytics Team Postgres",
     *     "destinationType": "postgres",
     *     "workspaceId": "871d9b60-11d1-44cb-8c92-c246d53bf87e"
     *   }
     * }
     */
    DestinationsResponse: {
      previous?: string
      next?: string
      data: components['schemas']['DestinationResponse'][]
    }
    /**
     * Root Type for WorkspacesResponse
     * @example {
     *   "next": "https://api.airbyte.com/v1/workspaces?limit=5&offset=10",
     *   "previous": "https://api.airbyte.com/v1/workspaces?limit=5&offset=0",
     *   "data": {
     *     "workspaceId": "18dccc91-0ab1-4f72-9ed7-0b8fc27c5826",
     *     "name": "Acme Company",
     *     "dataResidency": "auto"
     *   }
     * }
     */
    WorkspacesResponse: {
      previous?: string
      next?: string
      data: components['schemas']['WorkspaceResponse'][]
    }
    /** @description Configurations for a single stream. */
    StreamConfiguration: {
      name: string
      syncMode?: components['schemas']['ConnectionSyncModeEnum']
      /** @description Path to the field that will be used to determine if a record is new or modified since the last sync. This field is REQUIRED if `sync_mode` is `incremental` unless there is a default. */
      cursorField?: string[]
      /** @description Paths to the fields that will be used as primary key. This field is REQUIRED if `destination_sync_mode` is `*_dedup` unless it is already supplied by the source schema. */
      primaryKey?: string[][]
    }
    /** @description A list of configured stream options for a connection. */
    StreamConfigurations: {
      streams?: components['schemas']['StreamConfiguration'][]
    }
    /** @description A list of stream properties. */
    StreamPropertiesResponse: {
      streams?: components['schemas']['StreamProperties'][]
    }
    /** @description The stream properties associated with a connection. */
    StreamProperties: {
      streamName?: string
      syncModes?: components['schemas']['ConnectionSyncModeEnum'][]
      defaultCursorField?: string[]
      sourceDefinedCursorField?: boolean
      sourceDefinedPrimaryKey?: string[][]
      propertyFields?: string[][]
    }
    /** @enum {unknown} */
    ConnectionSyncModeEnum:
      | 'full_refresh_overwrite'
      | 'full_refresh_append'
      | 'incremental_append'
      | 'incremental_deduped_history'
    /**
     * @description Whether you're setting this override for a source or destination
     * @enum {unknown}
     */
    ActorTypeEnum: 'source' | 'destination'
    /** GCS Destination Spec */
    'destination-gcs': {
      /**
       * GCS Bucket Name
       * @description You can find the bucket name in the App Engine Admin console Application Settings page, under the label Google Cloud Storage Bucket. Read more <a href="https://cloud.google.com/storage/docs/naming-buckets">here</a>.
       */
      gcs_bucket_name: string
      /**
       * GCS Bucket Path
       * @description GCS Bucket Path string Subdirectory under the above bucket to sync the data into.
       */
      gcs_bucket_path: string
      /**
       * GCS Bucket Region
       * @description Select a Region of the GCS Bucket. Read more <a href="https://cloud.google.com/storage/docs/locations">here</a>.
       * @default us
       * @enum {string}
       */
      gcs_bucket_region?:
        | 'northamerica-northeast1'
        | 'northamerica-northeast2'
        | 'us-central1'
        | 'us-east1'
        | 'us-east4'
        | 'us-west1'
        | 'us-west2'
        | 'us-west3'
        | 'us-west4'
        | 'southamerica-east1'
        | 'southamerica-west1'
        | 'europe-central2'
        | 'europe-north1'
        | 'europe-west1'
        | 'europe-west2'
        | 'europe-west3'
        | 'europe-west4'
        | 'europe-west6'
        | 'asia-east1'
        | 'asia-east2'
        | 'asia-northeast1'
        | 'asia-northeast2'
        | 'asia-northeast3'
        | 'asia-south1'
        | 'asia-south2'
        | 'asia-southeast1'
        | 'asia-southeast2'
        | 'australia-southeast1'
        | 'australia-southeast2'
        | 'asia'
        | 'eu'
        | 'us'
        | 'asia1'
        | 'eur4'
        | 'nam4'
      /**
       * Authentication
       * @description An HMAC key is a type of credential and can be associated with a service account or a user account in Cloud Storage. Read more <a href="https://cloud.google.com/storage/docs/authentication/hmackeys">here</a>.
       */
      credential: {
        /**
         * @default HMAC_KEY
         * @enum {string}
         */
        credential_type: 'HMAC_KEY'
        /**
         * Access ID
         * @description When linked to a service account, this ID is 61 characters long; when linked to a user account, it is 24 characters long. Read more <a href="https://cloud.google.com/storage/docs/authentication/hmackeys#overview">here</a>.
         */
        hmac_key_access_id: string
        /**
         * Secret
         * @description The corresponding secret for the access ID. It is a 40-character base-64 encoded string.  Read more <a href="https://cloud.google.com/storage/docs/authentication/hmackeys#secrets">here</a>.
         */
        hmac_key_secret: string
      }
      /**
       * Output Format
       * @description Output data format. One of the following formats must be selected - <a href="https://cloud.google.com/bigquery/docs/loading-data-cloud-storage-avro#advantages_of_avro">AVRO</a> format, <a href="https://cloud.google.com/bigquery/docs/loading-data-cloud-storage-parquet#parquet_schemas">PARQUET</a> format, <a href="https://cloud.google.com/bigquery/docs/loading-data-cloud-storage-csv#loading_csv_data_into_a_table">CSV</a> format, or <a href="https://cloud.google.com/bigquery/docs/loading-data-cloud-storage-json#loading_json_data_into_a_new_table">JSONL</a> format.
       */
      format: OneOf<
        [
          {
            /**
             * @default Avro
             * @enum {string}
             */
            format_type: 'Avro'
            /**
             * Compression Codec
             * @description The compression algorithm used to compress data. Default to no compression.
             */
            compression_codec:
              | {
                  /**
                   * @default no compression
                   * @enum {string}
                   */
                  codec: 'no compression'
                }
              | {
                  /**
                   * @default Deflate
                   * @enum {string}
                   */
                  codec: 'Deflate'
                  /**
                   * Deflate level
                   * @description 0: no compression & fastest, 9: best compression & slowest.
                   * @default 0
                   */
                  compression_level?: number
                }
              | {
                  /**
                   * @default bzip2
                   * @enum {string}
                   */
                  codec: 'bzip2'
                }
              | {
                  /**
                   * @default xz
                   * @enum {string}
                   */
                  codec: 'xz'
                  /**
                   * Compression Level
                   * @description The presets 0-3 are fast presets with medium compression. The presets 4-6 are fairly slow presets with high compression. The default preset is 6. The presets 7-9 are like the preset 6 but use bigger dictionaries and have higher compressor and decompressor memory requirements. Unless the uncompressed size of the file exceeds 8 MiB, 16 MiB, or 32 MiB, it is waste of memory to use the presets 7, 8, or 9, respectively. Read more <a href="https://commons.apache.org/proper/commons-compress/apidocs/org/apache/commons/compress/compressors/xz/XZCompressorOutputStream.html#XZCompressorOutputStream-java.io.OutputStream-int-">here</a> for details.
                   * @default 6
                   */
                  compression_level?: number
                }
              | {
                  /**
                   * @default zstandard
                   * @enum {string}
                   */
                  codec: 'zstandard'
                  /**
                   * Compression Level
                   * @description Negative levels are 'fast' modes akin to lz4 or snappy, levels above 9 are generally for archival purposes, and levels above 18 use a lot of memory.
                   * @default 3
                   */
                  compression_level?: number
                  /**
                   * Include Checksum
                   * @description If true, include a checksum with each data block.
                   * @default false
                   */
                  include_checksum?: boolean
                }
              | {
                  /**
                   * @default snappy
                   * @enum {string}
                   */
                  codec: 'snappy'
                }
          },
          {
            /**
             * @default CSV
             * @enum {string}
             */
            format_type: 'CSV'
            /**
             * Normalization
             * @description Whether the input JSON data should be normalized (flattened) in the output CSV. Please refer to docs for details.
             * @default No flattening
             * @enum {string}
             */
            flattening?: 'No flattening' | 'Root level flattening'
            /**
             * Compression
             * @description Whether the output files should be compressed. If compression is selected, the output filename will have an extra extension (GZIP: ".csv.gz").
             */
            compression?: OneOf<
              [
                {
                  /**
                   * @default No Compression
                   * @enum {string}
                   */
                  compression_type?: 'No Compression'
                },
                {
                  /**
                   * @default GZIP
                   * @enum {string}
                   */
                  compression_type?: 'GZIP'
                },
              ]
            >
          },
          {
            /**
             * @default JSONL
             * @enum {string}
             */
            format_type: 'JSONL'
            /**
             * Compression
             * @description Whether the output files should be compressed. If compression is selected, the output filename will have an extra extension (GZIP: ".jsonl.gz").
             */
            compression?: OneOf<
              [
                {
                  /**
                   * @default No Compression
                   * @enum {string}
                   */
                  compression_type?: 'No Compression'
                },
                {
                  /**
                   * @default GZIP
                   * @enum {string}
                   */
                  compression_type?: 'GZIP'
                },
              ]
            >
          },
          {
            /**
             * @default Parquet
             * @enum {string}
             */
            format_type: 'Parquet'
            /**
             * Compression Codec
             * @description The compression algorithm used to compress data pages.
             * @default UNCOMPRESSED
             * @enum {string}
             */
            compression_codec?:
              | 'UNCOMPRESSED'
              | 'SNAPPY'
              | 'GZIP'
              | 'LZO'
              | 'BROTLI'
              | 'LZ4'
              | 'ZSTD'
            /**
             * Block Size (Row Group Size) (MB)
             * @description This is the size of a row group being buffered in memory. It limits the memory usage when writing. Larger values will improve the IO when reading, but consume more memory when writing. Default: 128 MB.
             * @default 128
             */
            block_size_mb?: number
            /**
             * Max Padding Size (MB)
             * @description Maximum size allowed as padding to align row groups. This is also the minimum size of a row group. Default: 8 MB.
             * @default 8
             */
            max_padding_size_mb?: number
            /**
             * Page Size (KB)
             * @description The page size is for compression. A block is composed of pages. A page is the smallest unit that must be read fully to access a single record. If this value is too small, the compression will deteriorate. Default: 1024 KB.
             * @default 1024
             */
            page_size_kb?: number
            /**
             * Dictionary Page Size (KB)
             * @description There is one dictionary page per column per row group when dictionary encoding is used. The dictionary page size works like the page size but for dictionary. Default: 1024 KB.
             * @default 1024
             */
            dictionary_page_size_kb?: number
            /**
             * Dictionary Encoding
             * @description Default: true.
             * @default true
             */
            dictionary_encoding?: boolean
          },
        ]
      >
      /**
       * gcs
       * @constant
       * @enum {string}
       */
      destinationType: 'gcs'
    }
    /** GCS Destination Spec */
    'destination-gcs-update': {
      /**
       * GCS Bucket Name
       * @description You can find the bucket name in the App Engine Admin console Application Settings page, under the label Google Cloud Storage Bucket. Read more <a href="https://cloud.google.com/storage/docs/naming-buckets">here</a>.
       */
      gcs_bucket_name: string
      /**
       * GCS Bucket Path
       * @description GCS Bucket Path string Subdirectory under the above bucket to sync the data into.
       */
      gcs_bucket_path: string
      /**
       * GCS Bucket Region
       * @description Select a Region of the GCS Bucket. Read more <a href="https://cloud.google.com/storage/docs/locations">here</a>.
       * @default us
       * @enum {string}
       */
      gcs_bucket_region?:
        | 'northamerica-northeast1'
        | 'northamerica-northeast2'
        | 'us-central1'
        | 'us-east1'
        | 'us-east4'
        | 'us-west1'
        | 'us-west2'
        | 'us-west3'
        | 'us-west4'
        | 'southamerica-east1'
        | 'southamerica-west1'
        | 'europe-central2'
        | 'europe-north1'
        | 'europe-west1'
        | 'europe-west2'
        | 'europe-west3'
        | 'europe-west4'
        | 'europe-west6'
        | 'asia-east1'
        | 'asia-east2'
        | 'asia-northeast1'
        | 'asia-northeast2'
        | 'asia-northeast3'
        | 'asia-south1'
        | 'asia-south2'
        | 'asia-southeast1'
        | 'asia-southeast2'
        | 'australia-southeast1'
        | 'australia-southeast2'
        | 'asia'
        | 'eu'
        | 'us'
        | 'asia1'
        | 'eur4'
        | 'nam4'
      /**
       * Authentication
       * @description An HMAC key is a type of credential and can be associated with a service account or a user account in Cloud Storage. Read more <a href="https://cloud.google.com/storage/docs/authentication/hmackeys">here</a>.
       */
      credential: {
        /**
         * @default HMAC_KEY
         * @enum {string}
         */
        credential_type: 'HMAC_KEY'
        /**
         * Access ID
         * @description When linked to a service account, this ID is 61 characters long; when linked to a user account, it is 24 characters long. Read more <a href="https://cloud.google.com/storage/docs/authentication/hmackeys#overview">here</a>.
         */
        hmac_key_access_id: string
        /**
         * Secret
         * @description The corresponding secret for the access ID. It is a 40-character base-64 encoded string.  Read more <a href="https://cloud.google.com/storage/docs/authentication/hmackeys#secrets">here</a>.
         */
        hmac_key_secret: string
      }
      /**
       * Output Format
       * @description Output data format. One of the following formats must be selected - <a href="https://cloud.google.com/bigquery/docs/loading-data-cloud-storage-avro#advantages_of_avro">AVRO</a> format, <a href="https://cloud.google.com/bigquery/docs/loading-data-cloud-storage-parquet#parquet_schemas">PARQUET</a> format, <a href="https://cloud.google.com/bigquery/docs/loading-data-cloud-storage-csv#loading_csv_data_into_a_table">CSV</a> format, or <a href="https://cloud.google.com/bigquery/docs/loading-data-cloud-storage-json#loading_json_data_into_a_new_table">JSONL</a> format.
       */
      format: OneOf<
        [
          {
            /**
             * @default Avro
             * @enum {string}
             */
            format_type: 'Avro'
            /**
             * Compression Codec
             * @description The compression algorithm used to compress data. Default to no compression.
             */
            compression_codec:
              | {
                  /**
                   * @default no compression
                   * @enum {string}
                   */
                  codec: 'no compression'
                }
              | {
                  /**
                   * @default Deflate
                   * @enum {string}
                   */
                  codec: 'Deflate'
                  /**
                   * Deflate level
                   * @description 0: no compression & fastest, 9: best compression & slowest.
                   * @default 0
                   */
                  compression_level?: number
                }
              | {
                  /**
                   * @default bzip2
                   * @enum {string}
                   */
                  codec: 'bzip2'
                }
              | {
                  /**
                   * @default xz
                   * @enum {string}
                   */
                  codec: 'xz'
                  /**
                   * Compression Level
                   * @description The presets 0-3 are fast presets with medium compression. The presets 4-6 are fairly slow presets with high compression. The default preset is 6. The presets 7-9 are like the preset 6 but use bigger dictionaries and have higher compressor and decompressor memory requirements. Unless the uncompressed size of the file exceeds 8 MiB, 16 MiB, or 32 MiB, it is waste of memory to use the presets 7, 8, or 9, respectively. Read more <a href="https://commons.apache.org/proper/commons-compress/apidocs/org/apache/commons/compress/compressors/xz/XZCompressorOutputStream.html#XZCompressorOutputStream-java.io.OutputStream-int-">here</a> for details.
                   * @default 6
                   */
                  compression_level?: number
                }
              | {
                  /**
                   * @default zstandard
                   * @enum {string}
                   */
                  codec: 'zstandard'
                  /**
                   * Compression Level
                   * @description Negative levels are 'fast' modes akin to lz4 or snappy, levels above 9 are generally for archival purposes, and levels above 18 use a lot of memory.
                   * @default 3
                   */
                  compression_level?: number
                  /**
                   * Include Checksum
                   * @description If true, include a checksum with each data block.
                   * @default false
                   */
                  include_checksum?: boolean
                }
              | {
                  /**
                   * @default snappy
                   * @enum {string}
                   */
                  codec: 'snappy'
                }
          },
          {
            /**
             * @default CSV
             * @enum {string}
             */
            format_type: 'CSV'
            /**
             * Normalization
             * @description Whether the input JSON data should be normalized (flattened) in the output CSV. Please refer to docs for details.
             * @default No flattening
             * @enum {string}
             */
            flattening?: 'No flattening' | 'Root level flattening'
            /**
             * Compression
             * @description Whether the output files should be compressed. If compression is selected, the output filename will have an extra extension (GZIP: ".csv.gz").
             */
            compression?: OneOf<
              [
                {
                  /**
                   * @default No Compression
                   * @enum {string}
                   */
                  compression_type?: 'No Compression'
                },
                {
                  /**
                   * @default GZIP
                   * @enum {string}
                   */
                  compression_type?: 'GZIP'
                },
              ]
            >
          },
          {
            /**
             * @default JSONL
             * @enum {string}
             */
            format_type: 'JSONL'
            /**
             * Compression
             * @description Whether the output files should be compressed. If compression is selected, the output filename will have an extra extension (GZIP: ".jsonl.gz").
             */
            compression?: OneOf<
              [
                {
                  /**
                   * @default No Compression
                   * @enum {string}
                   */
                  compression_type?: 'No Compression'
                },
                {
                  /**
                   * @default GZIP
                   * @enum {string}
                   */
                  compression_type?: 'GZIP'
                },
              ]
            >
          },
          {
            /**
             * @default Parquet
             * @enum {string}
             */
            format_type: 'Parquet'
            /**
             * Compression Codec
             * @description The compression algorithm used to compress data pages.
             * @default UNCOMPRESSED
             * @enum {string}
             */
            compression_codec?:
              | 'UNCOMPRESSED'
              | 'SNAPPY'
              | 'GZIP'
              | 'LZO'
              | 'BROTLI'
              | 'LZ4'
              | 'ZSTD'
            /**
             * Block Size (Row Group Size) (MB)
             * @description This is the size of a row group being buffered in memory. It limits the memory usage when writing. Larger values will improve the IO when reading, but consume more memory when writing. Default: 128 MB.
             * @default 128
             */
            block_size_mb?: number
            /**
             * Max Padding Size (MB)
             * @description Maximum size allowed as padding to align row groups. This is also the minimum size of a row group. Default: 8 MB.
             * @default 8
             */
            max_padding_size_mb?: number
            /**
             * Page Size (KB)
             * @description The page size is for compression. A block is composed of pages. A page is the smallest unit that must be read fully to access a single record. If this value is too small, the compression will deteriorate. Default: 1024 KB.
             * @default 1024
             */
            page_size_kb?: number
            /**
             * Dictionary Page Size (KB)
             * @description There is one dictionary page per column per row group when dictionary encoding is used. The dictionary page size works like the page size but for dictionary. Default: 1024 KB.
             * @default 1024
             */
            dictionary_page_size_kb?: number
            /**
             * Dictionary Encoding
             * @description Default: true.
             * @default true
             */
            dictionary_encoding?: boolean
          },
        ]
      >
    }
    /** Destination Xata */
    'destination-xata': {
      /**
       * API Key
       * @description API Key to connect.
       */
      api_key: string
      /**
       * Database URL
       * @description URL pointing to your workspace.
       * @example https://my-workspace-abc123.us-east-1.xata.sh/db/nyc-taxi-fares:main
       */
      db_url: string
      /**
       * xata
       * @constant
       * @enum {string}
       */
      destinationType: 'xata'
    }
    /** Destination Xata */
    'destination-xata-update': {
      /**
       * API Key
       * @description API Key to connect.
       */
      api_key: string
      /**
       * Database URL
       * @description URL pointing to your workspace.
       * @example https://my-workspace-abc123.us-east-1.xata.sh/db/nyc-taxi-fares:main
       */
      db_url: string
    }
    /** ClickHouse Destination Spec */
    'destination-clickhouse': {
      /**
       * Host
       * @description Hostname of the database.
       */
      host: string
      /**
       * Port
       * @description HTTP port of the database.
       * @default 8123
       */
      port: number
      /**
       * DB Name
       * @description Name of the database.
       */
      database: string
      /**
       * User
       * @description Username to use to access the database.
       */
      username: string
      /**
       * Password
       * @description Password associated with the username.
       */
      password?: string
      /**
       * JDBC URL Params
       * @description Additional properties to pass to the JDBC URL string when connecting to the database formatted as 'key=value' pairs separated by the symbol '&'. (example: key1=value1&key2=value2&key3=value3).
       */
      jdbc_url_params?: string
      /**
       * SSH Tunnel Method
       * @description Whether to initiate an SSH tunnel before connecting to the database, and if so, which kind of authentication to use.
       */
      tunnel_method?: OneOf<
        [
          {
            /**
             * @description No ssh tunnel needed to connect to database
             * @constant
             * @enum {string}
             */
            tunnel_method: 'NO_TUNNEL'
          },
          {
            /**
             * @description Connect through a jump server tunnel host using username and ssh key
             * @constant
             * @enum {string}
             */
            tunnel_method: 'SSH_KEY_AUTH'
            /**
             * SSH Tunnel Jump Server Host
             * @description Hostname of the jump server host that allows inbound ssh tunnel.
             */
            tunnel_host: string
            /**
             * SSH Connection Port
             * @description Port on the proxy/jump server that accepts inbound ssh connections.
             * @default 22
             */
            tunnel_port: number
            /**
             * SSH Login Username
             * @description OS-level username for logging into the jump server host.
             */
            tunnel_user: string
            /**
             * SSH Private Key
             * @description OS-level user account ssh key credentials in RSA PEM format ( created with ssh-keygen -t rsa -m PEM -f myuser_rsa )
             */
            ssh_key: string
          },
          {
            /**
             * @description Connect through a jump server tunnel host using username and password authentication
             * @constant
             * @enum {string}
             */
            tunnel_method: 'SSH_PASSWORD_AUTH'
            /**
             * SSH Tunnel Jump Server Host
             * @description Hostname of the jump server host that allows inbound ssh tunnel.
             */
            tunnel_host: string
            /**
             * SSH Connection Port
             * @description Port on the proxy/jump server that accepts inbound ssh connections.
             * @default 22
             */
            tunnel_port: number
            /**
             * SSH Login Username
             * @description OS-level username for logging into the jump server host
             */
            tunnel_user: string
            /**
             * Password
             * @description OS-level password for logging into the jump server host
             */
            tunnel_user_password: string
          },
        ]
      >
      /**
       * clickhouse
       * @constant
       * @enum {string}
       */
      destinationType: 'clickhouse'
    }
    /** ClickHouse Destination Spec */
    'destination-clickhouse-update': {
      /**
       * Host
       * @description Hostname of the database.
       */
      host: string
      /**
       * Port
       * @description HTTP port of the database.
       * @default 8123
       */
      port: number
      /**
       * DB Name
       * @description Name of the database.
       */
      database: string
      /**
       * User
       * @description Username to use to access the database.
       */
      username: string
      /**
       * Password
       * @description Password associated with the username.
       */
      password?: string
      /**
       * JDBC URL Params
       * @description Additional properties to pass to the JDBC URL string when connecting to the database formatted as 'key=value' pairs separated by the symbol '&'. (example: key1=value1&key2=value2&key3=value3).
       */
      jdbc_url_params?: string
      /**
       * SSH Tunnel Method
       * @description Whether to initiate an SSH tunnel before connecting to the database, and if so, which kind of authentication to use.
       */
      tunnel_method?: OneOf<
        [
          {
            /**
             * @description No ssh tunnel needed to connect to database
             * @constant
             * @enum {string}
             */
            tunnel_method: 'NO_TUNNEL'
          },
          {
            /**
             * @description Connect through a jump server tunnel host using username and ssh key
             * @constant
             * @enum {string}
             */
            tunnel_method: 'SSH_KEY_AUTH'
            /**
             * SSH Tunnel Jump Server Host
             * @description Hostname of the jump server host that allows inbound ssh tunnel.
             */
            tunnel_host: string
            /**
             * SSH Connection Port
             * @description Port on the proxy/jump server that accepts inbound ssh connections.
             * @default 22
             */
            tunnel_port: number
            /**
             * SSH Login Username
             * @description OS-level username for logging into the jump server host.
             */
            tunnel_user: string
            /**
             * SSH Private Key
             * @description OS-level user account ssh key credentials in RSA PEM format ( created with ssh-keygen -t rsa -m PEM -f myuser_rsa )
             */
            ssh_key: string
          },
          {
            /**
             * @description Connect through a jump server tunnel host using username and password authentication
             * @constant
             * @enum {string}
             */
            tunnel_method: 'SSH_PASSWORD_AUTH'
            /**
             * SSH Tunnel Jump Server Host
             * @description Hostname of the jump server host that allows inbound ssh tunnel.
             */
            tunnel_host: string
            /**
             * SSH Connection Port
             * @description Port on the proxy/jump server that accepts inbound ssh connections.
             * @default 22
             */
            tunnel_port: number
            /**
             * SSH Login Username
             * @description OS-level username for logging into the jump server host
             */
            tunnel_user: string
            /**
             * Password
             * @description OS-level password for logging into the jump server host
             */
            tunnel_user_password: string
          },
        ]
      >
    }
    /** MS SQL Server Destination Spec */
    'destination-mssql': {
      /**
       * Host
       * @description The host name of the MSSQL database.
       */
      host: string
      /**
       * Port
       * @description The port of the MSSQL database.
       * @default 1433
       */
      port: number
      /**
       * DB Name
       * @description The name of the MSSQL database.
       */
      database: string
      /**
       * Default Schema
       * @description The default schema tables are written to if the source does not specify a namespace. The usual value for this field is "public".
       * @default public
       */
      schema: string
      /**
       * User
       * @description The username which is used to access the database.
       */
      username: string
      /**
       * Password
       * @description The password associated with this username.
       */
      password?: string
      /**
       * JDBC URL Params
       * @description Additional properties to pass to the JDBC URL string when connecting to the database formatted as 'key=value' pairs separated by the symbol '&'. (example: key1=value1&key2=value2&key3=value3).
       */
      jdbc_url_params?: string
      /**
       * SSL Method
       * @description The encryption method which is used to communicate with the database.
       */
      ssl_method?: OneOf<
        [
          {
            /**
             * @default encrypted_trust_server_certificate
             * @constant
             * @enum {string}
             */
            ssl_method: 'encrypted_trust_server_certificate'
          },
          {
            /**
             * @default encrypted_verify_certificate
             * @constant
             * @enum {string}
             */
            ssl_method: 'encrypted_verify_certificate'
            /**
             * Host Name In Certificate
             * @description Specifies the host name of the server. The value of this property must match the subject property of the certificate.
             */
            hostNameInCertificate?: string
          },
        ]
      >
      /**
       * SSH Tunnel Method
       * @description Whether to initiate an SSH tunnel before connecting to the database, and if so, which kind of authentication to use.
       */
      tunnel_method?: OneOf<
        [
          {
            /**
             * @description No ssh tunnel needed to connect to database
             * @constant
             * @enum {string}
             */
            tunnel_method: 'NO_TUNNEL'
          },
          {
            /**
             * @description Connect through a jump server tunnel host using username and ssh key
             * @constant
             * @enum {string}
             */
            tunnel_method: 'SSH_KEY_AUTH'
            /**
             * SSH Tunnel Jump Server Host
             * @description Hostname of the jump server host that allows inbound ssh tunnel.
             */
            tunnel_host: string
            /**
             * SSH Connection Port
             * @description Port on the proxy/jump server that accepts inbound ssh connections.
             * @default 22
             */
            tunnel_port: number
            /**
             * SSH Login Username
             * @description OS-level username for logging into the jump server host.
             */
            tunnel_user: string
            /**
             * SSH Private Key
             * @description OS-level user account ssh key credentials in RSA PEM format ( created with ssh-keygen -t rsa -m PEM -f myuser_rsa )
             */
            ssh_key: string
          },
          {
            /**
             * @description Connect through a jump server tunnel host using username and password authentication
             * @constant
             * @enum {string}
             */
            tunnel_method: 'SSH_PASSWORD_AUTH'
            /**
             * SSH Tunnel Jump Server Host
             * @description Hostname of the jump server host that allows inbound ssh tunnel.
             */
            tunnel_host: string
            /**
             * SSH Connection Port
             * @description Port on the proxy/jump server that accepts inbound ssh connections.
             * @default 22
             */
            tunnel_port: number
            /**
             * SSH Login Username
             * @description OS-level username for logging into the jump server host
             */
            tunnel_user: string
            /**
             * Password
             * @description OS-level password for logging into the jump server host
             */
            tunnel_user_password: string
          },
        ]
      >
      /**
       * mssql
       * @constant
       * @enum {string}
       */
      destinationType: 'mssql'
    }
    /** MS SQL Server Destination Spec */
    'destination-mssql-update': {
      /**
       * Host
       * @description The host name of the MSSQL database.
       */
      host: string
      /**
       * Port
       * @description The port of the MSSQL database.
       * @default 1433
       */
      port: number
      /**
       * DB Name
       * @description The name of the MSSQL database.
       */
      database: string
      /**
       * Default Schema
       * @description The default schema tables are written to if the source does not specify a namespace. The usual value for this field is "public".
       * @default public
       */
      schema: string
      /**
       * User
       * @description The username which is used to access the database.
       */
      username: string
      /**
       * Password
       * @description The password associated with this username.
       */
      password?: string
      /**
       * JDBC URL Params
       * @description Additional properties to pass to the JDBC URL string when connecting to the database formatted as 'key=value' pairs separated by the symbol '&'. (example: key1=value1&key2=value2&key3=value3).
       */
      jdbc_url_params?: string
      /**
       * SSL Method
       * @description The encryption method which is used to communicate with the database.
       */
      ssl_method?: OneOf<
        [
          {
            /**
             * @default encrypted_trust_server_certificate
             * @constant
             * @enum {string}
             */
            ssl_method: 'encrypted_trust_server_certificate'
          },
          {
            /**
             * @default encrypted_verify_certificate
             * @constant
             * @enum {string}
             */
            ssl_method: 'encrypted_verify_certificate'
            /**
             * Host Name In Certificate
             * @description Specifies the host name of the server. The value of this property must match the subject property of the certificate.
             */
            hostNameInCertificate?: string
          },
        ]
      >
      /**
       * SSH Tunnel Method
       * @description Whether to initiate an SSH tunnel before connecting to the database, and if so, which kind of authentication to use.
       */
      tunnel_method?: OneOf<
        [
          {
            /**
             * @description No ssh tunnel needed to connect to database
             * @constant
             * @enum {string}
             */
            tunnel_method: 'NO_TUNNEL'
          },
          {
            /**
             * @description Connect through a jump server tunnel host using username and ssh key
             * @constant
             * @enum {string}
             */
            tunnel_method: 'SSH_KEY_AUTH'
            /**
             * SSH Tunnel Jump Server Host
             * @description Hostname of the jump server host that allows inbound ssh tunnel.
             */
            tunnel_host: string
            /**
             * SSH Connection Port
             * @description Port on the proxy/jump server that accepts inbound ssh connections.
             * @default 22
             */
            tunnel_port: number
            /**
             * SSH Login Username
             * @description OS-level username for logging into the jump server host.
             */
            tunnel_user: string
            /**
             * SSH Private Key
             * @description OS-level user account ssh key credentials in RSA PEM format ( created with ssh-keygen -t rsa -m PEM -f myuser_rsa )
             */
            ssh_key: string
          },
          {
            /**
             * @description Connect through a jump server tunnel host using username and password authentication
             * @constant
             * @enum {string}
             */
            tunnel_method: 'SSH_PASSWORD_AUTH'
            /**
             * SSH Tunnel Jump Server Host
             * @description Hostname of the jump server host that allows inbound ssh tunnel.
             */
            tunnel_host: string
            /**
             * SSH Connection Port
             * @description Port on the proxy/jump server that accepts inbound ssh connections.
             * @default 22
             */
            tunnel_port: number
            /**
             * SSH Login Username
             * @description OS-level username for logging into the jump server host
             */
            tunnel_user: string
            /**
             * Password
             * @description OS-level password for logging into the jump server host
             */
            tunnel_user_password: string
          },
        ]
      >
    }
    /** MySQL Destination Spec */
    'destination-mysql': {
      /**
       * Host
       * @description Hostname of the database.
       */
      host: string
      /**
       * Port
       * @description Port of the database.
       * @default 3306
       */
      port: number
      /**
       * DB Name
       * @description Name of the database.
       */
      database: string
      /**
       * User
       * @description Username to use to access the database.
       */
      username: string
      /**
       * Password
       * @description Password associated with the username.
       */
      password?: string
      /**
       * JDBC URL Params
       * @description Additional properties to pass to the JDBC URL string when connecting to the database formatted as 'key=value' pairs separated by the symbol '&'. (example: key1=value1&key2=value2&key3=value3).
       */
      jdbc_url_params?: string
      /**
       * SSH Tunnel Method
       * @description Whether to initiate an SSH tunnel before connecting to the database, and if so, which kind of authentication to use.
       */
      tunnel_method?: OneOf<
        [
          {
            /**
             * @description No ssh tunnel needed to connect to database
             * @constant
             * @enum {string}
             */
            tunnel_method: 'NO_TUNNEL'
          },
          {
            /**
             * @description Connect through a jump server tunnel host using username and ssh key
             * @constant
             * @enum {string}
             */
            tunnel_method: 'SSH_KEY_AUTH'
            /**
             * SSH Tunnel Jump Server Host
             * @description Hostname of the jump server host that allows inbound ssh tunnel.
             */
            tunnel_host: string
            /**
             * SSH Connection Port
             * @description Port on the proxy/jump server that accepts inbound ssh connections.
             * @default 22
             */
            tunnel_port: number
            /**
             * SSH Login Username
             * @description OS-level username for logging into the jump server host.
             */
            tunnel_user: string
            /**
             * SSH Private Key
             * @description OS-level user account ssh key credentials in RSA PEM format ( created with ssh-keygen -t rsa -m PEM -f myuser_rsa )
             */
            ssh_key: string
          },
          {
            /**
             * @description Connect through a jump server tunnel host using username and password authentication
             * @constant
             * @enum {string}
             */
            tunnel_method: 'SSH_PASSWORD_AUTH'
            /**
             * SSH Tunnel Jump Server Host
             * @description Hostname of the jump server host that allows inbound ssh tunnel.
             */
            tunnel_host: string
            /**
             * SSH Connection Port
             * @description Port on the proxy/jump server that accepts inbound ssh connections.
             * @default 22
             */
            tunnel_port: number
            /**
             * SSH Login Username
             * @description OS-level username for logging into the jump server host
             */
            tunnel_user: string
            /**
             * Password
             * @description OS-level password for logging into the jump server host
             */
            tunnel_user_password: string
          },
        ]
      >
      /**
       * mysql
       * @constant
       * @enum {string}
       */
      destinationType: 'mysql'
    }
    /** MySQL Destination Spec */
    'destination-mysql-update': {
      /**
       * Host
       * @description Hostname of the database.
       */
      host: string
      /**
       * Port
       * @description Port of the database.
       * @default 3306
       */
      port: number
      /**
       * DB Name
       * @description Name of the database.
       */
      database: string
      /**
       * User
       * @description Username to use to access the database.
       */
      username: string
      /**
       * Password
       * @description Password associated with the username.
       */
      password?: string
      /**
       * JDBC URL Params
       * @description Additional properties to pass to the JDBC URL string when connecting to the database formatted as 'key=value' pairs separated by the symbol '&'. (example: key1=value1&key2=value2&key3=value3).
       */
      jdbc_url_params?: string
      /**
       * SSH Tunnel Method
       * @description Whether to initiate an SSH tunnel before connecting to the database, and if so, which kind of authentication to use.
       */
      tunnel_method?: OneOf<
        [
          {
            /**
             * @description No ssh tunnel needed to connect to database
             * @constant
             * @enum {string}
             */
            tunnel_method: 'NO_TUNNEL'
          },
          {
            /**
             * @description Connect through a jump server tunnel host using username and ssh key
             * @constant
             * @enum {string}
             */
            tunnel_method: 'SSH_KEY_AUTH'
            /**
             * SSH Tunnel Jump Server Host
             * @description Hostname of the jump server host that allows inbound ssh tunnel.
             */
            tunnel_host: string
            /**
             * SSH Connection Port
             * @description Port on the proxy/jump server that accepts inbound ssh connections.
             * @default 22
             */
            tunnel_port: number
            /**
             * SSH Login Username
             * @description OS-level username for logging into the jump server host.
             */
            tunnel_user: string
            /**
             * SSH Private Key
             * @description OS-level user account ssh key credentials in RSA PEM format ( created with ssh-keygen -t rsa -m PEM -f myuser_rsa )
             */
            ssh_key: string
          },
          {
            /**
             * @description Connect through a jump server tunnel host using username and password authentication
             * @constant
             * @enum {string}
             */
            tunnel_method: 'SSH_PASSWORD_AUTH'
            /**
             * SSH Tunnel Jump Server Host
             * @description Hostname of the jump server host that allows inbound ssh tunnel.
             */
            tunnel_host: string
            /**
             * SSH Connection Port
             * @description Port on the proxy/jump server that accepts inbound ssh connections.
             * @default 22
             */
            tunnel_port: number
            /**
             * SSH Login Username
             * @description OS-level username for logging into the jump server host
             */
            tunnel_user: string
            /**
             * Password
             * @description OS-level password for logging into the jump server host
             */
            tunnel_user_password: string
          },
        ]
      >
    }
    /** Google PubSub Destination Spec */
    'destination-pubsub': {
      /**
       * Project ID
       * @description The GCP project ID for the project containing the target PubSub.
       */
      project_id: string
      /**
       * PubSub Topic ID
       * @description The PubSub topic ID in the given GCP project ID.
       */
      topic_id: string
      /**
       * Credentials JSON
       * @description The contents of the JSON service account key. Check out the <a href="https://docs.airbyte.com/integrations/destinations/pubsub">docs</a> if you need help generating this key.
       */
      credentials_json: string
      /**
       * Message Ordering Enabled
       * @description If TRUE PubSub publisher will have <a href="https://cloud.google.com/pubsub/docs/ordering">message ordering</a> enabled. Every message will have an ordering key of stream
       * @default false
       */
      ordering_enabled: boolean
      /**
       * Message Batching Enabled
       * @description If TRUE messages will be buffered instead of sending them one by one
       * @default false
       */
      batching_enabled: boolean
      /**
       * Message Batching: Delay Threshold
       * @description Number of ms before the buffer is flushed
       * @default 1
       */
      batching_delay_threshold?: number
      /**
       * Message Batching: Element Count Threshold
       * @description Number of messages before the buffer is flushed
       * @default 1
       */
      batching_element_count_threshold?: number
      /**
       * Message Batching: Request Bytes Threshold
       * @description Number of bytes before the buffer is flushed
       * @default 1
       */
      batching_request_bytes_threshold?: number
      /**
       * pubsub
       * @constant
       * @enum {string}
       */
      destinationType: 'pubsub'
    }
    /** Google PubSub Destination Spec */
    'destination-pubsub-update': {
      /**
       * Project ID
       * @description The GCP project ID for the project containing the target PubSub.
       */
      project_id: string
      /**
       * PubSub Topic ID
       * @description The PubSub topic ID in the given GCP project ID.
       */
      topic_id: string
      /**
       * Credentials JSON
       * @description The contents of the JSON service account key. Check out the <a href="https://docs.airbyte.com/integrations/destinations/pubsub">docs</a> if you need help generating this key.
       */
      credentials_json: string
      /**
       * Message Ordering Enabled
       * @description If TRUE PubSub publisher will have <a href="https://cloud.google.com/pubsub/docs/ordering">message ordering</a> enabled. Every message will have an ordering key of stream
       * @default false
       */
      ordering_enabled: boolean
      /**
       * Message Batching Enabled
       * @description If TRUE messages will be buffered instead of sending them one by one
       * @default false
       */
      batching_enabled: boolean
      /**
       * Message Batching: Delay Threshold
       * @description Number of ms before the buffer is flushed
       * @default 1
       */
      batching_delay_threshold?: number
      /**
       * Message Batching: Element Count Threshold
       * @description Number of messages before the buffer is flushed
       * @default 1
       */
      batching_element_count_threshold?: number
      /**
       * Message Batching: Request Bytes Threshold
       * @description Number of bytes before the buffer is flushed
       * @default 1
       */
      batching_request_bytes_threshold?: number
    }
    /**
     * Destination Config
     * @description The configuration model for the Vector DB based destinations. This model is used to generate the UI for the destination configuration,
     * as well as to provide type safety for the configuration passed to the destination.
     *
     * The configuration model is composed of four parts:
     * * Processing configuration
     * * Embedding configuration
     * * Indexing configuration
     * * Advanced configuration
     *
     * Processing, embedding and advanced configuration are provided by this base class, while the indexing configuration is provided by the destination connector in the sub class.
     */
    'destination-weaviate': {
      /**
       * Embedding
       * @description Embedding configuration
       */
      embedding:
        | {
            /**
             * Mode
             * @default no_embedding
             * @constant
             * @enum {string}
             */
            mode: 'no_embedding'
          }
        | {
            /**
             * Mode
             * @default azure_openai
             * @constant
             * @enum {string}
             */
            mode: 'azure_openai'
            /**
             * Azure OpenAI API key
             * @description The API key for your Azure OpenAI resource.  You can find this in the Azure portal under your Azure OpenAI resource
             */
            openai_key: string
            /**
             * Resource base URL
             * @description The base URL for your Azure OpenAI resource.  You can find this in the Azure portal under your Azure OpenAI resource
             */
            api_base: string
            /**
             * Deployment
             * @description The deployment for your Azure OpenAI resource.  You can find this in the Azure portal under your Azure OpenAI resource
             */
            deployment: string
          }
        | {
            /**
             * Mode
             * @default openai
             * @constant
             * @enum {string}
             */
            mode: 'openai'
            /** OpenAI API key */
            openai_key: string
          }
        | {
            /**
             * Mode
             * @default cohere
             * @constant
             * @enum {string}
             */
            mode: 'cohere'
            /** Cohere API key */
            cohere_key: string
          }
        | {
            /**
             * Mode
             * @default from_field
             * @constant
             * @enum {string}
             */
            mode: 'from_field'
            /**
             * Field name
             * @description Name of the field in the record that contains the embedding
             */
            field_name: string
            /**
             * Embedding dimensions
             * @description The number of dimensions the embedding model is generating
             */
            dimensions: number
          }
        | {
            /**
             * Mode
             * @default fake
             * @constant
             * @enum {string}
             */
            mode: 'fake'
          }
        | {
            /**
             * Mode
             * @default openai_compatible
             * @constant
             * @enum {string}
             */
            mode: 'openai_compatible'
            /**
             * API key
             * @default
             */
            api_key?: string
            /**
             * Base URL
             * @description The base URL for your OpenAI-compatible service
             */
            base_url: string
            /**
             * Model name
             * @description The name of the model to use for embedding
             * @default text-embedding-ada-002
             */
            model_name?: string
            /**
             * Embedding dimensions
             * @description The number of dimensions the embedding model is generating
             */
            dimensions: number
          }
      /** ProcessingConfigModel */
      processing: {
        /**
         * Chunk size
         * @description Size of chunks in tokens to store in vector store (make sure it is not too big for the context if your LLM)
         */
        chunk_size: number
        /**
         * Chunk overlap
         * @description Size of overlap between chunks in tokens to store in vector store to better capture relevant context
         * @default 0
         */
        chunk_overlap?: number
        /**
         * Text fields to embed
         * @description List of fields in the record that should be used to calculate the embedding. The field list is applied to all streams in the same way and non-existing fields are ignored. If none are defined, all fields are considered text fields. When specifying text fields, you can access nested fields in the record by using dot notation, e.g. `user.name` will access the `name` field in the `user` object. It's also possible to use wildcards to access all fields in an object, e.g. `users.*.name` will access all `names` fields in all entries of the `users` array.
         * @default []
         */
        text_fields?: string[]
        /**
         * Fields to store as metadata
         * @description List of fields in the record that should be stored as metadata. The field list is applied to all streams in the same way and non-existing fields are ignored. If none are defined, all fields are considered metadata fields. When specifying text fields, you can access nested fields in the record by using dot notation, e.g. `user.name` will access the `name` field in the `user` object. It's also possible to use wildcards to access all fields in an object, e.g. `users.*.name` will access all `names` fields in all entries of the `users` array. When specifying nested paths, all matching values are flattened into an array set to a field named by the path.
         * @default []
         */
        metadata_fields?: string[]
        /**
         * Text splitter
         * @description Split text fields into chunks based on the specified method.
         */
        text_splitter?: OneOf<
          [
            {
              /**
               * Mode
               * @default separator
               * @constant
               * @enum {string}
               */
              mode: 'separator'
              /**
               * Separators
               * @description List of separator strings to split text fields by. The separator itself needs to be wrapped in double quotes, e.g. to split by the dot character, use ".". To split by a newline, use "\n".
               * @default [
               *   "\"\\n\\n\"",
               *   "\"\\n\"",
               *   "\" \"",
               *   "\"\""
               * ]
               */
              separators?: string[]
              /**
               * Keep separator
               * @description Whether to keep the separator in the resulting chunks
               * @default false
               */
              keep_separator?: boolean
            },
            {
              /**
               * Mode
               * @default markdown
               * @constant
               * @enum {string}
               */
              mode: 'markdown'
              /**
               * Split level
               * @description Level of markdown headers to split text fields by. Headings down to the specified level will be used as split points
               * @default 1
               */
              split_level?: number
            },
            {
              /**
               * Mode
               * @default code
               * @constant
               * @enum {string}
               */
              mode: 'code'
              /**
               * Language
               * @description Split code in suitable places based on the programming language
               * @enum {string}
               */
              language:
                | 'cpp'
                | 'go'
                | 'java'
                | 'js'
                | 'php'
                | 'proto'
                | 'python'
                | 'rst'
                | 'ruby'
                | 'rust'
                | 'scala'
                | 'swift'
                | 'markdown'
                | 'latex'
                | 'html'
                | 'sol'
            },
          ]
        >
        /**
         * Field name mappings
         * @description List of fields to rename. Not applicable for nested fields, but can be used to rename fields already flattened via dot notation.
         * @default []
         */
        field_name_mappings?: {
          /**
           * From field name
           * @description The field name in the source
           */
          from_field: string
          /**
           * To field name
           * @description The field name to use in the destination
           */
          to_field: string
        }[]
      }
      /**
       * Do not store raw text
       * @description Do not store the text that gets embedded along with the vector and the metadata in the destination. If set to true, only the vector and the metadata will be stored - in this case raw text for LLM use cases needs to be retrieved from another source.
       * @default false
       */
      omit_raw_text?: boolean
      /**
       * Indexing
       * @description Indexing configuration
       */
      indexing: {
        /**
         * Public Endpoint
         * @description The public endpoint of the Weaviate cluster.
         */
        host: string
        /**
         * Authentication
         * @description Authentication method
         */
        auth: OneOf<
          [
            {
              /**
               * Mode
               * @default token
               * @constant
               * @enum {string}
               */
              mode: 'token'
              /**
               * API Token
               * @description API Token for the Weaviate instance
               */
              token: string
            },
            {
              /**
               * Mode
               * @default username_password
               * @constant
               * @enum {string}
               */
              mode: 'username_password'
              /**
               * Username
               * @description Username for the Weaviate cluster
               */
              username: string
              /**
               * Password
               * @description Password for the Weaviate cluster
               */
              password: string
            },
            {
              /**
               * Mode
               * @default no_auth
               * @constant
               * @enum {string}
               */
              mode: 'no_auth'
            },
          ]
        >
        /**
         * Batch Size
         * @description The number of records to send to Weaviate in each batch
         * @default 128
         */
        batch_size?: number
        /**
         * Text Field
         * @description The field in the object that contains the embedded text
         * @default text
         */
        text_field?: string
        /**
         * Tenant ID
         * @description The tenant ID to use for multi tenancy
         * @default
         */
        tenant_id?: string
        /**
         * Default Vectorizer
         * @description The vectorizer to use if new classes need to be created
         * @default none
         * @enum {string}
         */
        default_vectorizer?:
          | 'none'
          | 'text2vec-cohere'
          | 'text2vec-huggingface'
          | 'text2vec-openai'
          | 'text2vec-palm'
          | 'text2vec-contextionary'
          | 'text2vec-transformers'
          | 'text2vec-gpt4all'
        /**
         * Additional headers
         * @description Additional HTTP headers to send with every request.
         * @default []
         */
        additional_headers?: {
          /** Header Key */
          header_key: string
          /** Header Value */
          value: string
        }[]
      }
      /**
       * weaviate
       * @constant
       * @enum {string}
       */
      destinationType: 'weaviate'
    }
    /**
     * Destination Config
     * @description The configuration model for the Vector DB based destinations. This model is used to generate the UI for the destination configuration,
     * as well as to provide type safety for the configuration passed to the destination.
     *
     * The configuration model is composed of four parts:
     * * Processing configuration
     * * Embedding configuration
     * * Indexing configuration
     * * Advanced configuration
     *
     * Processing, embedding and advanced configuration are provided by this base class, while the indexing configuration is provided by the destination connector in the sub class.
     */
    'destination-weaviate-update': {
      /**
       * Embedding
       * @description Embedding configuration
       */
      embedding:
        | {
            /**
             * Mode
             * @default no_embedding
             * @constant
             * @enum {string}
             */
            mode: 'no_embedding'
          }
        | {
            /**
             * Mode
             * @default azure_openai
             * @constant
             * @enum {string}
             */
            mode: 'azure_openai'
            /**
             * Azure OpenAI API key
             * @description The API key for your Azure OpenAI resource.  You can find this in the Azure portal under your Azure OpenAI resource
             */
            openai_key: string
            /**
             * Resource base URL
             * @description The base URL for your Azure OpenAI resource.  You can find this in the Azure portal under your Azure OpenAI resource
             */
            api_base: string
            /**
             * Deployment
             * @description The deployment for your Azure OpenAI resource.  You can find this in the Azure portal under your Azure OpenAI resource
             */
            deployment: string
          }
        | {
            /**
             * Mode
             * @default openai
             * @constant
             * @enum {string}
             */
            mode: 'openai'
            /** OpenAI API key */
            openai_key: string
          }
        | {
            /**
             * Mode
             * @default cohere
             * @constant
             * @enum {string}
             */
            mode: 'cohere'
            /** Cohere API key */
            cohere_key: string
          }
        | {
            /**
             * Mode
             * @default from_field
             * @constant
             * @enum {string}
             */
            mode: 'from_field'
            /**
             * Field name
             * @description Name of the field in the record that contains the embedding
             */
            field_name: string
            /**
             * Embedding dimensions
             * @description The number of dimensions the embedding model is generating
             */
            dimensions: number
          }
        | {
            /**
             * Mode
             * @default fake
             * @constant
             * @enum {string}
             */
            mode: 'fake'
          }
        | {
            /**
             * Mode
             * @default openai_compatible
             * @constant
             * @enum {string}
             */
            mode: 'openai_compatible'
            /**
             * API key
             * @default
             */
            api_key?: string
            /**
             * Base URL
             * @description The base URL for your OpenAI-compatible service
             */
            base_url: string
            /**
             * Model name
             * @description The name of the model to use for embedding
             * @default text-embedding-ada-002
             */
            model_name?: string
            /**
             * Embedding dimensions
             * @description The number of dimensions the embedding model is generating
             */
            dimensions: number
          }
      /** ProcessingConfigModel */
      processing: {
        /**
         * Chunk size
         * @description Size of chunks in tokens to store in vector store (make sure it is not too big for the context if your LLM)
         */
        chunk_size: number
        /**
         * Chunk overlap
         * @description Size of overlap between chunks in tokens to store in vector store to better capture relevant context
         * @default 0
         */
        chunk_overlap?: number
        /**
         * Text fields to embed
         * @description List of fields in the record that should be used to calculate the embedding. The field list is applied to all streams in the same way and non-existing fields are ignored. If none are defined, all fields are considered text fields. When specifying text fields, you can access nested fields in the record by using dot notation, e.g. `user.name` will access the `name` field in the `user` object. It's also possible to use wildcards to access all fields in an object, e.g. `users.*.name` will access all `names` fields in all entries of the `users` array.
         * @default []
         */
        text_fields?: string[]
        /**
         * Fields to store as metadata
         * @description List of fields in the record that should be stored as metadata. The field list is applied to all streams in the same way and non-existing fields are ignored. If none are defined, all fields are considered metadata fields. When specifying text fields, you can access nested fields in the record by using dot notation, e.g. `user.name` will access the `name` field in the `user` object. It's also possible to use wildcards to access all fields in an object, e.g. `users.*.name` will access all `names` fields in all entries of the `users` array. When specifying nested paths, all matching values are flattened into an array set to a field named by the path.
         * @default []
         */
        metadata_fields?: string[]
        /**
         * Text splitter
         * @description Split text fields into chunks based on the specified method.
         */
        text_splitter?: OneOf<
          [
            {
              /**
               * Mode
               * @default separator
               * @constant
               * @enum {string}
               */
              mode: 'separator'
              /**
               * Separators
               * @description List of separator strings to split text fields by. The separator itself needs to be wrapped in double quotes, e.g. to split by the dot character, use ".". To split by a newline, use "\n".
               * @default [
               *   "\"\\n\\n\"",
               *   "\"\\n\"",
               *   "\" \"",
               *   "\"\""
               * ]
               */
              separators?: string[]
              /**
               * Keep separator
               * @description Whether to keep the separator in the resulting chunks
               * @default false
               */
              keep_separator?: boolean
            },
            {
              /**
               * Mode
               * @default markdown
               * @constant
               * @enum {string}
               */
              mode: 'markdown'
              /**
               * Split level
               * @description Level of markdown headers to split text fields by. Headings down to the specified level will be used as split points
               * @default 1
               */
              split_level?: number
            },
            {
              /**
               * Mode
               * @default code
               * @constant
               * @enum {string}
               */
              mode: 'code'
              /**
               * Language
               * @description Split code in suitable places based on the programming language
               * @enum {string}
               */
              language:
                | 'cpp'
                | 'go'
                | 'java'
                | 'js'
                | 'php'
                | 'proto'
                | 'python'
                | 'rst'
                | 'ruby'
                | 'rust'
                | 'scala'
                | 'swift'
                | 'markdown'
                | 'latex'
                | 'html'
                | 'sol'
            },
          ]
        >
        /**
         * Field name mappings
         * @description List of fields to rename. Not applicable for nested fields, but can be used to rename fields already flattened via dot notation.
         * @default []
         */
        field_name_mappings?: {
          /**
           * From field name
           * @description The field name in the source
           */
          from_field: string
          /**
           * To field name
           * @description The field name to use in the destination
           */
          to_field: string
        }[]
      }
      /**
       * Do not store raw text
       * @description Do not store the text that gets embedded along with the vector and the metadata in the destination. If set to true, only the vector and the metadata will be stored - in this case raw text for LLM use cases needs to be retrieved from another source.
       * @default false
       */
      omit_raw_text?: boolean
      /**
       * Indexing
       * @description Indexing configuration
       */
      indexing: {
        /**
         * Public Endpoint
         * @description The public endpoint of the Weaviate cluster.
         */
        host: string
        /**
         * Authentication
         * @description Authentication method
         */
        auth: OneOf<
          [
            {
              /**
               * Mode
               * @default token
               * @constant
               * @enum {string}
               */
              mode: 'token'
              /**
               * API Token
               * @description API Token for the Weaviate instance
               */
              token: string
            },
            {
              /**
               * Mode
               * @default username_password
               * @constant
               * @enum {string}
               */
              mode: 'username_password'
              /**
               * Username
               * @description Username for the Weaviate cluster
               */
              username: string
              /**
               * Password
               * @description Password for the Weaviate cluster
               */
              password: string
            },
            {
              /**
               * Mode
               * @default no_auth
               * @constant
               * @enum {string}
               */
              mode: 'no_auth'
            },
          ]
        >
        /**
         * Batch Size
         * @description The number of records to send to Weaviate in each batch
         * @default 128
         */
        batch_size?: number
        /**
         * Text Field
         * @description The field in the object that contains the embedded text
         * @default text
         */
        text_field?: string
        /**
         * Tenant ID
         * @description The tenant ID to use for multi tenancy
         * @default
         */
        tenant_id?: string
        /**
         * Default Vectorizer
         * @description The vectorizer to use if new classes need to be created
         * @default none
         * @enum {string}
         */
        default_vectorizer?:
          | 'none'
          | 'text2vec-cohere'
          | 'text2vec-huggingface'
          | 'text2vec-openai'
          | 'text2vec-palm'
          | 'text2vec-contextionary'
          | 'text2vec-transformers'
          | 'text2vec-gpt4all'
        /**
         * Additional headers
         * @description Additional HTTP headers to send with every request.
         * @default []
         */
        additional_headers?: {
          /** Header Key */
          header_key: string
          /** Header Value */
          value: string
        }[]
      }
    }
    /** Keen Spec */
    'destination-keen': {
      /**
       * Project ID
       * @description To get Keen Project ID, navigate to the Access tab from the left-hand, side panel and check the Project Details section.
       */
      project_id: string
      /**
       * API Key
       * @description To get Keen Master API Key, navigate to the Access tab from the left-hand, side panel and check the Project Details section.
       */
      api_key: string
      /**
       * Infer Timestamp
       * @description Allow connector to guess keen.timestamp value based on the streamed data.
       * @default true
       */
      infer_timestamp?: boolean
      /**
       * keen
       * @constant
       * @enum {string}
       */
      destinationType: 'keen'
    }
    /** Keen Spec */
    'destination-keen-update': {
      /**
       * Project ID
       * @description To get Keen Project ID, navigate to the Access tab from the left-hand, side panel and check the Project Details section.
       */
      project_id: string
      /**
       * API Key
       * @description To get Keen Master API Key, navigate to the Access tab from the left-hand, side panel and check the Project Details section.
       */
      api_key: string
      /**
       * Infer Timestamp
       * @description Allow connector to guess keen.timestamp value based on the streamed data.
       * @default true
       */
      infer_timestamp?: boolean
    }
    /** MongoDB Destination Spec */
    'destination-mongodb': {
      /**
       * MongoDb Instance Type
       * @description MongoDb instance to connect to. For MongoDB Atlas and Replica Set TLS connection is used by default.
       */
      instance_type?: OneOf<
        [
          {
            /**
             * @default standalone
             * @enum {string}
             */
            instance: 'standalone'
            /**
             * Host
             * @description The Host of a Mongo database to be replicated.
             */
            host: string
            /**
             * Port
             * @description The Port of a Mongo database to be replicated.
             * @default 27017
             */
            port: number
          },
          {
            /**
             * @default replica
             * @enum {string}
             */
            instance: 'replica'
            /**
             * Server addresses
             * @description The members of a replica set. Please specify `host`:`port` of each member seperated by comma.
             */
            server_addresses: string
            /**
             * Replica Set
             * @description A replica set name.
             */
            replica_set?: string
          },
          {
            /**
             * @default atlas
             * @enum {string}
             */
            instance: 'atlas'
            /**
             * Cluster URL
             * @description URL of a cluster to connect to.
             */
            cluster_url: string
          },
        ]
      >
      /**
       * DB Name
       * @description Name of the database.
       */
      database: string
      /**
       * Authorization type
       * @description Authorization type.
       */
      auth_type: OneOf<
        [
          {
            /**
             * @constant
             * @enum {string}
             */
            authorization: 'none'
          },
          {
            /**
             * @constant
             * @enum {string}
             */
            authorization: 'login/password'
            /**
             * User
             * @description Username to use to access the database.
             */
            username: string
            /**
             * Password
             * @description Password associated with the username.
             */
            password: string
          },
        ]
      >
      /**
       * SSH Tunnel Method
       * @description Whether to initiate an SSH tunnel before connecting to the database, and if so, which kind of authentication to use.
       */
      tunnel_method?: OneOf<
        [
          {
            /**
             * @description No ssh tunnel needed to connect to database
             * @constant
             * @enum {string}
             */
            tunnel_method: 'NO_TUNNEL'
          },
          {
            /**
             * @description Connect through a jump server tunnel host using username and ssh key
             * @constant
             * @enum {string}
             */
            tunnel_method: 'SSH_KEY_AUTH'
            /**
             * SSH Tunnel Jump Server Host
             * @description Hostname of the jump server host that allows inbound ssh tunnel.
             */
            tunnel_host: string
            /**
             * SSH Connection Port
             * @description Port on the proxy/jump server that accepts inbound ssh connections.
             * @default 22
             */
            tunnel_port: number
            /**
             * SSH Login Username
             * @description OS-level username for logging into the jump server host.
             */
            tunnel_user: string
            /**
             * SSH Private Key
             * @description OS-level user account ssh key credentials in RSA PEM format ( created with ssh-keygen -t rsa -m PEM -f myuser_rsa )
             */
            ssh_key: string
          },
          {
            /**
             * @description Connect through a jump server tunnel host using username and password authentication
             * @constant
             * @enum {string}
             */
            tunnel_method: 'SSH_PASSWORD_AUTH'
            /**
             * SSH Tunnel Jump Server Host
             * @description Hostname of the jump server host that allows inbound ssh tunnel.
             */
            tunnel_host: string
            /**
             * SSH Connection Port
             * @description Port on the proxy/jump server that accepts inbound ssh connections.
             * @default 22
             */
            tunnel_port: number
            /**
             * SSH Login Username
             * @description OS-level username for logging into the jump server host
             */
            tunnel_user: string
            /**
             * Password
             * @description OS-level password for logging into the jump server host
             */
            tunnel_user_password: string
          },
        ]
      >
      /**
       * mongodb
       * @constant
       * @enum {string}
       */
      destinationType: 'mongodb'
    }
    /** MongoDB Destination Spec */
    'destination-mongodb-update': {
      /**
       * MongoDb Instance Type
       * @description MongoDb instance to connect to. For MongoDB Atlas and Replica Set TLS connection is used by default.
       */
      instance_type?: OneOf<
        [
          {
            /**
             * @default standalone
             * @enum {string}
             */
            instance: 'standalone'
            /**
             * Host
             * @description The Host of a Mongo database to be replicated.
             */
            host: string
            /**
             * Port
             * @description The Port of a Mongo database to be replicated.
             * @default 27017
             */
            port: number
          },
          {
            /**
             * @default replica
             * @enum {string}
             */
            instance: 'replica'
            /**
             * Server addresses
             * @description The members of a replica set. Please specify `host`:`port` of each member seperated by comma.
             */
            server_addresses: string
            /**
             * Replica Set
             * @description A replica set name.
             */
            replica_set?: string
          },
          {
            /**
             * @default atlas
             * @enum {string}
             */
            instance: 'atlas'
            /**
             * Cluster URL
             * @description URL of a cluster to connect to.
             */
            cluster_url: string
          },
        ]
      >
      /**
       * DB Name
       * @description Name of the database.
       */
      database: string
      /**
       * Authorization type
       * @description Authorization type.
       */
      auth_type: OneOf<
        [
          {
            /**
             * @constant
             * @enum {string}
             */
            authorization: 'none'
          },
          {
            /**
             * @constant
             * @enum {string}
             */
            authorization: 'login/password'
            /**
             * User
             * @description Username to use to access the database.
             */
            username: string
            /**
             * Password
             * @description Password associated with the username.
             */
            password: string
          },
        ]
      >
      /**
       * SSH Tunnel Method
       * @description Whether to initiate an SSH tunnel before connecting to the database, and if so, which kind of authentication to use.
       */
      tunnel_method?: OneOf<
        [
          {
            /**
             * @description No ssh tunnel needed to connect to database
             * @constant
             * @enum {string}
             */
            tunnel_method: 'NO_TUNNEL'
          },
          {
            /**
             * @description Connect through a jump server tunnel host using username and ssh key
             * @constant
             * @enum {string}
             */
            tunnel_method: 'SSH_KEY_AUTH'
            /**
             * SSH Tunnel Jump Server Host
             * @description Hostname of the jump server host that allows inbound ssh tunnel.
             */
            tunnel_host: string
            /**
             * SSH Connection Port
             * @description Port on the proxy/jump server that accepts inbound ssh connections.
             * @default 22
             */
            tunnel_port: number
            /**
             * SSH Login Username
             * @description OS-level username for logging into the jump server host.
             */
            tunnel_user: string
            /**
             * SSH Private Key
             * @description OS-level user account ssh key credentials in RSA PEM format ( created with ssh-keygen -t rsa -m PEM -f myuser_rsa )
             */
            ssh_key: string
          },
          {
            /**
             * @description Connect through a jump server tunnel host using username and password authentication
             * @constant
             * @enum {string}
             */
            tunnel_method: 'SSH_PASSWORD_AUTH'
            /**
             * SSH Tunnel Jump Server Host
             * @description Hostname of the jump server host that allows inbound ssh tunnel.
             */
            tunnel_host: string
            /**
             * SSH Connection Port
             * @description Port on the proxy/jump server that accepts inbound ssh connections.
             * @default 22
             */
            tunnel_port: number
            /**
             * SSH Login Username
             * @description OS-level username for logging into the jump server host
             */
            tunnel_user: string
            /**
             * Password
             * @description OS-level password for logging into the jump server host
             */
            tunnel_user_password: string
          },
        ]
      >
    }
    /**
     * Vectara Config
     * @description Configuration to connect to the Vectara instance
     */
    'destination-vectara': {
      /**
       * OAuth2.0 Credentials
       * @description OAuth2.0 credentials used to authenticate admin actions (creating/deleting corpora)
       */
      oauth2: {
        /**
         * OAuth Client ID
         * @description OAuth2.0 client id
         */
        client_id: string
        /**
         * OAuth Client Secret
         * @description OAuth2.0 client secret
         */
        client_secret: string
      }
      /**
       * Customer ID
       * @description Your customer id as it is in the authenticaion url
       */
      customer_id: string
      /**
       * Corpus Name
       * @description The Name of Corpus to load data into
       */
      corpus_name: string
      /**
       * Parallelize
       * @description Parallelize indexing into Vectara with multiple threads
       * @default false
       */
      parallelize?: boolean
      /**
       * Text fields to index with Vectara
       * @description List of fields in the record that should be in the section of the document. The field list is applied to all streams in the same way and non-existing fields are ignored. If none are defined, all fields are considered text fields. When specifying text fields, you can access nested fields in the record by using dot notation, e.g. `user.name` will access the `name` field in the `user` object. It's also possible to use wildcards to access all fields in an object, e.g. `users.*.name` will access all `names` fields in all entries of the `users` array.
       * @default []
       */
      text_fields?: string[]
      /**
       * Text field to use as document title with Vectara
       * @description A field that will be used to populate the `title` of each document. The field list is applied to all streams in the same way and non-existing fields are ignored. If none are defined, all fields are considered text fields. When specifying text fields, you can access nested fields in the record by using dot notation, e.g. `user.name` will access the `name` field in the `user` object. It's also possible to use wildcards to access all fields in an object, e.g. `users.*.name` will access all `names` fields in all entries of the `users` array.
       * @default
       */
      title_field?: string
      /**
       * Fields to store as metadata
       * @description List of fields in the record that should be stored as metadata. The field list is applied to all streams in the same way and non-existing fields are ignored. If none are defined, all fields are considered metadata fields. When specifying text fields, you can access nested fields in the record by using dot notation, e.g. `user.name` will access the `name` field in the `user` object. It's also possible to use wildcards to access all fields in an object, e.g. `users.*.name` will access all `names` fields in all entries of the `users` array. When specifying nested paths, all matching values are flattened into an array set to a field named by the path.
       * @default []
       */
      metadata_fields?: string[]
      /**
       * vectara
       * @constant
       * @enum {string}
       */
      destinationType: 'vectara'
    }
    /**
     * Vectara Config
     * @description Configuration to connect to the Vectara instance
     */
    'destination-vectara-update': {
      /**
       * OAuth2.0 Credentials
       * @description OAuth2.0 credentials used to authenticate admin actions (creating/deleting corpora)
       */
      oauth2: {
        /**
         * OAuth Client ID
         * @description OAuth2.0 client id
         */
        client_id: string
        /**
         * OAuth Client Secret
         * @description OAuth2.0 client secret
         */
        client_secret: string
      }
      /**
       * Customer ID
       * @description Your customer id as it is in the authenticaion url
       */
      customer_id: string
      /**
       * Corpus Name
       * @description The Name of Corpus to load data into
       */
      corpus_name: string
      /**
       * Parallelize
       * @description Parallelize indexing into Vectara with multiple threads
       * @default false
       */
      parallelize?: boolean
      /**
       * Text fields to index with Vectara
       * @description List of fields in the record that should be in the section of the document. The field list is applied to all streams in the same way and non-existing fields are ignored. If none are defined, all fields are considered text fields. When specifying text fields, you can access nested fields in the record by using dot notation, e.g. `user.name` will access the `name` field in the `user` object. It's also possible to use wildcards to access all fields in an object, e.g. `users.*.name` will access all `names` fields in all entries of the `users` array.
       * @default []
       */
      text_fields?: string[]
      /**
       * Text field to use as document title with Vectara
       * @description A field that will be used to populate the `title` of each document. The field list is applied to all streams in the same way and non-existing fields are ignored. If none are defined, all fields are considered text fields. When specifying text fields, you can access nested fields in the record by using dot notation, e.g. `user.name` will access the `name` field in the `user` object. It's also possible to use wildcards to access all fields in an object, e.g. `users.*.name` will access all `names` fields in all entries of the `users` array.
       * @default
       */
      title_field?: string
      /**
       * Fields to store as metadata
       * @description List of fields in the record that should be stored as metadata. The field list is applied to all streams in the same way and non-existing fields are ignored. If none are defined, all fields are considered metadata fields. When specifying text fields, you can access nested fields in the record by using dot notation, e.g. `user.name` will access the `name` field in the `user` object. It's also possible to use wildcards to access all fields in an object, e.g. `users.*.name` will access all `names` fields in all entries of the `users` array. When specifying nested paths, all matching values are flattened into an array set to a field named by the path.
       * @default []
       */
      metadata_fields?: string[]
    }
    /** S3 Destination Spec */
    'destination-s3-glue': {
      /**
       * S3 Key ID
       * @description The access key ID to access the S3 bucket. Airbyte requires Read and Write permissions to the given bucket. Read more <a href="https://docs.aws.amazon.com/general/latest/gr/aws-sec-cred-types.html#access-keys-and-secret-access-keys">here</a>.
       */
      access_key_id?: string
      /**
       * S3 Access Key
       * @description The corresponding secret to the access key ID. Read more <a href="https://docs.aws.amazon.com/general/latest/gr/aws-sec-cred-types.html#access-keys-and-secret-access-keys">here</a>
       */
      secret_access_key?: string
      /**
       * S3 Bucket Name
       * @description The name of the S3 bucket. Read more <a href="https://docs.aws.amazon.com/AmazonS3/latest/userguide/create-bucket-overview.html">here</a>.
       */
      s3_bucket_name: string
      /**
       * S3 Bucket Path
       * @description Directory under the S3 bucket where data will be written. Read more <a href="https://docs.airbyte.com/integrations/destinations/s3#:~:text=to%20format%20the-,bucket%20path,-%3A">here</a>
       */
      s3_bucket_path: string
      /**
       * S3 Bucket Region
       * @description The region of the S3 bucket. See <a href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-regions-availability-zones.html#concepts-available-regions">here</a> for all region codes.
       * @default
       * @enum {string}
       */
      s3_bucket_region:
        | ''
        | 'af-south-1'
        | 'ap-east-1'
        | 'ap-northeast-1'
        | 'ap-northeast-2'
        | 'ap-northeast-3'
        | 'ap-south-1'
        | 'ap-south-2'
        | 'ap-southeast-1'
        | 'ap-southeast-2'
        | 'ap-southeast-3'
        | 'ap-southeast-4'
        | 'ca-central-1'
        | 'ca-west-1'
        | 'cn-north-1'
        | 'cn-northwest-1'
        | 'eu-central-1'
        | 'eu-central-2'
        | 'eu-north-1'
        | 'eu-south-1'
        | 'eu-south-2'
        | 'eu-west-1'
        | 'eu-west-2'
        | 'eu-west-3'
        | 'il-central-1'
        | 'me-central-1'
        | 'me-south-1'
        | 'sa-east-1'
        | 'us-east-1'
        | 'us-east-2'
        | 'us-gov-east-1'
        | 'us-gov-west-1'
        | 'us-west-1'
        | 'us-west-2'
      /**
       * Output Format
       * @description Format of the data output. See <a href="https://docs.airbyte.com/integrations/destinations/s3/#supported-output-schema">here</a> for more details
       */
      format: {
        /**
         * Format Type
         * @default JSONL
         * @enum {string}
         */
        format_type: 'JSONL'
        /**
         * Compression
         * @description Whether the output files should be compressed. If compression is selected, the output filename will have an extra extension (GZIP: ".jsonl.gz").
         */
        compression?: OneOf<
          [
            {
              /**
               * @default No Compression
               * @enum {string}
               */
              compression_type?: 'No Compression'
            },
            {
              /**
               * @default GZIP
               * @enum {string}
               */
              compression_type?: 'GZIP'
            },
          ]
        >
        /**
         * Flattening
         * @description Whether the input json data should be normalized (flattened) in the output JSON Lines. Please refer to docs for details.
         * @default Root level flattening
         * @enum {string}
         */
        flattening?: 'No flattening' | 'Root level flattening'
      }
      /**
       * Endpoint
       * @description Your S3 endpoint url. Read more <a href="https://docs.aws.amazon.com/general/latest/gr/s3.html#:~:text=Service%20endpoints-,Amazon%20S3%20endpoints,-When%20you%20use">here</a>
       * @default
       */
      s3_endpoint?: string
      /**
       * S3 Path Format
       * @description Format string on how data will be organized inside the S3 bucket directory. Read more <a href="https://docs.airbyte.com/integrations/destinations/s3#:~:text=The%20full%20path%20of%20the%20output%20data%20with%20the%20default%20S3%20path%20format">here</a>
       */
      s3_path_format?: string
      /**
       * S3 Filename pattern
       * @description The pattern allows you to set the file-name format for the S3 staging file(s)
       */
      file_name_pattern?: string
      /**
       * Glue database name
       * @description Name of the glue database for creating the tables, leave blank if no integration
       */
      glue_database: string
      /**
       * Serialization Library
       * @description The library that your query engine will use for reading and writing data in your lake.
       * @default org.openx.data.jsonserde.JsonSerDe
       * @enum {string}
       */
      glue_serialization_library:
        | 'org.openx.data.jsonserde.JsonSerDe'
        | 'org.apache.hive.hcatalog.data.JsonSerDe'
      /**
       * s3-glue
       * @constant
       * @enum {string}
       */
      destinationType: 's3-glue'
    }
    /** S3 Destination Spec */
    'destination-s3-glue-update': {
      /**
       * S3 Key ID
       * @description The access key ID to access the S3 bucket. Airbyte requires Read and Write permissions to the given bucket. Read more <a href="https://docs.aws.amazon.com/general/latest/gr/aws-sec-cred-types.html#access-keys-and-secret-access-keys">here</a>.
       */
      access_key_id?: string
      /**
       * S3 Access Key
       * @description The corresponding secret to the access key ID. Read more <a href="https://docs.aws.amazon.com/general/latest/gr/aws-sec-cred-types.html#access-keys-and-secret-access-keys">here</a>
       */
      secret_access_key?: string
      /**
       * S3 Bucket Name
       * @description The name of the S3 bucket. Read more <a href="https://docs.aws.amazon.com/AmazonS3/latest/userguide/create-bucket-overview.html">here</a>.
       */
      s3_bucket_name: string
      /**
       * S3 Bucket Path
       * @description Directory under the S3 bucket where data will be written. Read more <a href="https://docs.airbyte.com/integrations/destinations/s3#:~:text=to%20format%20the-,bucket%20path,-%3A">here</a>
       */
      s3_bucket_path: string
      /**
       * S3 Bucket Region
       * @description The region of the S3 bucket. See <a href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-regions-availability-zones.html#concepts-available-regions">here</a> for all region codes.
       * @default
       * @enum {string}
       */
      s3_bucket_region:
        | ''
        | 'af-south-1'
        | 'ap-east-1'
        | 'ap-northeast-1'
        | 'ap-northeast-2'
        | 'ap-northeast-3'
        | 'ap-south-1'
        | 'ap-south-2'
        | 'ap-southeast-1'
        | 'ap-southeast-2'
        | 'ap-southeast-3'
        | 'ap-southeast-4'
        | 'ca-central-1'
        | 'ca-west-1'
        | 'cn-north-1'
        | 'cn-northwest-1'
        | 'eu-central-1'
        | 'eu-central-2'
        | 'eu-north-1'
        | 'eu-south-1'
        | 'eu-south-2'
        | 'eu-west-1'
        | 'eu-west-2'
        | 'eu-west-3'
        | 'il-central-1'
        | 'me-central-1'
        | 'me-south-1'
        | 'sa-east-1'
        | 'us-east-1'
        | 'us-east-2'
        | 'us-gov-east-1'
        | 'us-gov-west-1'
        | 'us-west-1'
        | 'us-west-2'
      /**
       * Output Format
       * @description Format of the data output. See <a href="https://docs.airbyte.com/integrations/destinations/s3/#supported-output-schema">here</a> for more details
       */
      format: {
        /**
         * Format Type
         * @default JSONL
         * @enum {string}
         */
        format_type: 'JSONL'
        /**
         * Compression
         * @description Whether the output files should be compressed. If compression is selected, the output filename will have an extra extension (GZIP: ".jsonl.gz").
         */
        compression?: OneOf<
          [
            {
              /**
               * @default No Compression
               * @enum {string}
               */
              compression_type?: 'No Compression'
            },
            {
              /**
               * @default GZIP
               * @enum {string}
               */
              compression_type?: 'GZIP'
            },
          ]
        >
        /**
         * Flattening
         * @description Whether the input json data should be normalized (flattened) in the output JSON Lines. Please refer to docs for details.
         * @default Root level flattening
         * @enum {string}
         */
        flattening?: 'No flattening' | 'Root level flattening'
      }
      /**
       * Endpoint
       * @description Your S3 endpoint url. Read more <a href="https://docs.aws.amazon.com/general/latest/gr/s3.html#:~:text=Service%20endpoints-,Amazon%20S3%20endpoints,-When%20you%20use">here</a>
       * @default
       */
      s3_endpoint?: string
      /**
       * S3 Path Format
       * @description Format string on how data will be organized inside the S3 bucket directory. Read more <a href="https://docs.airbyte.com/integrations/destinations/s3#:~:text=The%20full%20path%20of%20the%20output%20data%20with%20the%20default%20S3%20path%20format">here</a>
       */
      s3_path_format?: string
      /**
       * S3 Filename pattern
       * @description The pattern allows you to set the file-name format for the S3 staging file(s)
       */
      file_name_pattern?: string
      /**
       * Glue database name
       * @description Name of the glue database for creating the tables, leave blank if no integration
       */
      glue_database: string
      /**
       * Serialization Library
       * @description The library that your query engine will use for reading and writing data in your lake.
       * @default org.openx.data.jsonserde.JsonSerDe
       * @enum {string}
       */
      glue_serialization_library:
        | 'org.openx.data.jsonserde.JsonSerDe'
        | 'org.apache.hive.hcatalog.data.JsonSerDe'
    }
    /** E2E Test (/dev/null) Destination Spec */
    'destination-dev-null': {
      /**
       * Test Destination
       * @description The type of destination to be used
       */
      test_destination: {
        /**
         * @default SILENT
         * @constant
         * @enum {string}
         */
        test_destination_type: 'SILENT'
      }
      /**
       * dev-null
       * @constant
       * @enum {string}
       */
      destinationType: 'dev-null'
    }
    /** E2E Test (/dev/null) Destination Spec */
    'destination-dev-null-update': {
      /**
       * Test Destination
       * @description The type of destination to be used
       */
      test_destination: {
        /**
         * @default SILENT
         * @constant
         * @enum {string}
         */
        test_destination_type: 'SILENT'
      }
    }
    /** Destination Timeplus */
    'destination-timeplus': {
      /**
       * Endpoint
       * @description Timeplus workspace endpoint
       * @default https://us.timeplus.cloud/<workspace_id>
       */
      endpoint: string
      /**
       * API key
       * @description Personal API key
       */
      apikey: string
      /**
       * timeplus
       * @constant
       * @enum {string}
       */
      destinationType: 'timeplus'
    }
    /** Destination Timeplus */
    'destination-timeplus-update': {
      /**
       * Endpoint
       * @description Timeplus workspace endpoint
       * @default https://us.timeplus.cloud/<workspace_id>
       */
      endpoint: string
      /**
       * API key
       * @description Personal API key
       */
      apikey: string
    }
    /** Destination Convex */
    'destination-convex': {
      /** @description URL of the Convex deployment that is the destination */
      deployment_url: string
      /** @description API access key used to send data to a Convex deployment. */
      access_key: string
      /**
       * convex
       * @constant
       * @enum {string}
       */
      destinationType: 'convex'
    }
    /** Destination Convex */
    'destination-convex-update': {
      /** @description URL of the Convex deployment that is the destination */
      deployment_url: string
      /** @description API access key used to send data to a Convex deployment. */
      access_key: string
    }
    /** Destination Google Firestore */
    'destination-firestore': {
      /**
       * Project ID
       * @description The GCP project ID for the project containing the target BigQuery dataset.
       */
      project_id: string
      /**
       * Credentials JSON
       * @description The contents of the JSON service account key. Check out the <a href="https://docs.airbyte.io/integrations/destinations/firestore">docs</a> if you need help generating this key. Default credentials will be used if this field is left empty.
       */
      credentials_json?: string
      /**
       * firestore
       * @constant
       * @enum {string}
       */
      destinationType: 'firestore'
    }
    /** Destination Google Firestore */
    'destination-firestore-update': {
      /**
       * Project ID
       * @description The GCP project ID for the project containing the target BigQuery dataset.
       */
      project_id: string
      /**
       * Credentials JSON
       * @description The contents of the JSON service account key. Check out the <a href="https://docs.airbyte.io/integrations/destinations/firestore">docs</a> if you need help generating this key. Default credentials will be used if this field is left empty.
       */
      credentials_json?: string
    }
    /** Redshift Destination Spec */
    'destination-redshift': {
      /**
       * Host
       * @description Host Endpoint of the Redshift Cluster (must include the cluster-id, region and end with .redshift.amazonaws.com)
       */
      host: string
      /**
       * Port
       * @description Port of the database.
       * @default 5439
       */
      port: number
      /**
       * Username
       * @description Username to use to access the database.
       */
      username: string
      /**
       * Password
       * @description Password associated with the username.
       */
      password: string
      /**
       * Database
       * @description Name of the database.
       */
      database: string
      /**
       * Default Schema
       * @description The default schema tables are written to if the source does not specify a namespace. Unless specifically configured, the usual value for this field is "public".
       * @default public
       */
      schema: string
      /**
       * JDBC URL Params
       * @description Additional properties to pass to the JDBC URL string when connecting to the database formatted as 'key=value' pairs separated by the symbol '&'. (example: key1=value1&key2=value2&key3=value3).
       */
      jdbc_url_params?: string
      /**
       * Uploading Method
       * @description The way data will be uploaded to Redshift.
       */
      uploading_method?: OneOf<
        [
          {
            /**
             * @constant
             * @enum {string}
             */
            method: 'S3 Staging'
            /**
             * S3 Bucket Name
             * @description The name of the staging S3 bucket.
             */
            s3_bucket_name: string
            /**
             * S3 Bucket Path
             * @description The directory under the S3 bucket where data will be written. If not provided, then defaults to the root directory. See <a href="https://docs.aws.amazon.com/prescriptive-guidance/latest/defining-bucket-names-data-lakes/faq.html#:~:text=be%20globally%20unique.-,For%20S3%20bucket%20paths,-%2C%20you%20can%20use">path's name recommendations</a> for more details.
             */
            s3_bucket_path?: string
            /**
             * S3 Bucket Region
             * @description The region of the S3 staging bucket.
             * @default
             * @enum {string}
             */
            s3_bucket_region:
              | ''
              | 'af-south-1'
              | 'ap-east-1'
              | 'ap-northeast-1'
              | 'ap-northeast-2'
              | 'ap-northeast-3'
              | 'ap-south-1'
              | 'ap-south-2'
              | 'ap-southeast-1'
              | 'ap-southeast-2'
              | 'ap-southeast-3'
              | 'ap-southeast-4'
              | 'ca-central-1'
              | 'ca-west-1'
              | 'cn-north-1'
              | 'cn-northwest-1'
              | 'eu-central-1'
              | 'eu-central-2'
              | 'eu-north-1'
              | 'eu-south-1'
              | 'eu-south-2'
              | 'eu-west-1'
              | 'eu-west-2'
              | 'eu-west-3'
              | 'il-central-1'
              | 'me-central-1'
              | 'me-south-1'
              | 'sa-east-1'
              | 'us-east-1'
              | 'us-east-2'
              | 'us-gov-east-1'
              | 'us-gov-west-1'
              | 'us-west-1'
              | 'us-west-2'
            /**
             * S3 Access Key Id
             * @description This ID grants access to the above S3 staging bucket. Airbyte requires Read and Write permissions to the given bucket. See <a href="https://docs.aws.amazon.com/general/latest/gr/aws-sec-cred-types.html#access-keys-and-secret-access-keys">AWS docs</a> on how to generate an access key ID and secret access key.
             */
            access_key_id: string
            /**
             * S3 Secret Access Key
             * @description The corresponding secret to the above access key id. See <a href="https://docs.aws.amazon.com/general/latest/gr/aws-sec-cred-types.html#access-keys-and-secret-access-keys">AWS docs</a> on how to generate an access key ID and secret access key.
             */
            secret_access_key: string
            /**
             * S3 Filename pattern
             * @description The pattern allows you to set the file-name format for the S3 staging file(s)
             */
            file_name_pattern?: string
            /**
             * Purge Staging Files and Tables
             * @description Whether to delete the staging files from S3 after completing the sync. See <a href="https://docs.airbyte.com/integrations/destinations/redshift/#:~:text=the%20root%20directory.-,Purge%20Staging%20Data,-Whether%20to%20delete"> docs</a> for details.
             * @default true
             */
            purge_staging_data?: boolean
            /**
             * Encryption
             * @description How to encrypt the staging data
             * @default {
             *   "encryption_type": "none"
             * }
             */
            encryption?: OneOf<
              [
                {
                  /**
                   * @default none
                   * @constant
                   * @enum {string}
                   */
                  encryption_type: 'none'
                },
                {
                  /**
                   * @default aes_cbc_envelope
                   * @constant
                   * @enum {string}
                   */
                  encryption_type: 'aes_cbc_envelope'
                  /**
                   * Key
                   * @description The key, base64-encoded. Must be either 128, 192, or 256 bits. Leave blank to have Airbyte generate an ephemeral key for each sync.
                   */
                  key_encrypting_key?: string
                },
              ]
            >
            /**
             * File Buffer Count
             * @description Number of file buffers allocated for writing data. Increasing this number is beneficial for connections using Change Data Capture (CDC) and up to the number of streams within a connection. Increasing the number of file buffers past the maximum number of streams has deteriorating effects
             * @default 10
             */
            file_buffer_count?: number
          },
          {
            /**
             * @constant
             * @enum {string}
             */
            method: 'Standard'
          },
        ]
      >
      /**
       * Destinations V2 Raw Table Schema
       * @description The schema to write raw tables into
       */
      raw_data_schema?: string
      /**
       * Enable Loading Data Incrementally to Final Tables
       * @description When enabled your data will load into your final tables incrementally while your data is still being synced. When Disabled (the default), your data loads into your final tables once at the end of a sync. Note that this option only applies if you elect to create Final tables
       * @default false
       */
      enable_incremental_final_table_updates?: boolean
      /**
       * Disable Final Tables. (WARNING! Unstable option; Columns in raw table schema might change between versions)
       * @description Disable Writing Final Tables. WARNING! The data format in _airbyte_data is likely stable but there are no guarantees that other metadata columns will remain the same in future versions
       * @default false
       */
      disable_type_dedupe?: boolean
      /**
       * SSH Tunnel Method
       * @description Whether to initiate an SSH tunnel before connecting to the database, and if so, which kind of authentication to use.
       */
      tunnel_method?: OneOf<
        [
          {
            /**
             * @description No ssh tunnel needed to connect to database
             * @constant
             * @enum {string}
             */
            tunnel_method: 'NO_TUNNEL'
          },
          {
            /**
             * @description Connect through a jump server tunnel host using username and ssh key
             * @constant
             * @enum {string}
             */
            tunnel_method: 'SSH_KEY_AUTH'
            /**
             * SSH Tunnel Jump Server Host
             * @description Hostname of the jump server host that allows inbound ssh tunnel.
             */
            tunnel_host: string
            /**
             * SSH Connection Port
             * @description Port on the proxy/jump server that accepts inbound ssh connections.
             * @default 22
             */
            tunnel_port: number
            /**
             * SSH Login Username
             * @description OS-level username for logging into the jump server host.
             */
            tunnel_user: string
            /**
             * SSH Private Key
             * @description OS-level user account ssh key credentials in RSA PEM format ( created with ssh-keygen -t rsa -m PEM -f myuser_rsa )
             */
            ssh_key: string
          },
          {
            /**
             * @description Connect through a jump server tunnel host using username and password authentication
             * @constant
             * @enum {string}
             */
            tunnel_method: 'SSH_PASSWORD_AUTH'
            /**
             * SSH Tunnel Jump Server Host
             * @description Hostname of the jump server host that allows inbound ssh tunnel.
             */
            tunnel_host: string
            /**
             * SSH Connection Port
             * @description Port on the proxy/jump server that accepts inbound ssh connections.
             * @default 22
             */
            tunnel_port: number
            /**
             * SSH Login Username
             * @description OS-level username for logging into the jump server host
             */
            tunnel_user: string
            /**
             * Password
             * @description OS-level password for logging into the jump server host
             */
            tunnel_user_password: string
          },
        ]
      >
      /**
       * redshift
       * @constant
       * @enum {string}
       */
      destinationType: 'redshift'
    }
    /** Redshift Destination Spec */
    'destination-redshift-update': {
      /**
       * Host
       * @description Host Endpoint of the Redshift Cluster (must include the cluster-id, region and end with .redshift.amazonaws.com)
       */
      host: string
      /**
       * Port
       * @description Port of the database.
       * @default 5439
       */
      port: number
      /**
       * Username
       * @description Username to use to access the database.
       */
      username: string
      /**
       * Password
       * @description Password associated with the username.
       */
      password: string
      /**
       * Database
       * @description Name of the database.
       */
      database: string
      /**
       * Default Schema
       * @description The default schema tables are written to if the source does not specify a namespace. Unless specifically configured, the usual value for this field is "public".
       * @default public
       */
      schema: string
      /**
       * JDBC URL Params
       * @description Additional properties to pass to the JDBC URL string when connecting to the database formatted as 'key=value' pairs separated by the symbol '&'. (example: key1=value1&key2=value2&key3=value3).
       */
      jdbc_url_params?: string
      /**
       * Uploading Method
       * @description The way data will be uploaded to Redshift.
       */
      uploading_method?: OneOf<
        [
          {
            /**
             * @constant
             * @enum {string}
             */
            method: 'S3 Staging'
            /**
             * S3 Bucket Name
             * @description The name of the staging S3 bucket.
             */
            s3_bucket_name: string
            /**
             * S3 Bucket Path
             * @description The directory under the S3 bucket where data will be written. If not provided, then defaults to the root directory. See <a href="https://docs.aws.amazon.com/prescriptive-guidance/latest/defining-bucket-names-data-lakes/faq.html#:~:text=be%20globally%20unique.-,For%20S3%20bucket%20paths,-%2C%20you%20can%20use">path's name recommendations</a> for more details.
             */
            s3_bucket_path?: string
            /**
             * S3 Bucket Region
             * @description The region of the S3 staging bucket.
             * @default
             * @enum {string}
             */
            s3_bucket_region:
              | ''
              | 'af-south-1'
              | 'ap-east-1'
              | 'ap-northeast-1'
              | 'ap-northeast-2'
              | 'ap-northeast-3'
              | 'ap-south-1'
              | 'ap-south-2'
              | 'ap-southeast-1'
              | 'ap-southeast-2'
              | 'ap-southeast-3'
              | 'ap-southeast-4'
              | 'ca-central-1'
              | 'ca-west-1'
              | 'cn-north-1'
              | 'cn-northwest-1'
              | 'eu-central-1'
              | 'eu-central-2'
              | 'eu-north-1'
              | 'eu-south-1'
              | 'eu-south-2'
              | 'eu-west-1'
              | 'eu-west-2'
              | 'eu-west-3'
              | 'il-central-1'
              | 'me-central-1'
              | 'me-south-1'
              | 'sa-east-1'
              | 'us-east-1'
              | 'us-east-2'
              | 'us-gov-east-1'
              | 'us-gov-west-1'
              | 'us-west-1'
              | 'us-west-2'
            /**
             * S3 Access Key Id
             * @description This ID grants access to the above S3 staging bucket. Airbyte requires Read and Write permissions to the given bucket. See <a href="https://docs.aws.amazon.com/general/latest/gr/aws-sec-cred-types.html#access-keys-and-secret-access-keys">AWS docs</a> on how to generate an access key ID and secret access key.
             */
            access_key_id: string
            /**
             * S3 Secret Access Key
             * @description The corresponding secret to the above access key id. See <a href="https://docs.aws.amazon.com/general/latest/gr/aws-sec-cred-types.html#access-keys-and-secret-access-keys">AWS docs</a> on how to generate an access key ID and secret access key.
             */
            secret_access_key: string
            /**
             * S3 Filename pattern
             * @description The pattern allows you to set the file-name format for the S3 staging file(s)
             */
            file_name_pattern?: string
            /**
             * Purge Staging Files and Tables
             * @description Whether to delete the staging files from S3 after completing the sync. See <a href="https://docs.airbyte.com/integrations/destinations/redshift/#:~:text=the%20root%20directory.-,Purge%20Staging%20Data,-Whether%20to%20delete"> docs</a> for details.
             * @default true
             */
            purge_staging_data?: boolean
            /**
             * Encryption
             * @description How to encrypt the staging data
             * @default {
             *   "encryption_type": "none"
             * }
             */
            encryption?: OneOf<
              [
                {
                  /**
                   * @default none
                   * @constant
                   * @enum {string}
                   */
                  encryption_type: 'none'
                },
                {
                  /**
                   * @default aes_cbc_envelope
                   * @constant
                   * @enum {string}
                   */
                  encryption_type: 'aes_cbc_envelope'
                  /**
                   * Key
                   * @description The key, base64-encoded. Must be either 128, 192, or 256 bits. Leave blank to have Airbyte generate an ephemeral key for each sync.
                   */
                  key_encrypting_key?: string
                },
              ]
            >
            /**
             * File Buffer Count
             * @description Number of file buffers allocated for writing data. Increasing this number is beneficial for connections using Change Data Capture (CDC) and up to the number of streams within a connection. Increasing the number of file buffers past the maximum number of streams has deteriorating effects
             * @default 10
             */
            file_buffer_count?: number
          },
          {
            /**
             * @constant
             * @enum {string}
             */
            method: 'Standard'
          },
        ]
      >
      /**
       * Destinations V2 Raw Table Schema
       * @description The schema to write raw tables into
       */
      raw_data_schema?: string
      /**
       * Enable Loading Data Incrementally to Final Tables
       * @description When enabled your data will load into your final tables incrementally while your data is still being synced. When Disabled (the default), your data loads into your final tables once at the end of a sync. Note that this option only applies if you elect to create Final tables
       * @default false
       */
      enable_incremental_final_table_updates?: boolean
      /**
       * Disable Final Tables. (WARNING! Unstable option; Columns in raw table schema might change between versions)
       * @description Disable Writing Final Tables. WARNING! The data format in _airbyte_data is likely stable but there are no guarantees that other metadata columns will remain the same in future versions
       * @default false
       */
      disable_type_dedupe?: boolean
      /**
       * SSH Tunnel Method
       * @description Whether to initiate an SSH tunnel before connecting to the database, and if so, which kind of authentication to use.
       */
      tunnel_method?: OneOf<
        [
          {
            /**
             * @description No ssh tunnel needed to connect to database
             * @constant
             * @enum {string}
             */
            tunnel_method: 'NO_TUNNEL'
          },
          {
            /**
             * @description Connect through a jump server tunnel host using username and ssh key
             * @constant
             * @enum {string}
             */
            tunnel_method: 'SSH_KEY_AUTH'
            /**
             * SSH Tunnel Jump Server Host
             * @description Hostname of the jump server host that allows inbound ssh tunnel.
             */
            tunnel_host: string
            /**
             * SSH Connection Port
             * @description Port on the proxy/jump server that accepts inbound ssh connections.
             * @default 22
             */
            tunnel_port: number
            /**
             * SSH Login Username
             * @description OS-level username for logging into the jump server host.
             */
            tunnel_user: string
            /**
             * SSH Private Key
             * @description OS-level user account ssh key credentials in RSA PEM format ( created with ssh-keygen -t rsa -m PEM -f myuser_rsa )
             */
            ssh_key: string
          },
          {
            /**
             * @description Connect through a jump server tunnel host using username and password authentication
             * @constant
             * @enum {string}
             */
            tunnel_method: 'SSH_PASSWORD_AUTH'
            /**
             * SSH Tunnel Jump Server Host
             * @description Hostname of the jump server host that allows inbound ssh tunnel.
             */
            tunnel_host: string
            /**
             * SSH Connection Port
             * @description Port on the proxy/jump server that accepts inbound ssh connections.
             * @default 22
             */
            tunnel_port: number
            /**
             * SSH Login Username
             * @description OS-level username for logging into the jump server host
             */
            tunnel_user: string
            /**
             * Password
             * @description OS-level password for logging into the jump server host
             */
            tunnel_user_password: string
          },
        ]
      >
    }
    /** DynamoDB Destination Spec */
    'destination-dynamodb': {
      /**
       * Endpoint
       * @description This is your DynamoDB endpoint url.(if you are working with AWS DynamoDB, just leave empty).
       * @default
       */
      dynamodb_endpoint?: string
      /**
       * Table name prefix
       * @description The prefix to use when naming DynamoDB tables.
       */
      dynamodb_table_name_prefix: string
      /**
       * DynamoDB Region
       * @description The region of the DynamoDB.
       * @default
       * @enum {string}
       */
      dynamodb_region:
        | ''
        | 'af-south-1'
        | 'ap-east-1'
        | 'ap-northeast-1'
        | 'ap-northeast-2'
        | 'ap-northeast-3'
        | 'ap-south-1'
        | 'ap-south-2'
        | 'ap-southeast-1'
        | 'ap-southeast-2'
        | 'ap-southeast-3'
        | 'ap-southeast-4'
        | 'ca-central-1'
        | 'ca-west-1'
        | 'cn-north-1'
        | 'cn-northwest-1'
        | 'eu-central-1'
        | 'eu-central-2'
        | 'eu-north-1'
        | 'eu-south-1'
        | 'eu-south-2'
        | 'eu-west-1'
        | 'eu-west-2'
        | 'eu-west-3'
        | 'il-central-1'
        | 'me-central-1'
        | 'me-south-1'
        | 'sa-east-1'
        | 'us-east-1'
        | 'us-east-2'
        | 'us-gov-east-1'
        | 'us-gov-west-1'
        | 'us-west-1'
        | 'us-west-2'
      /**
       * DynamoDB Key Id
       * @description The access key id to access the DynamoDB. Airbyte requires Read and Write permissions to the DynamoDB.
       */
      access_key_id: string
      /**
       * DynamoDB Access Key
       * @description The corresponding secret to the access key id.
       */
      secret_access_key: string
      /**
       * dynamodb
       * @constant
       * @enum {string}
       */
      destinationType: 'dynamodb'
    }
    /** DynamoDB Destination Spec */
    'destination-dynamodb-update': {
      /**
       * Endpoint
       * @description This is your DynamoDB endpoint url.(if you are working with AWS DynamoDB, just leave empty).
       * @default
       */
      dynamodb_endpoint?: string
      /**
       * Table name prefix
       * @description The prefix to use when naming DynamoDB tables.
       */
      dynamodb_table_name_prefix: string
      /**
       * DynamoDB Region
       * @description The region of the DynamoDB.
       * @default
       * @enum {string}
       */
      dynamodb_region:
        | ''
        | 'af-south-1'
        | 'ap-east-1'
        | 'ap-northeast-1'
        | 'ap-northeast-2'
        | 'ap-northeast-3'
        | 'ap-south-1'
        | 'ap-south-2'
        | 'ap-southeast-1'
        | 'ap-southeast-2'
        | 'ap-southeast-3'
        | 'ap-southeast-4'
        | 'ca-central-1'
        | 'ca-west-1'
        | 'cn-north-1'
        | 'cn-northwest-1'
        | 'eu-central-1'
        | 'eu-central-2'
        | 'eu-north-1'
        | 'eu-south-1'
        | 'eu-south-2'
        | 'eu-west-1'
        | 'eu-west-2'
        | 'eu-west-3'
        | 'il-central-1'
        | 'me-central-1'
        | 'me-south-1'
        | 'sa-east-1'
        | 'us-east-1'
        | 'us-east-2'
        | 'us-gov-east-1'
        | 'us-gov-west-1'
        | 'us-west-1'
        | 'us-west-2'
      /**
       * DynamoDB Key Id
       * @description The access key id to access the DynamoDB. Airbyte requires Read and Write permissions to the DynamoDB.
       */
      access_key_id: string
      /**
       * DynamoDB Access Key
       * @description The corresponding secret to the access key id.
       */
      secret_access_key: string
    }
    /**
     * Destination Config
     * @description The configuration model for the Vector DB based destinations. This model is used to generate the UI for the destination configuration,
     * as well as to provide type safety for the configuration passed to the destination.
     *
     * The configuration model is composed of four parts:
     * * Processing configuration
     * * Embedding configuration
     * * Indexing configuration
     * * Advanced configuration
     *
     * Processing, embedding and advanced configuration are provided by this base class, while the indexing configuration is provided by the destination connector in the sub class.
     */
    'destination-qdrant': {
      /**
       * Embedding
       * @description Embedding configuration
       */
      embedding: OneOf<
        [
          {
            /**
             * Mode
             * @default openai
             * @constant
             * @enum {string}
             */
            mode: 'openai'
            /** OpenAI API key */
            openai_key: string
          },
          {
            /**
             * Mode
             * @default cohere
             * @constant
             * @enum {string}
             */
            mode: 'cohere'
            /** Cohere API key */
            cohere_key: string
          },
          {
            /**
             * Mode
             * @default fake
             * @constant
             * @enum {string}
             */
            mode: 'fake'
          },
          {
            /**
             * Mode
             * @default azure_openai
             * @constant
             * @enum {string}
             */
            mode: 'azure_openai'
            /**
             * Azure OpenAI API key
             * @description The API key for your Azure OpenAI resource.  You can find this in the Azure portal under your Azure OpenAI resource
             */
            openai_key: string
            /**
             * Resource base URL
             * @description The base URL for your Azure OpenAI resource.  You can find this in the Azure portal under your Azure OpenAI resource
             */
            api_base: string
            /**
             * Deployment
             * @description The deployment for your Azure OpenAI resource.  You can find this in the Azure portal under your Azure OpenAI resource
             */
            deployment: string
          },
          {
            /**
             * Mode
             * @default openai_compatible
             * @constant
             * @enum {string}
             */
            mode: 'openai_compatible'
            /**
             * API key
             * @default
             */
            api_key?: string
            /**
             * Base URL
             * @description The base URL for your OpenAI-compatible service
             */
            base_url: string
            /**
             * Model name
             * @description The name of the model to use for embedding
             * @default text-embedding-ada-002
             */
            model_name?: string
            /**
             * Embedding dimensions
             * @description The number of dimensions the embedding model is generating
             */
            dimensions: number
          },
        ]
      >
      /** ProcessingConfigModel */
      processing: {
        /**
         * Chunk size
         * @description Size of chunks in tokens to store in vector store (make sure it is not too big for the context if your LLM)
         */
        chunk_size: number
        /**
         * Chunk overlap
         * @description Size of overlap between chunks in tokens to store in vector store to better capture relevant context
         * @default 0
         */
        chunk_overlap?: number
        /**
         * Text fields to embed
         * @description List of fields in the record that should be used to calculate the embedding. The field list is applied to all streams in the same way and non-existing fields are ignored. If none are defined, all fields are considered text fields. When specifying text fields, you can access nested fields in the record by using dot notation, e.g. `user.name` will access the `name` field in the `user` object. It's also possible to use wildcards to access all fields in an object, e.g. `users.*.name` will access all `names` fields in all entries of the `users` array.
         * @default []
         */
        text_fields?: string[]
        /**
         * Fields to store as metadata
         * @description List of fields in the record that should be stored as metadata. The field list is applied to all streams in the same way and non-existing fields are ignored. If none are defined, all fields are considered metadata fields. When specifying text fields, you can access nested fields in the record by using dot notation, e.g. `user.name` will access the `name` field in the `user` object. It's also possible to use wildcards to access all fields in an object, e.g. `users.*.name` will access all `names` fields in all entries of the `users` array. When specifying nested paths, all matching values are flattened into an array set to a field named by the path.
         * @default []
         */
        metadata_fields?: string[]
        /**
         * Text splitter
         * @description Split text fields into chunks based on the specified method.
         */
        text_splitter?: OneOf<
          [
            {
              /**
               * Mode
               * @default separator
               * @constant
               * @enum {string}
               */
              mode: 'separator'
              /**
               * Separators
               * @description List of separator strings to split text fields by. The separator itself needs to be wrapped in double quotes, e.g. to split by the dot character, use ".". To split by a newline, use "\n".
               * @default [
               *   "\"\\n\\n\"",
               *   "\"\\n\"",
               *   "\" \"",
               *   "\"\""
               * ]
               */
              separators?: string[]
              /**
               * Keep separator
               * @description Whether to keep the separator in the resulting chunks
               * @default false
               */
              keep_separator?: boolean
            },
            {
              /**
               * Mode
               * @default markdown
               * @constant
               * @enum {string}
               */
              mode: 'markdown'
              /**
               * Split level
               * @description Level of markdown headers to split text fields by. Headings down to the specified level will be used as split points
               * @default 1
               */
              split_level?: number
            },
            {
              /**
               * Mode
               * @default code
               * @constant
               * @enum {string}
               */
              mode: 'code'
              /**
               * Language
               * @description Split code in suitable places based on the programming language
               * @enum {string}
               */
              language:
                | 'cpp'
                | 'go'
                | 'java'
                | 'js'
                | 'php'
                | 'proto'
                | 'python'
                | 'rst'
                | 'ruby'
                | 'rust'
                | 'scala'
                | 'swift'
                | 'markdown'
                | 'latex'
                | 'html'
                | 'sol'
            },
          ]
        >
        /**
         * Field name mappings
         * @description List of fields to rename. Not applicable for nested fields, but can be used to rename fields already flattened via dot notation.
         * @default []
         */
        field_name_mappings?: {
          /**
           * From field name
           * @description The field name in the source
           */
          from_field: string
          /**
           * To field name
           * @description The field name to use in the destination
           */
          to_field: string
        }[]
      }
      /**
       * Do not store raw text
       * @description Do not store the text that gets embedded along with the vector and the metadata in the destination. If set to true, only the vector and the metadata will be stored - in this case raw text for LLM use cases needs to be retrieved from another source.
       * @default false
       */
      omit_raw_text?: boolean
      /**
       * Indexing
       * @description Indexing configuration
       */
      indexing: {
        /**
         * Public Endpoint
         * @description Public Endpoint of the Qdrant cluser
         */
        url: string
        /**
         * Authentication Method
         * @description Method to authenticate with the Qdrant Instance
         * @default api_key_auth
         */
        auth_method?: OneOf<
          [
            {
              /**
               * Mode
               * @default api_key_auth
               * @constant
               * @enum {string}
               */
              mode?: 'api_key_auth'
              /**
               * API Key
               * @description API Key for the Qdrant instance
               */
              api_key: string
            },
            {
              /**
               * Mode
               * @default no_auth
               * @constant
               * @enum {string}
               */
              mode?: 'no_auth'
            },
          ]
        >
        /**
         * Prefer gRPC
         * @description Whether to prefer gRPC over HTTP. Set to true for Qdrant cloud clusters
         * @default true
         */
        prefer_grpc?: boolean
        /**
         * Collection Name
         * @description The collection to load data into
         */
        collection: string
        /**
         * Distance Metric
         * @description The Distance metric used to measure similarities among vectors. This field is only used if the collection defined in the does not exist yet and is created automatically by the connector.
         * @default cos
         * @enum {string}
         */
        distance_metric?: 'dot' | 'cos' | 'euc'
        /**
         * Text Field
         * @description The field in the payload that contains the embedded text
         * @default text
         */
        text_field?: string
      }
      /**
       * qdrant
       * @constant
       * @enum {string}
       */
      destinationType: 'qdrant'
    }
    /**
     * Destination Config
     * @description The configuration model for the Vector DB based destinations. This model is used to generate the UI for the destination configuration,
     * as well as to provide type safety for the configuration passed to the destination.
     *
     * The configuration model is composed of four parts:
     * * Processing configuration
     * * Embedding configuration
     * * Indexing configuration
     * * Advanced configuration
     *
     * Processing, embedding and advanced configuration are provided by this base class, while the indexing configuration is provided by the destination connector in the sub class.
     */
    'destination-qdrant-update': {
      /**
       * Embedding
       * @description Embedding configuration
       */
      embedding: OneOf<
        [
          {
            /**
             * Mode
             * @default openai
             * @constant
             * @enum {string}
             */
            mode: 'openai'
            /** OpenAI API key */
            openai_key: string
          },
          {
            /**
             * Mode
             * @default cohere
             * @constant
             * @enum {string}
             */
            mode: 'cohere'
            /** Cohere API key */
            cohere_key: string
          },
          {
            /**
             * Mode
             * @default fake
             * @constant
             * @enum {string}
             */
            mode: 'fake'
          },
          {
            /**
             * Mode
             * @default azure_openai
             * @constant
             * @enum {string}
             */
            mode: 'azure_openai'
            /**
             * Azure OpenAI API key
             * @description The API key for your Azure OpenAI resource.  You can find this in the Azure portal under your Azure OpenAI resource
             */
            openai_key: string
            /**
             * Resource base URL
             * @description The base URL for your Azure OpenAI resource.  You can find this in the Azure portal under your Azure OpenAI resource
             */
            api_base: string
            /**
             * Deployment
             * @description The deployment for your Azure OpenAI resource.  You can find this in the Azure portal under your Azure OpenAI resource
             */
            deployment: string
          },
          {
            /**
             * Mode
             * @default openai_compatible
             * @constant
             * @enum {string}
             */
            mode: 'openai_compatible'
            /**
             * API key
             * @default
             */
            api_key?: string
            /**
             * Base URL
             * @description The base URL for your OpenAI-compatible service
             */
            base_url: string
            /**
             * Model name
             * @description The name of the model to use for embedding
             * @default text-embedding-ada-002
             */
            model_name?: string
            /**
             * Embedding dimensions
             * @description The number of dimensions the embedding model is generating
             */
            dimensions: number
          },
        ]
      >
      /** ProcessingConfigModel */
      processing: {
        /**
         * Chunk size
         * @description Size of chunks in tokens to store in vector store (make sure it is not too big for the context if your LLM)
         */
        chunk_size: number
        /**
         * Chunk overlap
         * @description Size of overlap between chunks in tokens to store in vector store to better capture relevant context
         * @default 0
         */
        chunk_overlap?: number
        /**
         * Text fields to embed
         * @description List of fields in the record that should be used to calculate the embedding. The field list is applied to all streams in the same way and non-existing fields are ignored. If none are defined, all fields are considered text fields. When specifying text fields, you can access nested fields in the record by using dot notation, e.g. `user.name` will access the `name` field in the `user` object. It's also possible to use wildcards to access all fields in an object, e.g. `users.*.name` will access all `names` fields in all entries of the `users` array.
         * @default []
         */
        text_fields?: string[]
        /**
         * Fields to store as metadata
         * @description List of fields in the record that should be stored as metadata. The field list is applied to all streams in the same way and non-existing fields are ignored. If none are defined, all fields are considered metadata fields. When specifying text fields, you can access nested fields in the record by using dot notation, e.g. `user.name` will access the `name` field in the `user` object. It's also possible to use wildcards to access all fields in an object, e.g. `users.*.name` will access all `names` fields in all entries of the `users` array. When specifying nested paths, all matching values are flattened into an array set to a field named by the path.
         * @default []
         */
        metadata_fields?: string[]
        /**
         * Text splitter
         * @description Split text fields into chunks based on the specified method.
         */
        text_splitter?: OneOf<
          [
            {
              /**
               * Mode
               * @default separator
               * @constant
               * @enum {string}
               */
              mode: 'separator'
              /**
               * Separators
               * @description List of separator strings to split text fields by. The separator itself needs to be wrapped in double quotes, e.g. to split by the dot character, use ".". To split by a newline, use "\n".
               * @default [
               *   "\"\\n\\n\"",
               *   "\"\\n\"",
               *   "\" \"",
               *   "\"\""
               * ]
               */
              separators?: string[]
              /**
               * Keep separator
               * @description Whether to keep the separator in the resulting chunks
               * @default false
               */
              keep_separator?: boolean
            },
            {
              /**
               * Mode
               * @default markdown
               * @constant
               * @enum {string}
               */
              mode: 'markdown'
              /**
               * Split level
               * @description Level of markdown headers to split text fields by. Headings down to the specified level will be used as split points
               * @default 1
               */
              split_level?: number
            },
            {
              /**
               * Mode
               * @default code
               * @constant
               * @enum {string}
               */
              mode: 'code'
              /**
               * Language
               * @description Split code in suitable places based on the programming language
               * @enum {string}
               */
              language:
                | 'cpp'
                | 'go'
                | 'java'
                | 'js'
                | 'php'
                | 'proto'
                | 'python'
                | 'rst'
                | 'ruby'
                | 'rust'
                | 'scala'
                | 'swift'
                | 'markdown'
                | 'latex'
                | 'html'
                | 'sol'
            },
          ]
        >
        /**
         * Field name mappings
         * @description List of fields to rename. Not applicable for nested fields, but can be used to rename fields already flattened via dot notation.
         * @default []
         */
        field_name_mappings?: {
          /**
           * From field name
           * @description The field name in the source
           */
          from_field: string
          /**
           * To field name
           * @description The field name to use in the destination
           */
          to_field: string
        }[]
      }
      /**
       * Do not store raw text
       * @description Do not store the text that gets embedded along with the vector and the metadata in the destination. If set to true, only the vector and the metadata will be stored - in this case raw text for LLM use cases needs to be retrieved from another source.
       * @default false
       */
      omit_raw_text?: boolean
      /**
       * Indexing
       * @description Indexing configuration
       */
      indexing: {
        /**
         * Public Endpoint
         * @description Public Endpoint of the Qdrant cluser
         */
        url: string
        /**
         * Authentication Method
         * @description Method to authenticate with the Qdrant Instance
         * @default api_key_auth
         */
        auth_method?: OneOf<
          [
            {
              /**
               * Mode
               * @default api_key_auth
               * @constant
               * @enum {string}
               */
              mode?: 'api_key_auth'
              /**
               * API Key
               * @description API Key for the Qdrant instance
               */
              api_key: string
            },
            {
              /**
               * Mode
               * @default no_auth
               * @constant
               * @enum {string}
               */
              mode?: 'no_auth'
            },
          ]
        >
        /**
         * Prefer gRPC
         * @description Whether to prefer gRPC over HTTP. Set to true for Qdrant cloud clusters
         * @default true
         */
        prefer_grpc?: boolean
        /**
         * Collection Name
         * @description The collection to load data into
         */
        collection: string
        /**
         * Distance Metric
         * @description The Distance metric used to measure similarities among vectors. This field is only used if the collection defined in the does not exist yet and is created automatically by the connector.
         * @default cos
         * @enum {string}
         */
        distance_metric?: 'dot' | 'cos' | 'euc'
        /**
         * Text Field
         * @description The field in the payload that contains the embedded text
         * @default text
         */
        text_field?: string
      }
    }
    /** Snowflake Destination Spec */
    'destination-snowflake': {
      /**
       * Host
       * @description Enter your Snowflake account's <a href="https://docs.snowflake.com/en/user-guide/admin-account-identifier.html#using-an-account-locator-as-an-identifier">locator</a> (in the format <account_locator>.<region>.<cloud>.snowflakecomputing.com)
       */
      host: string
      /**
       * Role
       * @description Enter the <a href="https://docs.snowflake.com/en/user-guide/security-access-control-overview.html#roles">role</a> that you want to use to access Snowflake
       */
      role: string
      /**
       * Warehouse
       * @description Enter the name of the <a href="https://docs.snowflake.com/en/user-guide/warehouses-overview.html#overview-of-warehouses">warehouse</a> that you want to sync data into
       */
      warehouse: string
      /**
       * Database
       * @description Enter the name of the <a href="https://docs.snowflake.com/en/sql-reference/ddl-database.html#database-schema-share-ddl">database</a> you want to sync data into
       */
      database: string
      /**
       * Default Schema
       * @description Enter the name of the default <a href="https://docs.snowflake.com/en/sql-reference/ddl-database.html#database-schema-share-ddl">schema</a>
       */
      schema: string
      /**
       * Username
       * @description Enter the name of the user you want to use to access the database
       */
      username: string
      /** Authorization Method */
      credentials?: OneOf<
        [
          {
            /**
             * @default Key Pair Authentication
             * @constant
             * @enum {string}
             */
            auth_type?: 'Key Pair Authentication'
            /**
             * Private Key
             * @description RSA Private key to use for Snowflake connection. See the <a href="https://docs.airbyte.com/integrations/destinations/snowflake">docs</a> for more information on how to obtain this key.
             */
            private_key: string
            /**
             * Passphrase
             * @description Passphrase for private key
             */
            private_key_password?: string
          },
          {
            /**
             * @default Username and Password
             * @constant
             * @enum {string}
             */
            auth_type?: 'Username and Password'
            /**
             * Password
             * @description Enter the password associated with the username.
             */
            password: string
          },
          {
            /**
             * @default OAuth2.0
             * @constant
             * @enum {string}
             */
            auth_type?: 'OAuth2.0'
            /**
             * Client ID
             * @description Enter your application's Client ID
             */
            client_id?: string
            /**
             * Client Secret
             * @description Enter your application's Client secret
             */
            client_secret?: string
            /**
             * Access Token
             * @description Enter you application's Access Token
             */
            access_token: string
            /**
             * Refresh Token
             * @description Enter your application's Refresh Token
             */
            refresh_token: string
          },
        ]
      >
      /**
       * JDBC URL Params
       * @description Enter the additional properties to pass to the JDBC URL string when connecting to the database (formatted as key=value pairs separated by the symbol &). Example: key1=value1&key2=value2&key3=value3
       */
      jdbc_url_params?: string
      /**
       * Raw Table Schema Name
       * @description The schema to write raw tables into (default: airbyte_internal)
       */
      raw_data_schema?: string
      /**
       * Disable Final Tables. (WARNING! Unstable option; Columns in raw table schema might change between versions)
       * @description Disable Writing Final Tables. WARNING! The data format in _airbyte_data is likely stable but there are no guarantees that other metadata columns will remain the same in future versions
       * @default false
       */
      disable_type_dedupe?: boolean
      /**
       * Enable Loading Data Incrementally to Final Tables
       * @description When enabled your data will load into your final tables incrementally while your data is still being synced. When Disabled (the default), your data loads into your final tables once at the end of a sync. Note that this option only applies if you elect to create Final tables
       * @default false
       */
      enable_incremental_final_table_updates?: boolean
      /**
       * snowflake
       * @constant
       * @enum {string}
       */
      destinationType: 'snowflake'
    }
    /** Snowflake Destination Spec */
    'destination-snowflake-update': {
      /**
       * Host
       * @description Enter your Snowflake account's <a href="https://docs.snowflake.com/en/user-guide/admin-account-identifier.html#using-an-account-locator-as-an-identifier">locator</a> (in the format <account_locator>.<region>.<cloud>.snowflakecomputing.com)
       */
      host: string
      /**
       * Role
       * @description Enter the <a href="https://docs.snowflake.com/en/user-guide/security-access-control-overview.html#roles">role</a> that you want to use to access Snowflake
       */
      role: string
      /**
       * Warehouse
       * @description Enter the name of the <a href="https://docs.snowflake.com/en/user-guide/warehouses-overview.html#overview-of-warehouses">warehouse</a> that you want to sync data into
       */
      warehouse: string
      /**
       * Database
       * @description Enter the name of the <a href="https://docs.snowflake.com/en/sql-reference/ddl-database.html#database-schema-share-ddl">database</a> you want to sync data into
       */
      database: string
      /**
       * Default Schema
       * @description Enter the name of the default <a href="https://docs.snowflake.com/en/sql-reference/ddl-database.html#database-schema-share-ddl">schema</a>
       */
      schema: string
      /**
       * Username
       * @description Enter the name of the user you want to use to access the database
       */
      username: string
      /** Authorization Method */
      credentials?: OneOf<
        [
          {
            /**
             * @default Key Pair Authentication
             * @constant
             * @enum {string}
             */
            auth_type?: 'Key Pair Authentication'
            /**
             * Private Key
             * @description RSA Private key to use for Snowflake connection. See the <a href="https://docs.airbyte.com/integrations/destinations/snowflake">docs</a> for more information on how to obtain this key.
             */
            private_key: string
            /**
             * Passphrase
             * @description Passphrase for private key
             */
            private_key_password?: string
          },
          {
            /**
             * @default Username and Password
             * @constant
             * @enum {string}
             */
            auth_type?: 'Username and Password'
            /**
             * Password
             * @description Enter the password associated with the username.
             */
            password: string
          },
          {
            /**
             * @default OAuth2.0
             * @constant
             * @enum {string}
             */
            auth_type?: 'OAuth2.0'
            /**
             * Client ID
             * @description Enter your application's Client ID
             */
            client_id?: string
            /**
             * Client Secret
             * @description Enter your application's Client secret
             */
            client_secret?: string
            /**
             * Access Token
             * @description Enter you application's Access Token
             */
            access_token: string
            /**
             * Refresh Token
             * @description Enter your application's Refresh Token
             */
            refresh_token: string
          },
        ]
      >
      /**
       * JDBC URL Params
       * @description Enter the additional properties to pass to the JDBC URL string when connecting to the database (formatted as key=value pairs separated by the symbol &). Example: key1=value1&key2=value2&key3=value3
       */
      jdbc_url_params?: string
      /**
       * Raw Table Schema Name
       * @description The schema to write raw tables into (default: airbyte_internal)
       */
      raw_data_schema?: string
      /**
       * Disable Final Tables. (WARNING! Unstable option; Columns in raw table schema might change between versions)
       * @description Disable Writing Final Tables. WARNING! The data format in _airbyte_data is likely stable but there are no guarantees that other metadata columns will remain the same in future versions
       * @default false
       */
      disable_type_dedupe?: boolean
      /**
       * Enable Loading Data Incrementally to Final Tables
       * @description When enabled your data will load into your final tables incrementally while your data is still being synced. When Disabled (the default), your data loads into your final tables once at the end of a sync. Note that this option only applies if you elect to create Final tables
       * @default false
       */
      enable_incremental_final_table_updates?: boolean
    }
    /** Databricks Lakehouse Destination Spec */
    'destination-databricks': {
      /**
       * Agree to the Databricks JDBC Driver Terms & Conditions
       * @description You must agree to the Databricks JDBC Driver <a href="https://databricks.com/jdbc-odbc-driver-license">Terms & Conditions</a> to use this connector.
       * @default false
       */
      accept_terms: boolean
      /**
       * Server Hostname
       * @description Databricks Cluster Server Hostname.
       */
      databricks_server_hostname: string
      /**
       * HTTP Path
       * @description Databricks Cluster HTTP Path.
       */
      databricks_http_path: string
      /**
       * Port
       * @description Databricks Cluster Port.
       * @default 443
       */
      databricks_port?: string
      /**
       * Access Token
       * @description Databricks Personal Access Token for making authenticated requests.
       */
      databricks_personal_access_token: string
      /**
       * Databricks catalog
       * @description The name of the catalog. If not specified otherwise, the "hive_metastore" will be used.
       */
      database?: string
      /**
       * Default Schema
       * @description The default schema tables are written. If not specified otherwise, the "default" will be used.
       * @default default
       */
      schema?: string
      /**
       * Support schema evolution for all streams.
       * @description Support schema evolution for all streams. If "false", the connector might fail when a stream's schema changes.
       * @default false
       */
      enable_schema_evolution?: boolean
      /**
       * Data Source
       * @description Storage on which the delta lake is built.
       * @default MANAGED_TABLES_STORAGE
       */
      data_source: OneOf<
        [
          {
            /**
             * @constant
             * @enum {string}
             */
            data_source_type: 'MANAGED_TABLES_STORAGE'
          },
          {
            /**
             * @constant
             * @enum {string}
             */
            data_source_type: 'S3_STORAGE'
            /**
             * S3 Bucket Name
             * @description The name of the S3 bucket to use for intermittent staging of the data.
             */
            s3_bucket_name: string
            /**
             * S3 Bucket Path
             * @description The directory under the S3 bucket where data will be written.
             */
            s3_bucket_path: string
            /**
             * S3 Bucket Region
             * @description The region of the S3 staging bucket to use if utilising a copy strategy.
             * @default
             * @enum {string}
             */
            s3_bucket_region:
              | ''
              | 'us-east-1'
              | 'us-east-2'
              | 'us-west-1'
              | 'us-west-2'
              | 'af-south-1'
              | 'ap-east-1'
              | 'ap-south-1'
              | 'ap-northeast-1'
              | 'ap-northeast-2'
              | 'ap-northeast-3'
              | 'ap-southeast-1'
              | 'ap-southeast-2'
              | 'ca-central-1'
              | 'cn-north-1'
              | 'cn-northwest-1'
              | 'eu-central-1'
              | 'eu-north-1'
              | 'eu-south-1'
              | 'eu-west-1'
              | 'eu-west-2'
              | 'eu-west-3'
              | 'sa-east-1'
              | 'me-south-1'
              | 'us-gov-east-1'
              | 'us-gov-west-1'
            /**
             * S3 Access Key ID
             * @description The Access Key Id granting allow one to access the above S3 staging bucket. Airbyte requires Read and Write permissions to the given bucket.
             */
            s3_access_key_id: string
            /**
             * S3 Secret Access Key
             * @description The corresponding secret to the above access key id.
             */
            s3_secret_access_key: string
            /**
             * S3 Filename pattern
             * @description The pattern allows you to set the file-name format for the S3 staging file(s)
             */
            file_name_pattern?: string
          },
          {
            /**
             * @constant
             * @enum {string}
             */
            data_source_type: 'AZURE_BLOB_STORAGE'
            /**
             * Endpoint Domain Name
             * @description This is Azure Blob Storage endpoint domain name. Leave default value (or leave it empty if run container from command line) to use Microsoft native from example.
             * @default blob.core.windows.net
             */
            azure_blob_storage_endpoint_domain_name?: string
            /**
             * Azure Blob Storage Account Name
             * @description The account's name of the Azure Blob Storage.
             */
            azure_blob_storage_account_name: string
            /**
             * Azure Blob Storage Container Name
             * @description The name of the Azure blob storage container.
             */
            azure_blob_storage_container_name: string
            /**
             * SAS Token
             * @description Shared access signature (SAS) token to grant limited access to objects in your storage account.
             */
            azure_blob_storage_sas_token: string
          },
        ]
      >
      /**
       * Purge Staging Files and Tables
       * @description Default to 'true'. Switch it to 'false' for debugging purpose.
       * @default true
       */
      purge_staging_data?: boolean
      /**
       * databricks
       * @constant
       * @enum {string}
       */
      destinationType: 'databricks'
    }
    /** Databricks Lakehouse Destination Spec */
    'destination-databricks-update': {
      /**
       * Agree to the Databricks JDBC Driver Terms & Conditions
       * @description You must agree to the Databricks JDBC Driver <a href="https://databricks.com/jdbc-odbc-driver-license">Terms & Conditions</a> to use this connector.
       * @default false
       */
      accept_terms: boolean
      /**
       * Server Hostname
       * @description Databricks Cluster Server Hostname.
       */
      databricks_server_hostname: string
      /**
       * HTTP Path
       * @description Databricks Cluster HTTP Path.
       */
      databricks_http_path: string
      /**
       * Port
       * @description Databricks Cluster Port.
       * @default 443
       */
      databricks_port?: string
      /**
       * Access Token
       * @description Databricks Personal Access Token for making authenticated requests.
       */
      databricks_personal_access_token: string
      /**
       * Databricks catalog
       * @description The name of the catalog. If not specified otherwise, the "hive_metastore" will be used.
       */
      database?: string
      /**
       * Default Schema
       * @description The default schema tables are written. If not specified otherwise, the "default" will be used.
       * @default default
       */
      schema?: string
      /**
       * Support schema evolution for all streams.
       * @description Support schema evolution for all streams. If "false", the connector might fail when a stream's schema changes.
       * @default false
       */
      enable_schema_evolution?: boolean
      /**
       * Data Source
       * @description Storage on which the delta lake is built.
       * @default MANAGED_TABLES_STORAGE
       */
      data_source: OneOf<
        [
          {
            /**
             * @constant
             * @enum {string}
             */
            data_source_type: 'MANAGED_TABLES_STORAGE'
          },
          {
            /**
             * @constant
             * @enum {string}
             */
            data_source_type: 'S3_STORAGE'
            /**
             * S3 Bucket Name
             * @description The name of the S3 bucket to use for intermittent staging of the data.
             */
            s3_bucket_name: string
            /**
             * S3 Bucket Path
             * @description The directory under the S3 bucket where data will be written.
             */
            s3_bucket_path: string
            /**
             * S3 Bucket Region
             * @description The region of the S3 staging bucket to use if utilising a copy strategy.
             * @default
             * @enum {string}
             */
            s3_bucket_region:
              | ''
              | 'us-east-1'
              | 'us-east-2'
              | 'us-west-1'
              | 'us-west-2'
              | 'af-south-1'
              | 'ap-east-1'
              | 'ap-south-1'
              | 'ap-northeast-1'
              | 'ap-northeast-2'
              | 'ap-northeast-3'
              | 'ap-southeast-1'
              | 'ap-southeast-2'
              | 'ca-central-1'
              | 'cn-north-1'
              | 'cn-northwest-1'
              | 'eu-central-1'
              | 'eu-north-1'
              | 'eu-south-1'
              | 'eu-west-1'
              | 'eu-west-2'
              | 'eu-west-3'
              | 'sa-east-1'
              | 'me-south-1'
              | 'us-gov-east-1'
              | 'us-gov-west-1'
            /**
             * S3 Access Key ID
             * @description The Access Key Id granting allow one to access the above S3 staging bucket. Airbyte requires Read and Write permissions to the given bucket.
             */
            s3_access_key_id: string
            /**
             * S3 Secret Access Key
             * @description The corresponding secret to the above access key id.
             */
            s3_secret_access_key: string
            /**
             * S3 Filename pattern
             * @description The pattern allows you to set the file-name format for the S3 staging file(s)
             */
            file_name_pattern?: string
          },
          {
            /**
             * @constant
             * @enum {string}
             */
            data_source_type: 'AZURE_BLOB_STORAGE'
            /**
             * Endpoint Domain Name
             * @description This is Azure Blob Storage endpoint domain name. Leave default value (or leave it empty if run container from command line) to use Microsoft native from example.
             * @default blob.core.windows.net
             */
            azure_blob_storage_endpoint_domain_name?: string
            /**
             * Azure Blob Storage Account Name
             * @description The account's name of the Azure Blob Storage.
             */
            azure_blob_storage_account_name: string
            /**
             * Azure Blob Storage Container Name
             * @description The name of the Azure blob storage container.
             */
            azure_blob_storage_container_name: string
            /**
             * SAS Token
             * @description Shared access signature (SAS) token to grant limited access to objects in your storage account.
             */
            azure_blob_storage_sas_token: string
          },
        ]
      >
      /**
       * Purge Staging Files and Tables
       * @description Default to 'true'. Switch it to 'false' for debugging purpose.
       * @default true
       */
      purge_staging_data?: boolean
    }
    /** Oracle Destination Spec */
    'destination-oracle': {
      /**
       * Host
       * @description The hostname of the database.
       */
      host: string
      /**
       * Port
       * @description The port of the database.
       * @default 1521
       */
      port: number
      /**
       * SID
       * @description The System Identifier uniquely distinguishes the instance from any other instance on the same computer.
       */
      sid: string
      /**
       * User
       * @description The username to access the database. This user must have CREATE USER privileges in the database.
       */
      username: string
      /**
       * Password
       * @description The password associated with the username.
       */
      password?: string
      /**
       * JDBC URL Params
       * @description Additional properties to pass to the JDBC URL string when connecting to the database formatted as 'key=value' pairs separated by the symbol '&'. (example: key1=value1&key2=value2&key3=value3).
       */
      jdbc_url_params?: string
      /**
       * Default Schema
       * @description The default schema is used as the target schema for all statements issued from the connection that do not explicitly specify a schema name. The usual value for this field is "airbyte".  In Oracle, schemas and users are the same thing, so the "user" parameter is used as the login credentials and this is used for the default Airbyte message schema.
       * @default airbyte
       */
      schema?: string
      /**
       * SSH Tunnel Method
       * @description Whether to initiate an SSH tunnel before connecting to the database, and if so, which kind of authentication to use.
       */
      tunnel_method?: OneOf<
        [
          {
            /**
             * @description No ssh tunnel needed to connect to database
             * @constant
             * @enum {string}
             */
            tunnel_method: 'NO_TUNNEL'
          },
          {
            /**
             * @description Connect through a jump server tunnel host using username and ssh key
             * @constant
             * @enum {string}
             */
            tunnel_method: 'SSH_KEY_AUTH'
            /**
             * SSH Tunnel Jump Server Host
             * @description Hostname of the jump server host that allows inbound ssh tunnel.
             */
            tunnel_host: string
            /**
             * SSH Connection Port
             * @description Port on the proxy/jump server that accepts inbound ssh connections.
             * @default 22
             */
            tunnel_port: number
            /**
             * SSH Login Username
             * @description OS-level username for logging into the jump server host.
             */
            tunnel_user: string
            /**
             * SSH Private Key
             * @description OS-level user account ssh key credentials in RSA PEM format ( created with ssh-keygen -t rsa -m PEM -f myuser_rsa )
             */
            ssh_key: string
          },
          {
            /**
             * @description Connect through a jump server tunnel host using username and password authentication
             * @constant
             * @enum {string}
             */
            tunnel_method: 'SSH_PASSWORD_AUTH'
            /**
             * SSH Tunnel Jump Server Host
             * @description Hostname of the jump server host that allows inbound ssh tunnel.
             */
            tunnel_host: string
            /**
             * SSH Connection Port
             * @description Port on the proxy/jump server that accepts inbound ssh connections.
             * @default 22
             */
            tunnel_port: number
            /**
             * SSH Login Username
             * @description OS-level username for logging into the jump server host
             */
            tunnel_user: string
            /**
             * Password
             * @description OS-level password for logging into the jump server host
             */
            tunnel_user_password: string
          },
        ]
      >
      /**
       * oracle
       * @constant
       * @enum {string}
       */
      destinationType: 'oracle'
    }
    /** Oracle Destination Spec */
    'destination-oracle-update': {
      /**
       * Host
       * @description The hostname of the database.
       */
      host: string
      /**
       * Port
       * @description The port of the database.
       * @default 1521
       */
      port: number
      /**
       * SID
       * @description The System Identifier uniquely distinguishes the instance from any other instance on the same computer.
       */
      sid: string
      /**
       * User
       * @description The username to access the database. This user must have CREATE USER privileges in the database.
       */
      username: string
      /**
       * Password
       * @description The password associated with the username.
       */
      password?: string
      /**
       * JDBC URL Params
       * @description Additional properties to pass to the JDBC URL string when connecting to the database formatted as 'key=value' pairs separated by the symbol '&'. (example: key1=value1&key2=value2&key3=value3).
       */
      jdbc_url_params?: string
      /**
       * Default Schema
       * @description The default schema is used as the target schema for all statements issued from the connection that do not explicitly specify a schema name. The usual value for this field is "airbyte".  In Oracle, schemas and users are the same thing, so the "user" parameter is used as the login credentials and this is used for the default Airbyte message schema.
       * @default airbyte
       */
      schema?: string
      /**
       * SSH Tunnel Method
       * @description Whether to initiate an SSH tunnel before connecting to the database, and if so, which kind of authentication to use.
       */
      tunnel_method?: OneOf<
        [
          {
            /**
             * @description No ssh tunnel needed to connect to database
             * @constant
             * @enum {string}
             */
            tunnel_method: 'NO_TUNNEL'
          },
          {
            /**
             * @description Connect through a jump server tunnel host using username and ssh key
             * @constant
             * @enum {string}
             */
            tunnel_method: 'SSH_KEY_AUTH'
            /**
             * SSH Tunnel Jump Server Host
             * @description Hostname of the jump server host that allows inbound ssh tunnel.
             */
            tunnel_host: string
            /**
             * SSH Connection Port
             * @description Port on the proxy/jump server that accepts inbound ssh connections.
             * @default 22
             */
            tunnel_port: number
            /**
             * SSH Login Username
             * @description OS-level username for logging into the jump server host.
             */
            tunnel_user: string
            /**
             * SSH Private Key
             * @description OS-level user account ssh key credentials in RSA PEM format ( created with ssh-keygen -t rsa -m PEM -f myuser_rsa )
             */
            ssh_key: string
          },
          {
            /**
             * @description Connect through a jump server tunnel host using username and password authentication
             * @constant
             * @enum {string}
             */
            tunnel_method: 'SSH_PASSWORD_AUTH'
            /**
             * SSH Tunnel Jump Server Host
             * @description Hostname of the jump server host that allows inbound ssh tunnel.
             */
            tunnel_host: string
            /**
             * SSH Connection Port
             * @description Port on the proxy/jump server that accepts inbound ssh connections.
             * @default 22
             */
            tunnel_port: number
            /**
             * SSH Login Username
             * @description OS-level username for logging into the jump server host
             */
            tunnel_user: string
            /**
             * Password
             * @description OS-level password for logging into the jump server host
             */
            tunnel_user_password: string
          },
        ]
      >
    }
    /** AWS Datalake Destination Spec */
    'destination-aws-datalake': {
      /**
       * AWS Account Id
       * @description target aws account id
       */
      aws_account_id?: string
      /**
       * Authentication mode
       * @description Choose How to Authenticate to AWS.
       */
      credentials: OneOf<
        [
          {
            /**
             * Credentials Title
             * @description Name of the credentials
             * @default IAM Role
             * @constant
             * @enum {string}
             */
            credentials_title: 'IAM Role'
            /**
             * Target Role Arn
             * @description Will assume this role to write data to s3
             */
            role_arn: string
          },
          {
            /**
             * Credentials Title
             * @description Name of the credentials
             * @default IAM User
             * @constant
             * @enum {string}
             */
            credentials_title: 'IAM User'
            /**
             * Access Key Id
             * @description AWS User Access Key Id
             */
            aws_access_key_id: string
            /**
             * Secret Access Key
             * @description Secret Access Key
             */
            aws_secret_access_key: string
          },
        ]
      >
      /**
       * S3 Bucket Region
       * @description The region of the S3 bucket. See <a href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-regions-availability-zones.html#concepts-available-regions">here</a> for all region codes.
       * @default
       * @enum {string}
       */
      region:
        | ''
        | 'af-south-1'
        | 'ap-east-1'
        | 'ap-northeast-1'
        | 'ap-northeast-2'
        | 'ap-northeast-3'
        | 'ap-south-1'
        | 'ap-south-2'
        | 'ap-southeast-1'
        | 'ap-southeast-2'
        | 'ap-southeast-3'
        | 'ap-southeast-4'
        | 'ca-central-1'
        | 'ca-west-1'
        | 'cn-north-1'
        | 'cn-northwest-1'
        | 'eu-central-1'
        | 'eu-central-2'
        | 'eu-north-1'
        | 'eu-south-1'
        | 'eu-south-2'
        | 'eu-west-1'
        | 'eu-west-2'
        | 'eu-west-3'
        | 'il-central-1'
        | 'me-central-1'
        | 'me-south-1'
        | 'sa-east-1'
        | 'us-east-1'
        | 'us-east-2'
        | 'us-gov-east-1'
        | 'us-gov-west-1'
        | 'us-west-1'
        | 'us-west-2'
      /**
       * S3 Bucket Name
       * @description The name of the S3 bucket. Read more <a href="https://docs.aws.amazon.com/AmazonS3/latest/userguide/create-bucket-overview.html">here</a>.
       */
      bucket_name: string
      /**
       * Target S3 Bucket Prefix
       * @description S3 prefix
       */
      bucket_prefix?: string
      /**
       * Lake Formation Database Name
       * @description The default database this destination will use to create tables in per stream. Can be changed per connection by customizing the namespace.
       */
      lakeformation_database_name: string
      /**
       * Lake Formation Database Tag Key
       * @description Add a default tag key to databases created by this destination
       */
      lakeformation_database_default_tag_key?: string
      /**
       * Lake Formation Database Tag Values
       * @description Add default values for the `Tag Key` to databases created by this destination. Comma separate for multiple values.
       */
      lakeformation_database_default_tag_values?: string
      /**
       * Lake Formation Governed Tables
       * @description Whether to create tables as LF governed tables.
       * @default false
       */
      lakeformation_governed_tables?: boolean
      /**
       * Output Format *
       * @description Format of the data output.
       */
      format?: OneOf<
        [
          {
            /**
             * Format Type *
             * @default JSONL
             * @enum {string}
             */
            format_type: 'JSONL'
            /**
             * Compression Codec (Optional)
             * @description The compression algorithm used to compress data.
             * @default UNCOMPRESSED
             * @enum {string}
             */
            compression_codec?: 'UNCOMPRESSED' | 'GZIP'
          },
          {
            /**
             * Format Type *
             * @default Parquet
             * @enum {string}
             */
            format_type: 'Parquet'
            /**
             * Compression Codec (Optional)
             * @description The compression algorithm used to compress data.
             * @default SNAPPY
             * @enum {string}
             */
            compression_codec?: 'UNCOMPRESSED' | 'SNAPPY' | 'GZIP' | 'ZSTD'
          },
        ]
      >
      /**
       * Choose how to partition data
       * @description Partition data by cursor fields when a cursor field is a date
       * @default NO PARTITIONING
       * @enum {string}
       */
      partitioning?:
        | 'NO PARTITIONING'
        | 'DATE'
        | 'YEAR'
        | 'MONTH'
        | 'DAY'
        | 'YEAR/MONTH'
        | 'YEAR/MONTH/DAY'
      /**
       * Glue Catalog: Float as Decimal
       * @description Cast float/double as decimal(38,18). This can help achieve higher accuracy and represent numbers correctly as received from the source.
       * @default false
       */
      glue_catalog_float_as_decimal?: boolean
      /**
       * aws-datalake
       * @constant
       * @enum {string}
       */
      destinationType: 'aws-datalake'
    }
    /** AWS Datalake Destination Spec */
    'destination-aws-datalake-update': {
      /**
       * AWS Account Id
       * @description target aws account id
       */
      aws_account_id?: string
      /**
       * Authentication mode
       * @description Choose How to Authenticate to AWS.
       */
      credentials: OneOf<
        [
          {
            /**
             * Credentials Title
             * @description Name of the credentials
             * @default IAM Role
             * @constant
             * @enum {string}
             */
            credentials_title: 'IAM Role'
            /**
             * Target Role Arn
             * @description Will assume this role to write data to s3
             */
            role_arn: string
          },
          {
            /**
             * Credentials Title
             * @description Name of the credentials
             * @default IAM User
             * @constant
             * @enum {string}
             */
            credentials_title: 'IAM User'
            /**
             * Access Key Id
             * @description AWS User Access Key Id
             */
            aws_access_key_id: string
            /**
             * Secret Access Key
             * @description Secret Access Key
             */
            aws_secret_access_key: string
          },
        ]
      >
      /**
       * S3 Bucket Region
       * @description The region of the S3 bucket. See <a href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-regions-availability-zones.html#concepts-available-regions">here</a> for all region codes.
       * @default
       * @enum {string}
       */
      region:
        | ''
        | 'af-south-1'
        | 'ap-east-1'
        | 'ap-northeast-1'
        | 'ap-northeast-2'
        | 'ap-northeast-3'
        | 'ap-south-1'
        | 'ap-south-2'
        | 'ap-southeast-1'
        | 'ap-southeast-2'
        | 'ap-southeast-3'
        | 'ap-southeast-4'
        | 'ca-central-1'
        | 'ca-west-1'
        | 'cn-north-1'
        | 'cn-northwest-1'
        | 'eu-central-1'
        | 'eu-central-2'
        | 'eu-north-1'
        | 'eu-south-1'
        | 'eu-south-2'
        | 'eu-west-1'
        | 'eu-west-2'
        | 'eu-west-3'
        | 'il-central-1'
        | 'me-central-1'
        | 'me-south-1'
        | 'sa-east-1'
        | 'us-east-1'
        | 'us-east-2'
        | 'us-gov-east-1'
        | 'us-gov-west-1'
        | 'us-west-1'
        | 'us-west-2'
      /**
       * S3 Bucket Name
       * @description The name of the S3 bucket. Read more <a href="https://docs.aws.amazon.com/AmazonS3/latest/userguide/create-bucket-overview.html">here</a>.
       */
      bucket_name: string
      /**
       * Target S3 Bucket Prefix
       * @description S3 prefix
       */
      bucket_prefix?: string
      /**
       * Lake Formation Database Name
       * @description The default database this destination will use to create tables in per stream. Can be changed per connection by customizing the namespace.
       */
      lakeformation_database_name: string
      /**
       * Lake Formation Database Tag Key
       * @description Add a default tag key to databases created by this destination
       */
      lakeformation_database_default_tag_key?: string
      /**
       * Lake Formation Database Tag Values
       * @description Add default values for the `Tag Key` to databases created by this destination. Comma separate for multiple values.
       */
      lakeformation_database_default_tag_values?: string
      /**
       * Lake Formation Governed Tables
       * @description Whether to create tables as LF governed tables.
       * @default false
       */
      lakeformation_governed_tables?: boolean
      /**
       * Output Format *
       * @description Format of the data output.
       */
      format?: OneOf<
        [
          {
            /**
             * Format Type *
             * @default JSONL
             * @enum {string}
             */
            format_type: 'JSONL'
            /**
             * Compression Codec (Optional)
             * @description The compression algorithm used to compress data.
             * @default UNCOMPRESSED
             * @enum {string}
             */
            compression_codec?: 'UNCOMPRESSED' | 'GZIP'
          },
          {
            /**
             * Format Type *
             * @default Parquet
             * @enum {string}
             */
            format_type: 'Parquet'
            /**
             * Compression Codec (Optional)
             * @description The compression algorithm used to compress data.
             * @default SNAPPY
             * @enum {string}
             */
            compression_codec?: 'UNCOMPRESSED' | 'SNAPPY' | 'GZIP' | 'ZSTD'
          },
        ]
      >
      /**
       * Choose how to partition data
       * @description Partition data by cursor fields when a cursor field is a date
       * @default NO PARTITIONING
       * @enum {string}
       */
      partitioning?:
        | 'NO PARTITIONING'
        | 'DATE'
        | 'YEAR'
        | 'MONTH'
        | 'DAY'
        | 'YEAR/MONTH'
        | 'YEAR/MONTH/DAY'
      /**
       * Glue Catalog: Float as Decimal
       * @description Cast float/double as decimal(38,18). This can help achieve higher accuracy and represent numbers correctly as received from the source.
       * @default false
       */
      glue_catalog_float_as_decimal?: boolean
    }
    /**
     * Destination Config
     * @description The configuration model for the Vector DB based destinations. This model is used to generate the UI for the destination configuration,
     * as well as to provide type safety for the configuration passed to the destination.
     *
     * The configuration model is composed of four parts:
     * * Processing configuration
     * * Embedding configuration
     * * Indexing configuration
     * * Advanced configuration
     *
     * Processing, embedding and advanced configuration are provided by this base class, while the indexing configuration is provided by the destination connector in the sub class.
     */
    'destination-milvus': {
      /**
       * Embedding
       * @description Embedding configuration
       */
      embedding: OneOf<
        [
          {
            /**
             * Mode
             * @default openai
             * @constant
             * @enum {string}
             */
            mode: 'openai'
            /** OpenAI API key */
            openai_key: string
          },
          {
            /**
             * Mode
             * @default cohere
             * @constant
             * @enum {string}
             */
            mode: 'cohere'
            /** Cohere API key */
            cohere_key: string
          },
          {
            /**
             * Mode
             * @default fake
             * @constant
             * @enum {string}
             */
            mode: 'fake'
          },
          {
            /**
             * Mode
             * @default azure_openai
             * @constant
             * @enum {string}
             */
            mode: 'azure_openai'
            /**
             * Azure OpenAI API key
             * @description The API key for your Azure OpenAI resource.  You can find this in the Azure portal under your Azure OpenAI resource
             */
            openai_key: string
            /**
             * Resource base URL
             * @description The base URL for your Azure OpenAI resource.  You can find this in the Azure portal under your Azure OpenAI resource
             */
            api_base: string
            /**
             * Deployment
             * @description The deployment for your Azure OpenAI resource.  You can find this in the Azure portal under your Azure OpenAI resource
             */
            deployment: string
          },
          {
            /**
             * Mode
             * @default openai_compatible
             * @constant
             * @enum {string}
             */
            mode: 'openai_compatible'
            /**
             * API key
             * @default
             */
            api_key?: string
            /**
             * Base URL
             * @description The base URL for your OpenAI-compatible service
             */
            base_url: string
            /**
             * Model name
             * @description The name of the model to use for embedding
             * @default text-embedding-ada-002
             */
            model_name?: string
            /**
             * Embedding dimensions
             * @description The number of dimensions the embedding model is generating
             */
            dimensions: number
          },
        ]
      >
      /** ProcessingConfigModel */
      processing: {
        /**
         * Chunk size
         * @description Size of chunks in tokens to store in vector store (make sure it is not too big for the context if your LLM)
         */
        chunk_size: number
        /**
         * Chunk overlap
         * @description Size of overlap between chunks in tokens to store in vector store to better capture relevant context
         * @default 0
         */
        chunk_overlap?: number
        /**
         * Text fields to embed
         * @description List of fields in the record that should be used to calculate the embedding. The field list is applied to all streams in the same way and non-existing fields are ignored. If none are defined, all fields are considered text fields. When specifying text fields, you can access nested fields in the record by using dot notation, e.g. `user.name` will access the `name` field in the `user` object. It's also possible to use wildcards to access all fields in an object, e.g. `users.*.name` will access all `names` fields in all entries of the `users` array.
         * @default []
         */
        text_fields?: string[]
        /**
         * Fields to store as metadata
         * @description List of fields in the record that should be stored as metadata. The field list is applied to all streams in the same way and non-existing fields are ignored. If none are defined, all fields are considered metadata fields. When specifying text fields, you can access nested fields in the record by using dot notation, e.g. `user.name` will access the `name` field in the `user` object. It's also possible to use wildcards to access all fields in an object, e.g. `users.*.name` will access all `names` fields in all entries of the `users` array. When specifying nested paths, all matching values are flattened into an array set to a field named by the path.
         * @default []
         */
        metadata_fields?: string[]
        /**
         * Text splitter
         * @description Split text fields into chunks based on the specified method.
         */
        text_splitter?: OneOf<
          [
            {
              /**
               * Mode
               * @default separator
               * @constant
               * @enum {string}
               */
              mode: 'separator'
              /**
               * Separators
               * @description List of separator strings to split text fields by. The separator itself needs to be wrapped in double quotes, e.g. to split by the dot character, use ".". To split by a newline, use "\n".
               * @default [
               *   "\"\\n\\n\"",
               *   "\"\\n\"",
               *   "\" \"",
               *   "\"\""
               * ]
               */
              separators?: string[]
              /**
               * Keep separator
               * @description Whether to keep the separator in the resulting chunks
               * @default false
               */
              keep_separator?: boolean
            },
            {
              /**
               * Mode
               * @default markdown
               * @constant
               * @enum {string}
               */
              mode: 'markdown'
              /**
               * Split level
               * @description Level of markdown headers to split text fields by. Headings down to the specified level will be used as split points
               * @default 1
               */
              split_level?: number
            },
            {
              /**
               * Mode
               * @default code
               * @constant
               * @enum {string}
               */
              mode: 'code'
              /**
               * Language
               * @description Split code in suitable places based on the programming language
               * @enum {string}
               */
              language:
                | 'cpp'
                | 'go'
                | 'java'
                | 'js'
                | 'php'
                | 'proto'
                | 'python'
                | 'rst'
                | 'ruby'
                | 'rust'
                | 'scala'
                | 'swift'
                | 'markdown'
                | 'latex'
                | 'html'
                | 'sol'
            },
          ]
        >
        /**
         * Field name mappings
         * @description List of fields to rename. Not applicable for nested fields, but can be used to rename fields already flattened via dot notation.
         * @default []
         */
        field_name_mappings?: {
          /**
           * From field name
           * @description The field name in the source
           */
          from_field: string
          /**
           * To field name
           * @description The field name to use in the destination
           */
          to_field: string
        }[]
      }
      /**
       * Do not store raw text
       * @description Do not store the text that gets embedded along with the vector and the metadata in the destination. If set to true, only the vector and the metadata will be stored - in this case raw text for LLM use cases needs to be retrieved from another source.
       * @default false
       */
      omit_raw_text?: boolean
      /**
       * Indexing
       * @description Indexing configuration
       */
      indexing: {
        /**
         * Public Endpoint
         * @description The public endpoint of the Milvus instance.
         */
        host: string
        /**
         * Database Name
         * @description The database to connect to
         * @default
         */
        db?: string
        /**
         * Collection Name
         * @description The collection to load data into
         */
        collection: string
        /**
         * Authentication
         * @description Authentication method
         */
        auth: OneOf<
          [
            {
              /**
               * Mode
               * @default token
               * @constant
               * @enum {string}
               */
              mode: 'token'
              /**
               * API Token
               * @description API Token for the Milvus instance
               */
              token: string
            },
            {
              /**
               * Mode
               * @default username_password
               * @constant
               * @enum {string}
               */
              mode: 'username_password'
              /**
               * Username
               * @description Username for the Milvus instance
               */
              username: string
              /**
               * Password
               * @description Password for the Milvus instance
               */
              password: string
            },
            {
              /**
               * Mode
               * @default no_auth
               * @constant
               * @enum {string}
               */
              mode: 'no_auth'
            },
          ]
        >
        /**
         * Vector Field
         * @description The field in the entity that contains the vector
         * @default vector
         */
        vector_field?: string
        /**
         * Text Field
         * @description The field in the entity that contains the embedded text
         * @default text
         */
        text_field?: string
      }
      /**
       * milvus
       * @constant
       * @enum {string}
       */
      destinationType: 'milvus'
    }
    /**
     * Destination Config
     * @description The configuration model for the Vector DB based destinations. This model is used to generate the UI for the destination configuration,
     * as well as to provide type safety for the configuration passed to the destination.
     *
     * The configuration model is composed of four parts:
     * * Processing configuration
     * * Embedding configuration
     * * Indexing configuration
     * * Advanced configuration
     *
     * Processing, embedding and advanced configuration are provided by this base class, while the indexing configuration is provided by the destination connector in the sub class.
     */
    'destination-milvus-update': {
      /**
       * Embedding
       * @description Embedding configuration
       */
      embedding: OneOf<
        [
          {
            /**
             * Mode
             * @default openai
             * @constant
             * @enum {string}
             */
            mode: 'openai'
            /** OpenAI API key */
            openai_key: string
          },
          {
            /**
             * Mode
             * @default cohere
             * @constant
             * @enum {string}
             */
            mode: 'cohere'
            /** Cohere API key */
            cohere_key: string
          },
          {
            /**
             * Mode
             * @default fake
             * @constant
             * @enum {string}
             */
            mode: 'fake'
          },
          {
            /**
             * Mode
             * @default azure_openai
             * @constant
             * @enum {string}
             */
            mode: 'azure_openai'
            /**
             * Azure OpenAI API key
             * @description The API key for your Azure OpenAI resource.  You can find this in the Azure portal under your Azure OpenAI resource
             */
            openai_key: string
            /**
             * Resource base URL
             * @description The base URL for your Azure OpenAI resource.  You can find this in the Azure portal under your Azure OpenAI resource
             */
            api_base: string
            /**
             * Deployment
             * @description The deployment for your Azure OpenAI resource.  You can find this in the Azure portal under your Azure OpenAI resource
             */
            deployment: string
          },
          {
            /**
             * Mode
             * @default openai_compatible
             * @constant
             * @enum {string}
             */
            mode: 'openai_compatible'
            /**
             * API key
             * @default
             */
            api_key?: string
            /**
             * Base URL
             * @description The base URL for your OpenAI-compatible service
             */
            base_url: string
            /**
             * Model name
             * @description The name of the model to use for embedding
             * @default text-embedding-ada-002
             */
            model_name?: string
            /**
             * Embedding dimensions
             * @description The number of dimensions the embedding model is generating
             */
            dimensions: number
          },
        ]
      >
      /** ProcessingConfigModel */
      processing: {
        /**
         * Chunk size
         * @description Size of chunks in tokens to store in vector store (make sure it is not too big for the context if your LLM)
         */
        chunk_size: number
        /**
         * Chunk overlap
         * @description Size of overlap between chunks in tokens to store in vector store to better capture relevant context
         * @default 0
         */
        chunk_overlap?: number
        /**
         * Text fields to embed
         * @description List of fields in the record that should be used to calculate the embedding. The field list is applied to all streams in the same way and non-existing fields are ignored. If none are defined, all fields are considered text fields. When specifying text fields, you can access nested fields in the record by using dot notation, e.g. `user.name` will access the `name` field in the `user` object. It's also possible to use wildcards to access all fields in an object, e.g. `users.*.name` will access all `names` fields in all entries of the `users` array.
         * @default []
         */
        text_fields?: string[]
        /**
         * Fields to store as metadata
         * @description List of fields in the record that should be stored as metadata. The field list is applied to all streams in the same way and non-existing fields are ignored. If none are defined, all fields are considered metadata fields. When specifying text fields, you can access nested fields in the record by using dot notation, e.g. `user.name` will access the `name` field in the `user` object. It's also possible to use wildcards to access all fields in an object, e.g. `users.*.name` will access all `names` fields in all entries of the `users` array. When specifying nested paths, all matching values are flattened into an array set to a field named by the path.
         * @default []
         */
        metadata_fields?: string[]
        /**
         * Text splitter
         * @description Split text fields into chunks based on the specified method.
         */
        text_splitter?: OneOf<
          [
            {
              /**
               * Mode
               * @default separator
               * @constant
               * @enum {string}
               */
              mode: 'separator'
              /**
               * Separators
               * @description List of separator strings to split text fields by. The separator itself needs to be wrapped in double quotes, e.g. to split by the dot character, use ".". To split by a newline, use "\n".
               * @default [
               *   "\"\\n\\n\"",
               *   "\"\\n\"",
               *   "\" \"",
               *   "\"\""
               * ]
               */
              separators?: string[]
              /**
               * Keep separator
               * @description Whether to keep the separator in the resulting chunks
               * @default false
               */
              keep_separator?: boolean
            },
            {
              /**
               * Mode
               * @default markdown
               * @constant
               * @enum {string}
               */
              mode: 'markdown'
              /**
               * Split level
               * @description Level of markdown headers to split text fields by. Headings down to the specified level will be used as split points
               * @default 1
               */
              split_level?: number
            },
            {
              /**
               * Mode
               * @default code
               * @constant
               * @enum {string}
               */
              mode: 'code'
              /**
               * Language
               * @description Split code in suitable places based on the programming language
               * @enum {string}
               */
              language:
                | 'cpp'
                | 'go'
                | 'java'
                | 'js'
                | 'php'
                | 'proto'
                | 'python'
                | 'rst'
                | 'ruby'
                | 'rust'
                | 'scala'
                | 'swift'
                | 'markdown'
                | 'latex'
                | 'html'
                | 'sol'
            },
          ]
        >
        /**
         * Field name mappings
         * @description List of fields to rename. Not applicable for nested fields, but can be used to rename fields already flattened via dot notation.
         * @default []
         */
        field_name_mappings?: {
          /**
           * From field name
           * @description The field name in the source
           */
          from_field: string
          /**
           * To field name
           * @description The field name to use in the destination
           */
          to_field: string
        }[]
      }
      /**
       * Do not store raw text
       * @description Do not store the text that gets embedded along with the vector and the metadata in the destination. If set to true, only the vector and the metadata will be stored - in this case raw text for LLM use cases needs to be retrieved from another source.
       * @default false
       */
      omit_raw_text?: boolean
      /**
       * Indexing
       * @description Indexing configuration
       */
      indexing: {
        /**
         * Public Endpoint
         * @description The public endpoint of the Milvus instance.
         */
        host: string
        /**
         * Database Name
         * @description The database to connect to
         * @default
         */
        db?: string
        /**
         * Collection Name
         * @description The collection to load data into
         */
        collection: string
        /**
         * Authentication
         * @description Authentication method
         */
        auth: OneOf<
          [
            {
              /**
               * Mode
               * @default token
               * @constant
               * @enum {string}
               */
              mode: 'token'
              /**
               * API Token
               * @description API Token for the Milvus instance
               */
              token: string
            },
            {
              /**
               * Mode
               * @default username_password
               * @constant
               * @enum {string}
               */
              mode: 'username_password'
              /**
               * Username
               * @description Username for the Milvus instance
               */
              username: string
              /**
               * Password
               * @description Password for the Milvus instance
               */
              password: string
            },
            {
              /**
               * Mode
               * @default no_auth
               * @constant
               * @enum {string}
               */
              mode: 'no_auth'
            },
          ]
        >
        /**
         * Vector Field
         * @description The field in the entity that contains the vector
         * @default vector
         */
        vector_field?: string
        /**
         * Text Field
         * @description The field in the entity that contains the embedded text
         * @default text
         */
        text_field?: string
      }
    }
    /** Firebolt Spec */
    'destination-firebolt': {
      /**
       * Username
       * @description Firebolt email address you use to login.
       */
      username: string
      /**
       * Password
       * @description Firebolt password.
       */
      password: string
      /**
       * Account
       * @description Firebolt account to login.
       */
      account?: string
      /**
       * Host
       * @description The host name of your Firebolt database.
       */
      host?: string
      /**
       * Database
       * @description The database to connect to.
       */
      database: string
      /**
       * Engine
       * @description Engine name or url to connect to.
       */
      engine?: string
      /**
       * Loading Method
       * @description Loading method used to select the way data will be uploaded to Firebolt
       */
      loading_method?: OneOf<
        [
          {
            /**
             * @constant
             * @enum {string}
             */
            method: 'SQL'
          },
          {
            /**
             * @constant
             * @enum {string}
             */
            method: 'S3'
            /**
             * S3 bucket name
             * @description The name of the S3 bucket.
             */
            s3_bucket: string
            /**
             * S3 region name
             * @description Region name of the S3 bucket.
             */
            s3_region: string
            /**
             * AWS Key ID
             * @description AWS access key granting read and write access to S3.
             */
            aws_key_id: string
            /**
             * AWS Key Secret
             * @description Corresponding secret part of the AWS Key
             */
            aws_key_secret: string
          },
        ]
      >
      /**
       * firebolt
       * @constant
       * @enum {string}
       */
      destinationType: 'firebolt'
    }
    /** Firebolt Spec */
    'destination-firebolt-update': {
      /**
       * Username
       * @description Firebolt email address you use to login.
       */
      username: string
      /**
       * Password
       * @description Firebolt password.
       */
      password: string
      /**
       * Account
       * @description Firebolt account to login.
       */
      account?: string
      /**
       * Host
       * @description The host name of your Firebolt database.
       */
      host?: string
      /**
       * Database
       * @description The database to connect to.
       */
      database: string
      /**
       * Engine
       * @description Engine name or url to connect to.
       */
      engine?: string
      /**
       * Loading Method
       * @description Loading method used to select the way data will be uploaded to Firebolt
       */
      loading_method?: OneOf<
        [
          {
            /**
             * @constant
             * @enum {string}
             */
            method: 'SQL'
          },
          {
            /**
             * @constant
             * @enum {string}
             */
            method: 'S3'
            /**
             * S3 bucket name
             * @description The name of the S3 bucket.
             */
            s3_bucket: string
            /**
             * S3 region name
             * @description Region name of the S3 bucket.
             */
            s3_region: string
            /**
             * AWS Key ID
             * @description AWS access key granting read and write access to S3.
             */
            aws_key_id: string
            /**
             * AWS Key Secret
             * @description Corresponding secret part of the AWS Key
             */
            aws_key_secret: string
          },
        ]
      >
    }
    /** Destination Google Sheets */
    'destination-google-sheets': {
      /**
       * Spreadsheet Link
       * @description The link to your spreadsheet. See <a href='https://docs.airbyte.com/integrations/destinations/google-sheets#sheetlink'>this guide</a> for more details.
       */
      spreadsheet_id: string
      /**
       * Authentication via Google (OAuth)
       * @description Google API Credentials for connecting to Google Sheets and Google Drive APIs
       */
      credentials: {
        /**
         * Client ID
         * @description The Client ID of your Google Sheets developer application.
         */
        client_id: string
        /**
         * Client Secret
         * @description The Client Secret of your Google Sheets developer application.
         */
        client_secret: string
        /**
         * Refresh Token
         * @description The token for obtaining new access token.
         */
        refresh_token: string
      }
      /**
       * google-sheets
       * @constant
       * @enum {string}
       */
      destinationType: 'google-sheets'
    }
    /** Destination Google Sheets */
    'destination-google-sheets-update': {
      /**
       * Spreadsheet Link
       * @description The link to your spreadsheet. See <a href='https://docs.airbyte.com/integrations/destinations/google-sheets#sheetlink'>this guide</a> for more details.
       */
      spreadsheet_id: string
      /**
       * Authentication via Google (OAuth)
       * @description Google API Credentials for connecting to Google Sheets and Google Drive APIs
       */
      credentials: {
        /**
         * Client ID
         * @description The Client ID of your Google Sheets developer application.
         */
        client_id: string
        /**
         * Client Secret
         * @description The Client Secret of your Google Sheets developer application.
         */
        client_secret: string
        /**
         * Refresh Token
         * @description The token for obtaining new access token.
         */
        refresh_token: string
      }
    }
    /**
     * Destination Config
     * @description The configuration model for the Vector DB based destinations. This model is used to generate the UI for the destination configuration,
     * as well as to provide type safety for the configuration passed to the destination.
     *
     * The configuration model is composed of four parts:
     * * Processing configuration
     * * Embedding configuration
     * * Indexing configuration
     * * Advanced configuration
     *
     * Processing, embedding and advanced configuration are provided by this base class, while the indexing configuration is provided by the destination connector in the sub class.
     */
    'destination-astra': {
      /**
       * Embedding
       * @description Embedding configuration
       */
      embedding: OneOf<
        [
          {
            /**
             * Mode
             * @default openai
             * @constant
             * @enum {string}
             */
            mode: 'openai'
            /** OpenAI API key */
            openai_key: string
          },
          {
            /**
             * Mode
             * @default cohere
             * @constant
             * @enum {string}
             */
            mode: 'cohere'
            /** Cohere API key */
            cohere_key: string
          },
          {
            /**
             * Mode
             * @default fake
             * @constant
             * @enum {string}
             */
            mode: 'fake'
          },
          {
            /**
             * Mode
             * @default azure_openai
             * @constant
             * @enum {string}
             */
            mode: 'azure_openai'
            /**
             * Azure OpenAI API key
             * @description The API key for your Azure OpenAI resource.  You can find this in the Azure portal under your Azure OpenAI resource
             */
            openai_key: string
            /**
             * Resource base URL
             * @description The base URL for your Azure OpenAI resource.  You can find this in the Azure portal under your Azure OpenAI resource
             */
            api_base: string
            /**
             * Deployment
             * @description The deployment for your Azure OpenAI resource.  You can find this in the Azure portal under your Azure OpenAI resource
             */
            deployment: string
          },
          {
            /**
             * Mode
             * @default openai_compatible
             * @constant
             * @enum {string}
             */
            mode: 'openai_compatible'
            /**
             * API key
             * @default
             */
            api_key?: string
            /**
             * Base URL
             * @description The base URL for your OpenAI-compatible service
             */
            base_url: string
            /**
             * Model name
             * @description The name of the model to use for embedding
             * @default text-embedding-ada-002
             */
            model_name?: string
            /**
             * Embedding dimensions
             * @description The number of dimensions the embedding model is generating
             */
            dimensions: number
          },
        ]
      >
      /** ProcessingConfigModel */
      processing: {
        /**
         * Chunk size
         * @description Size of chunks in tokens to store in vector store (make sure it is not too big for the context if your LLM)
         */
        chunk_size: number
        /**
         * Chunk overlap
         * @description Size of overlap between chunks in tokens to store in vector store to better capture relevant context
         * @default 0
         */
        chunk_overlap?: number
        /**
         * Text fields to embed
         * @description List of fields in the record that should be used to calculate the embedding. The field list is applied to all streams in the same way and non-existing fields are ignored. If none are defined, all fields are considered text fields. When specifying text fields, you can access nested fields in the record by using dot notation, e.g. `user.name` will access the `name` field in the `user` object. It's also possible to use wildcards to access all fields in an object, e.g. `users.*.name` will access all `names` fields in all entries of the `users` array.
         * @default []
         */
        text_fields?: string[]
        /**
         * Fields to store as metadata
         * @description List of fields in the record that should be stored as metadata. The field list is applied to all streams in the same way and non-existing fields are ignored. If none are defined, all fields are considered metadata fields. When specifying text fields, you can access nested fields in the record by using dot notation, e.g. `user.name` will access the `name` field in the `user` object. It's also possible to use wildcards to access all fields in an object, e.g. `users.*.name` will access all `names` fields in all entries of the `users` array. When specifying nested paths, all matching values are flattened into an array set to a field named by the path.
         * @default []
         */
        metadata_fields?: string[]
        /**
         * Text splitter
         * @description Split text fields into chunks based on the specified method.
         */
        text_splitter?: OneOf<
          [
            {
              /**
               * Mode
               * @default separator
               * @constant
               * @enum {string}
               */
              mode: 'separator'
              /**
               * Separators
               * @description List of separator strings to split text fields by. The separator itself needs to be wrapped in double quotes, e.g. to split by the dot character, use ".". To split by a newline, use "\n".
               * @default [
               *   "\"\\n\\n\"",
               *   "\"\\n\"",
               *   "\" \"",
               *   "\"\""
               * ]
               */
              separators?: string[]
              /**
               * Keep separator
               * @description Whether to keep the separator in the resulting chunks
               * @default false
               */
              keep_separator?: boolean
            },
            {
              /**
               * Mode
               * @default markdown
               * @constant
               * @enum {string}
               */
              mode: 'markdown'
              /**
               * Split level
               * @description Level of markdown headers to split text fields by. Headings down to the specified level will be used as split points
               * @default 1
               */
              split_level?: number
            },
            {
              /**
               * Mode
               * @default code
               * @constant
               * @enum {string}
               */
              mode: 'code'
              /**
               * Language
               * @description Split code in suitable places based on the programming language
               * @enum {string}
               */
              language:
                | 'cpp'
                | 'go'
                | 'java'
                | 'js'
                | 'php'
                | 'proto'
                | 'python'
                | 'rst'
                | 'ruby'
                | 'rust'
                | 'scala'
                | 'swift'
                | 'markdown'
                | 'latex'
                | 'html'
                | 'sol'
            },
          ]
        >
        /**
         * Field name mappings
         * @description List of fields to rename. Not applicable for nested fields, but can be used to rename fields already flattened via dot notation.
         * @default []
         */
        field_name_mappings?: {
          /**
           * From field name
           * @description The field name in the source
           */
          from_field: string
          /**
           * To field name
           * @description The field name to use in the destination
           */
          to_field: string
        }[]
      }
      /**
       * Do not store raw text
       * @description Do not store the text that gets embedded along with the vector and the metadata in the destination. If set to true, only the vector and the metadata will be stored - in this case raw text for LLM use cases needs to be retrieved from another source.
       * @default false
       */
      omit_raw_text?: boolean
      /**
       * Indexing
       * @description Astra DB gives developers the APIs, real-time data and ecosystem integrations to put accurate RAG and Gen AI apps with fewer hallucinations in production.
       */
      indexing: {
        /**
         * Astra DB Application Token
         * @description The application token authorizes a user to connect to a specific Astra DB database. It is created when the user clicks the Generate Token button on the Overview tab of the Database page in the Astra UI.
         */
        astra_db_app_token: string
        /**
         * Astra DB Endpoint
         * @description The endpoint specifies which Astra DB database queries are sent to. It can be copied from the Database Details section of the Overview tab of the Database page in the Astra UI.
         */
        astra_db_endpoint: string
        /**
         * Astra DB Keyspace
         * @description Keyspaces (or Namespaces) serve as containers for organizing data within a database. You can create a new keyspace uisng the Data Explorer tab in the Astra UI. The keyspace default_keyspace is created for you when you create a Vector Database in Astra DB.
         */
        astra_db_keyspace: string
        /**
         * Astra DB collection
         * @description Collections hold data. They are analagous to tables in traditional Cassandra terminology. This tool will create the collection with the provided name automatically if it does not already exist. Alternatively, you can create one thorugh the Data Explorer tab in the Astra UI.
         */
        collection: string
      }
      /**
       * astra
       * @constant
       * @enum {string}
       */
      destinationType: 'astra'
    }
    /**
     * Destination Config
     * @description The configuration model for the Vector DB based destinations. This model is used to generate the UI for the destination configuration,
     * as well as to provide type safety for the configuration passed to the destination.
     *
     * The configuration model is composed of four parts:
     * * Processing configuration
     * * Embedding configuration
     * * Indexing configuration
     * * Advanced configuration
     *
     * Processing, embedding and advanced configuration are provided by this base class, while the indexing configuration is provided by the destination connector in the sub class.
     */
    'destination-astra-update': {
      /**
       * Embedding
       * @description Embedding configuration
       */
      embedding: OneOf<
        [
          {
            /**
             * Mode
             * @default openai
             * @constant
             * @enum {string}
             */
            mode: 'openai'
            /** OpenAI API key */
            openai_key: string
          },
          {
            /**
             * Mode
             * @default cohere
             * @constant
             * @enum {string}
             */
            mode: 'cohere'
            /** Cohere API key */
            cohere_key: string
          },
          {
            /**
             * Mode
             * @default fake
             * @constant
             * @enum {string}
             */
            mode: 'fake'
          },
          {
            /**
             * Mode
             * @default azure_openai
             * @constant
             * @enum {string}
             */
            mode: 'azure_openai'
            /**
             * Azure OpenAI API key
             * @description The API key for your Azure OpenAI resource.  You can find this in the Azure portal under your Azure OpenAI resource
             */
            openai_key: string
            /**
             * Resource base URL
             * @description The base URL for your Azure OpenAI resource.  You can find this in the Azure portal under your Azure OpenAI resource
             */
            api_base: string
            /**
             * Deployment
             * @description The deployment for your Azure OpenAI resource.  You can find this in the Azure portal under your Azure OpenAI resource
             */
            deployment: string
          },
          {
            /**
             * Mode
             * @default openai_compatible
             * @constant
             * @enum {string}
             */
            mode: 'openai_compatible'
            /**
             * API key
             * @default
             */
            api_key?: string
            /**
             * Base URL
             * @description The base URL for your OpenAI-compatible service
             */
            base_url: string
            /**
             * Model name
             * @description The name of the model to use for embedding
             * @default text-embedding-ada-002
             */
            model_name?: string
            /**
             * Embedding dimensions
             * @description The number of dimensions the embedding model is generating
             */
            dimensions: number
          },
        ]
      >
      /** ProcessingConfigModel */
      processing: {
        /**
         * Chunk size
         * @description Size of chunks in tokens to store in vector store (make sure it is not too big for the context if your LLM)
         */
        chunk_size: number
        /**
         * Chunk overlap
         * @description Size of overlap between chunks in tokens to store in vector store to better capture relevant context
         * @default 0
         */
        chunk_overlap?: number
        /**
         * Text fields to embed
         * @description List of fields in the record that should be used to calculate the embedding. The field list is applied to all streams in the same way and non-existing fields are ignored. If none are defined, all fields are considered text fields. When specifying text fields, you can access nested fields in the record by using dot notation, e.g. `user.name` will access the `name` field in the `user` object. It's also possible to use wildcards to access all fields in an object, e.g. `users.*.name` will access all `names` fields in all entries of the `users` array.
         * @default []
         */
        text_fields?: string[]
        /**
         * Fields to store as metadata
         * @description List of fields in the record that should be stored as metadata. The field list is applied to all streams in the same way and non-existing fields are ignored. If none are defined, all fields are considered metadata fields. When specifying text fields, you can access nested fields in the record by using dot notation, e.g. `user.name` will access the `name` field in the `user` object. It's also possible to use wildcards to access all fields in an object, e.g. `users.*.name` will access all `names` fields in all entries of the `users` array. When specifying nested paths, all matching values are flattened into an array set to a field named by the path.
         * @default []
         */
        metadata_fields?: string[]
        /**
         * Text splitter
         * @description Split text fields into chunks based on the specified method.
         */
        text_splitter?: OneOf<
          [
            {
              /**
               * Mode
               * @default separator
               * @constant
               * @enum {string}
               */
              mode: 'separator'
              /**
               * Separators
               * @description List of separator strings to split text fields by. The separator itself needs to be wrapped in double quotes, e.g. to split by the dot character, use ".". To split by a newline, use "\n".
               * @default [
               *   "\"\\n\\n\"",
               *   "\"\\n\"",
               *   "\" \"",
               *   "\"\""
               * ]
               */
              separators?: string[]
              /**
               * Keep separator
               * @description Whether to keep the separator in the resulting chunks
               * @default false
               */
              keep_separator?: boolean
            },
            {
              /**
               * Mode
               * @default markdown
               * @constant
               * @enum {string}
               */
              mode: 'markdown'
              /**
               * Split level
               * @description Level of markdown headers to split text fields by. Headings down to the specified level will be used as split points
               * @default 1
               */
              split_level?: number
            },
            {
              /**
               * Mode
               * @default code
               * @constant
               * @enum {string}
               */
              mode: 'code'
              /**
               * Language
               * @description Split code in suitable places based on the programming language
               * @enum {string}
               */
              language:
                | 'cpp'
                | 'go'
                | 'java'
                | 'js'
                | 'php'
                | 'proto'
                | 'python'
                | 'rst'
                | 'ruby'
                | 'rust'
                | 'scala'
                | 'swift'
                | 'markdown'
                | 'latex'
                | 'html'
                | 'sol'
            },
          ]
        >
        /**
         * Field name mappings
         * @description List of fields to rename. Not applicable for nested fields, but can be used to rename fields already flattened via dot notation.
         * @default []
         */
        field_name_mappings?: {
          /**
           * From field name
           * @description The field name in the source
           */
          from_field: string
          /**
           * To field name
           * @description The field name to use in the destination
           */
          to_field: string
        }[]
      }
      /**
       * Do not store raw text
       * @description Do not store the text that gets embedded along with the vector and the metadata in the destination. If set to true, only the vector and the metadata will be stored - in this case raw text for LLM use cases needs to be retrieved from another source.
       * @default false
       */
      omit_raw_text?: boolean
      /**
       * Indexing
       * @description Astra DB gives developers the APIs, real-time data and ecosystem integrations to put accurate RAG and Gen AI apps with fewer hallucinations in production.
       */
      indexing: {
        /**
         * Astra DB Application Token
         * @description The application token authorizes a user to connect to a specific Astra DB database. It is created when the user clicks the Generate Token button on the Overview tab of the Database page in the Astra UI.
         */
        astra_db_app_token: string
        /**
         * Astra DB Endpoint
         * @description The endpoint specifies which Astra DB database queries are sent to. It can be copied from the Database Details section of the Overview tab of the Database page in the Astra UI.
         */
        astra_db_endpoint: string
        /**
         * Astra DB Keyspace
         * @description Keyspaces (or Namespaces) serve as containers for organizing data within a database. You can create a new keyspace uisng the Data Explorer tab in the Astra UI. The keyspace default_keyspace is created for you when you create a Vector Database in Astra DB.
         */
        astra_db_keyspace: string
        /**
         * Astra DB collection
         * @description Collections hold data. They are analagous to tables in traditional Cassandra terminology. This tool will create the collection with the provided name automatically if it does not already exist. Alternatively, you can create one thorugh the Data Explorer tab in the Astra UI.
         */
        collection: string
      }
    }
    /** Destination Databend */
    'destination-databend': {
      /**
       * Host
       * @description Hostname of the database.
       */
      host: string
      /**
       * Port
       * @description Port of the database.
       * @default 443
       */
      port?: number
      /**
       * DB Name
       * @description Name of the database.
       */
      database: string
      /**
       * Default Table
       * @description The default  table was written to.
       * @default default
       */
      table?: string
      /**
       * User
       * @description Username to use to access the database.
       */
      username: string
      /**
       * Password
       * @description Password associated with the username.
       */
      password?: string
      /**
       * databend
       * @constant
       * @enum {string}
       */
      destinationType: 'databend'
    }
    /** Destination Databend */
    'destination-databend-update': {
      /**
       * Host
       * @description Hostname of the database.
       */
      host: string
      /**
       * Port
       * @description Port of the database.
       * @default 443
       */
      port?: number
      /**
       * DB Name
       * @description Name of the database.
       */
      database: string
      /**
       * Default Table
       * @description The default  table was written to.
       * @default default
       */
      table?: string
      /**
       * User
       * @description Username to use to access the database.
       */
      username: string
      /**
       * Password
       * @description Password associated with the username.
       */
      password?: string
    }
    /** Teradata Destination Spec */
    'destination-teradata': {
      /**
       * Host
       * @description Hostname of the database.
       */
      host: string
      /**
       * User
       * @description Username to use to access the database.
       */
      username: string
      /**
       * Password
       * @description Password associated with the username.
       */
      password?: string
      /**
       * Default Schema
       * @description The default schema tables are written to if the source does not specify a namespace. The usual value for this field is "public".
       * @default airbyte_td
       */
      schema?: string
      /**
       * SSL Connection
       * @description Encrypt data using SSL. When activating SSL, please select one of the connection modes.
       * @default false
       */
      ssl?: boolean
      /**
       * SSL modes
       * @description SSL connection modes.
       *  <b>disable</b> - Chose this mode to disable encryption of communication between Airbyte and destination database
       *  <b>allow</b> - Chose this mode to enable encryption only when required by the destination database
       *  <b>prefer</b> - Chose this mode to allow unencrypted connection only if the destination database does not support encryption
       *  <b>require</b> - Chose this mode to always require encryption. If the destination database server does not support encryption, connection will fail
       *   <b>verify-ca</b> - Chose this mode to always require encryption and to verify that the destination database server has a valid SSL certificate
       *   <b>verify-full</b> - This is the most secure mode. Chose this mode to always require encryption and to verify the identity of the destination database server
       *  See more information - <a href="https://teradata-docs.s3.amazonaws.com/doc/connectivity/jdbc/reference/current/jdbcug_chapter_2.html#URL_SSLMODE"> in the docs</a>.
       */
      ssl_mode?:
        | {
            /**
             * @default disable
             * @constant
             * @enum {string}
             */
            mode: 'disable'
          }
        | {
            /**
             * @default allow
             * @constant
             * @enum {string}
             */
            mode: 'allow'
          }
        | {
            /**
             * @default prefer
             * @constant
             * @enum {string}
             */
            mode: 'prefer'
          }
        | {
            /**
             * @default require
             * @constant
             * @enum {string}
             */
            mode: 'require'
          }
        | {
            /**
             * @default verify-ca
             * @constant
             * @enum {string}
             */
            mode: 'verify-ca'
            /**
             * CA certificate
             * @description Specifies the file name of a PEM file that contains Certificate Authority (CA) certificates for use with SSLMODE=verify-ca.
             *  See more information - <a href="https://teradata-docs.s3.amazonaws.com/doc/connectivity/jdbc/reference/current/jdbcug_chapter_2.html#URL_SSLCA"> in the docs</a>.
             */
            ssl_ca_certificate: string
          }
        | {
            /**
             * @default verify-full
             * @constant
             * @enum {string}
             */
            mode: 'verify-full'
            /**
             * CA certificate
             * @description Specifies the file name of a PEM file that contains Certificate Authority (CA) certificates for use with SSLMODE=verify-full.
             *  See more information - <a href="https://teradata-docs.s3.amazonaws.com/doc/connectivity/jdbc/reference/current/jdbcug_chapter_2.html#URL_SSLCA"> in the docs</a>.
             */
            ssl_ca_certificate: string
          }
      /**
       * JDBC URL Params
       * @description Additional properties to pass to the JDBC URL string when connecting to the database formatted as 'key=value' pairs separated by the symbol '&'. (example: key1=value1&key2=value2&key3=value3).
       */
      jdbc_url_params?: string
      /**
       * teradata
       * @constant
       * @enum {string}
       */
      destinationType: 'teradata'
    }
    /** Teradata Destination Spec */
    'destination-teradata-update': {
      /**
       * Host
       * @description Hostname of the database.
       */
      host: string
      /**
       * User
       * @description Username to use to access the database.
       */
      username: string
      /**
       * Password
       * @description Password associated with the username.
       */
      password?: string
      /**
       * Default Schema
       * @description The default schema tables are written to if the source does not specify a namespace. The usual value for this field is "public".
       * @default airbyte_td
       */
      schema?: string
      /**
       * SSL Connection
       * @description Encrypt data using SSL. When activating SSL, please select one of the connection modes.
       * @default false
       */
      ssl?: boolean
      /**
       * SSL modes
       * @description SSL connection modes.
       *  <b>disable</b> - Chose this mode to disable encryption of communication between Airbyte and destination database
       *  <b>allow</b> - Chose this mode to enable encryption only when required by the destination database
       *  <b>prefer</b> - Chose this mode to allow unencrypted connection only if the destination database does not support encryption
       *  <b>require</b> - Chose this mode to always require encryption. If the destination database server does not support encryption, connection will fail
       *   <b>verify-ca</b> - Chose this mode to always require encryption and to verify that the destination database server has a valid SSL certificate
       *   <b>verify-full</b> - This is the most secure mode. Chose this mode to always require encryption and to verify the identity of the destination database server
       *  See more information - <a href="https://teradata-docs.s3.amazonaws.com/doc/connectivity/jdbc/reference/current/jdbcug_chapter_2.html#URL_SSLMODE"> in the docs</a>.
       */
      ssl_mode?:
        | {
            /**
             * @default disable
             * @constant
             * @enum {string}
             */
            mode: 'disable'
          }
        | {
            /**
             * @default allow
             * @constant
             * @enum {string}
             */
            mode: 'allow'
          }
        | {
            /**
             * @default prefer
             * @constant
             * @enum {string}
             */
            mode: 'prefer'
          }
        | {
            /**
             * @default require
             * @constant
             * @enum {string}
             */
            mode: 'require'
          }
        | {
            /**
             * @default verify-ca
             * @constant
             * @enum {string}
             */
            mode: 'verify-ca'
            /**
             * CA certificate
             * @description Specifies the file name of a PEM file that contains Certificate Authority (CA) certificates for use with SSLMODE=verify-ca.
             *  See more information - <a href="https://teradata-docs.s3.amazonaws.com/doc/connectivity/jdbc/reference/current/jdbcug_chapter_2.html#URL_SSLCA"> in the docs</a>.
             */
            ssl_ca_certificate: string
          }
        | {
            /**
             * @default verify-full
             * @constant
             * @enum {string}
             */
            mode: 'verify-full'
            /**
             * CA certificate
             * @description Specifies the file name of a PEM file that contains Certificate Authority (CA) certificates for use with SSLMODE=verify-full.
             *  See more information - <a href="https://teradata-docs.s3.amazonaws.com/doc/connectivity/jdbc/reference/current/jdbcug_chapter_2.html#URL_SSLCA"> in the docs</a>.
             */
            ssl_ca_certificate: string
          }
      /**
       * JDBC URL Params
       * @description Additional properties to pass to the JDBC URL string when connecting to the database formatted as 'key=value' pairs separated by the symbol '&'. (example: key1=value1&key2=value2&key3=value3).
       */
      jdbc_url_params?: string
    }
    /**
     * Destination Config
     * @description The configuration model for the Vector DB based destinations. This model is used to generate the UI for the destination configuration,
     * as well as to provide type safety for the configuration passed to the destination.
     *
     * The configuration model is composed of four parts:
     * * Processing configuration
     * * Embedding configuration
     * * Indexing configuration
     * * Advanced configuration
     *
     * Processing, embedding and advanced configuration are provided by this base class, while the indexing configuration is provided by the destination connector in the sub class.
     */
    'destination-pinecone': {
      /**
       * Embedding
       * @description Embedding configuration
       */
      embedding: OneOf<
        [
          {
            /**
             * Mode
             * @default openai
             * @constant
             * @enum {string}
             */
            mode: 'openai'
            /** OpenAI API key */
            openai_key: string
          },
          {
            /**
             * Mode
             * @default cohere
             * @constant
             * @enum {string}
             */
            mode: 'cohere'
            /** Cohere API key */
            cohere_key: string
          },
          {
            /**
             * Mode
             * @default fake
             * @constant
             * @enum {string}
             */
            mode: 'fake'
          },
          {
            /**
             * Mode
             * @default azure_openai
             * @constant
             * @enum {string}
             */
            mode: 'azure_openai'
            /**
             * Azure OpenAI API key
             * @description The API key for your Azure OpenAI resource.  You can find this in the Azure portal under your Azure OpenAI resource
             */
            openai_key: string
            /**
             * Resource base URL
             * @description The base URL for your Azure OpenAI resource.  You can find this in the Azure portal under your Azure OpenAI resource
             */
            api_base: string
            /**
             * Deployment
             * @description The deployment for your Azure OpenAI resource.  You can find this in the Azure portal under your Azure OpenAI resource
             */
            deployment: string
          },
          {
            /**
             * Mode
             * @default openai_compatible
             * @constant
             * @enum {string}
             */
            mode: 'openai_compatible'
            /**
             * API key
             * @default
             */
            api_key?: string
            /**
             * Base URL
             * @description The base URL for your OpenAI-compatible service
             */
            base_url: string
            /**
             * Model name
             * @description The name of the model to use for embedding
             * @default text-embedding-ada-002
             */
            model_name?: string
            /**
             * Embedding dimensions
             * @description The number of dimensions the embedding model is generating
             */
            dimensions: number
          },
        ]
      >
      /** ProcessingConfigModel */
      processing: {
        /**
         * Chunk size
         * @description Size of chunks in tokens to store in vector store (make sure it is not too big for the context if your LLM)
         */
        chunk_size: number
        /**
         * Chunk overlap
         * @description Size of overlap between chunks in tokens to store in vector store to better capture relevant context
         * @default 0
         */
        chunk_overlap?: number
        /**
         * Text fields to embed
         * @description List of fields in the record that should be used to calculate the embedding. The field list is applied to all streams in the same way and non-existing fields are ignored. If none are defined, all fields are considered text fields. When specifying text fields, you can access nested fields in the record by using dot notation, e.g. `user.name` will access the `name` field in the `user` object. It's also possible to use wildcards to access all fields in an object, e.g. `users.*.name` will access all `names` fields in all entries of the `users` array.
         * @default []
         */
        text_fields?: string[]
        /**
         * Fields to store as metadata
         * @description List of fields in the record that should be stored as metadata. The field list is applied to all streams in the same way and non-existing fields are ignored. If none are defined, all fields are considered metadata fields. When specifying text fields, you can access nested fields in the record by using dot notation, e.g. `user.name` will access the `name` field in the `user` object. It's also possible to use wildcards to access all fields in an object, e.g. `users.*.name` will access all `names` fields in all entries of the `users` array. When specifying nested paths, all matching values are flattened into an array set to a field named by the path.
         * @default []
         */
        metadata_fields?: string[]
        /**
         * Text splitter
         * @description Split text fields into chunks based on the specified method.
         */
        text_splitter?: OneOf<
          [
            {
              /**
               * Mode
               * @default separator
               * @constant
               * @enum {string}
               */
              mode: 'separator'
              /**
               * Separators
               * @description List of separator strings to split text fields by. The separator itself needs to be wrapped in double quotes, e.g. to split by the dot character, use ".". To split by a newline, use "\n".
               * @default [
               *   "\"\\n\\n\"",
               *   "\"\\n\"",
               *   "\" \"",
               *   "\"\""
               * ]
               */
              separators?: string[]
              /**
               * Keep separator
               * @description Whether to keep the separator in the resulting chunks
               * @default false
               */
              keep_separator?: boolean
            },
            {
              /**
               * Mode
               * @default markdown
               * @constant
               * @enum {string}
               */
              mode: 'markdown'
              /**
               * Split level
               * @description Level of markdown headers to split text fields by. Headings down to the specified level will be used as split points
               * @default 1
               */
              split_level?: number
            },
            {
              /**
               * Mode
               * @default code
               * @constant
               * @enum {string}
               */
              mode: 'code'
              /**
               * Language
               * @description Split code in suitable places based on the programming language
               * @enum {string}
               */
              language:
                | 'cpp'
                | 'go'
                | 'java'
                | 'js'
                | 'php'
                | 'proto'
                | 'python'
                | 'rst'
                | 'ruby'
                | 'rust'
                | 'scala'
                | 'swift'
                | 'markdown'
                | 'latex'
                | 'html'
                | 'sol'
            },
          ]
        >
        /**
         * Field name mappings
         * @description List of fields to rename. Not applicable for nested fields, but can be used to rename fields already flattened via dot notation.
         * @default []
         */
        field_name_mappings?: {
          /**
           * From field name
           * @description The field name in the source
           */
          from_field: string
          /**
           * To field name
           * @description The field name to use in the destination
           */
          to_field: string
        }[]
      }
      /**
       * Do not store raw text
       * @description Do not store the text that gets embedded along with the vector and the metadata in the destination. If set to true, only the vector and the metadata will be stored - in this case raw text for LLM use cases needs to be retrieved from another source.
       * @default false
       */
      omit_raw_text?: boolean
      /**
       * Indexing
       * @description Pinecone is a popular vector store that can be used to store and retrieve embeddings.
       */
      indexing: {
        /**
         * Pinecone API key
         * @description The Pinecone API key to use matching the environment (copy from Pinecone console)
         */
        pinecone_key: string
        /**
         * Pinecone Environment
         * @description Pinecone Cloud environment to use
         */
        pinecone_environment: string
        /**
         * Index
         * @description Pinecone index in your project to load data into
         */
        index: string
      }
      /**
       * pinecone
       * @constant
       * @enum {string}
       */
      destinationType: 'pinecone'
    }
    /**
     * Destination Config
     * @description The configuration model for the Vector DB based destinations. This model is used to generate the UI for the destination configuration,
     * as well as to provide type safety for the configuration passed to the destination.
     *
     * The configuration model is composed of four parts:
     * * Processing configuration
     * * Embedding configuration
     * * Indexing configuration
     * * Advanced configuration
     *
     * Processing, embedding and advanced configuration are provided by this base class, while the indexing configuration is provided by the destination connector in the sub class.
     */
    'destination-pinecone-update': {
      /**
       * Embedding
       * @description Embedding configuration
       */
      embedding: OneOf<
        [
          {
            /**
             * Mode
             * @default openai
             * @constant
             * @enum {string}
             */
            mode: 'openai'
            /** OpenAI API key */
            openai_key: string
          },
          {
            /**
             * Mode
             * @default cohere
             * @constant
             * @enum {string}
             */
            mode: 'cohere'
            /** Cohere API key */
            cohere_key: string
          },
          {
            /**
             * Mode
             * @default fake
             * @constant
             * @enum {string}
             */
            mode: 'fake'
          },
          {
            /**
             * Mode
             * @default azure_openai
             * @constant
             * @enum {string}
             */
            mode: 'azure_openai'
            /**
             * Azure OpenAI API key
             * @description The API key for your Azure OpenAI resource.  You can find this in the Azure portal under your Azure OpenAI resource
             */
            openai_key: string
            /**
             * Resource base URL
             * @description The base URL for your Azure OpenAI resource.  You can find this in the Azure portal under your Azure OpenAI resource
             */
            api_base: string
            /**
             * Deployment
             * @description The deployment for your Azure OpenAI resource.  You can find this in the Azure portal under your Azure OpenAI resource
             */
            deployment: string
          },
          {
            /**
             * Mode
             * @default openai_compatible
             * @constant
             * @enum {string}
             */
            mode: 'openai_compatible'
            /**
             * API key
             * @default
             */
            api_key?: string
            /**
             * Base URL
             * @description The base URL for your OpenAI-compatible service
             */
            base_url: string
            /**
             * Model name
             * @description The name of the model to use for embedding
             * @default text-embedding-ada-002
             */
            model_name?: string
            /**
             * Embedding dimensions
             * @description The number of dimensions the embedding model is generating
             */
            dimensions: number
          },
        ]
      >
      /** ProcessingConfigModel */
      processing: {
        /**
         * Chunk size
         * @description Size of chunks in tokens to store in vector store (make sure it is not too big for the context if your LLM)
         */
        chunk_size: number
        /**
         * Chunk overlap
         * @description Size of overlap between chunks in tokens to store in vector store to better capture relevant context
         * @default 0
         */
        chunk_overlap?: number
        /**
         * Text fields to embed
         * @description List of fields in the record that should be used to calculate the embedding. The field list is applied to all streams in the same way and non-existing fields are ignored. If none are defined, all fields are considered text fields. When specifying text fields, you can access nested fields in the record by using dot notation, e.g. `user.name` will access the `name` field in the `user` object. It's also possible to use wildcards to access all fields in an object, e.g. `users.*.name` will access all `names` fields in all entries of the `users` array.
         * @default []
         */
        text_fields?: string[]
        /**
         * Fields to store as metadata
         * @description List of fields in the record that should be stored as metadata. The field list is applied to all streams in the same way and non-existing fields are ignored. If none are defined, all fields are considered metadata fields. When specifying text fields, you can access nested fields in the record by using dot notation, e.g. `user.name` will access the `name` field in the `user` object. It's also possible to use wildcards to access all fields in an object, e.g. `users.*.name` will access all `names` fields in all entries of the `users` array. When specifying nested paths, all matching values are flattened into an array set to a field named by the path.
         * @default []
         */
        metadata_fields?: string[]
        /**
         * Text splitter
         * @description Split text fields into chunks based on the specified method.
         */
        text_splitter?: OneOf<
          [
            {
              /**
               * Mode
               * @default separator
               * @constant
               * @enum {string}
               */
              mode: 'separator'
              /**
               * Separators
               * @description List of separator strings to split text fields by. The separator itself needs to be wrapped in double quotes, e.g. to split by the dot character, use ".". To split by a newline, use "\n".
               * @default [
               *   "\"\\n\\n\"",
               *   "\"\\n\"",
               *   "\" \"",
               *   "\"\""
               * ]
               */
              separators?: string[]
              /**
               * Keep separator
               * @description Whether to keep the separator in the resulting chunks
               * @default false
               */
              keep_separator?: boolean
            },
            {
              /**
               * Mode
               * @default markdown
               * @constant
               * @enum {string}
               */
              mode: 'markdown'
              /**
               * Split level
               * @description Level of markdown headers to split text fields by. Headings down to the specified level will be used as split points
               * @default 1
               */
              split_level?: number
            },
            {
              /**
               * Mode
               * @default code
               * @constant
               * @enum {string}
               */
              mode: 'code'
              /**
               * Language
               * @description Split code in suitable places based on the programming language
               * @enum {string}
               */
              language:
                | 'cpp'
                | 'go'
                | 'java'
                | 'js'
                | 'php'
                | 'proto'
                | 'python'
                | 'rst'
                | 'ruby'
                | 'rust'
                | 'scala'
                | 'swift'
                | 'markdown'
                | 'latex'
                | 'html'
                | 'sol'
            },
          ]
        >
        /**
         * Field name mappings
         * @description List of fields to rename. Not applicable for nested fields, but can be used to rename fields already flattened via dot notation.
         * @default []
         */
        field_name_mappings?: {
          /**
           * From field name
           * @description The field name in the source
           */
          from_field: string
          /**
           * To field name
           * @description The field name to use in the destination
           */
          to_field: string
        }[]
      }
      /**
       * Do not store raw text
       * @description Do not store the text that gets embedded along with the vector and the metadata in the destination. If set to true, only the vector and the metadata will be stored - in this case raw text for LLM use cases needs to be retrieved from another source.
       * @default false
       */
      omit_raw_text?: boolean
      /**
       * Indexing
       * @description Pinecone is a popular vector store that can be used to store and retrieve embeddings.
       */
      indexing: {
        /**
         * Pinecone API key
         * @description The Pinecone API key to use matching the environment (copy from Pinecone console)
         */
        pinecone_key: string
        /**
         * Pinecone Environment
         * @description Pinecone Cloud environment to use
         */
        pinecone_environment: string
        /**
         * Index
         * @description Pinecone index in your project to load data into
         */
        index: string
      }
    }
    /** Destination Duckdb */
    'destination-duckdb': {
      /**
       * MotherDuck API Key
       * @description API key to use for authentication to a MotherDuck database.
       */
      motherduck_api_key?: string
      /**
       * Destination DB
       * @description Path to the .duckdb file, or the text 'md:' to connect to MotherDuck. The file will be placed inside that local mount. For more information check out our <a href="https://docs.airbyte.io/integrations/destinations/duckdb">docs</a>
       */
      destination_path: string
      /**
       * Destination Schema
       * @description Database schema name, default for duckdb is 'main'.
       * @example main
       */
      schema?: string
      /**
       * duckdb
       * @constant
       * @enum {string}
       */
      destinationType: 'duckdb'
    }
    /** Destination Duckdb */
    'destination-duckdb-update': {
      /**
       * MotherDuck API Key
       * @description API key to use for authentication to a MotherDuck database.
       */
      motherduck_api_key?: string
      /**
       * Destination DB
       * @description Path to the .duckdb file, or the text 'md:' to connect to MotherDuck. The file will be placed inside that local mount. For more information check out our <a href="https://docs.airbyte.io/integrations/destinations/duckdb">docs</a>
       */
      destination_path: string
      /**
       * Destination Schema
       * @description Database schema name, default for duckdb is 'main'.
       * @example main
       */
      schema?: string
    }
    /** Destination SFTP JSON */
    'destination-sftp-json': {
      /**
       * Host
       * @description Hostname of the SFTP server.
       */
      host: string
      /**
       * Port
       * @description Port of the SFTP server.
       * @default 22
       */
      port?: number
      /**
       * User
       * @description Username to use to access the SFTP server.
       */
      username: string
      /**
       * Password
       * @description Password associated with the username.
       */
      password: string
      /**
       * Destination path
       * @description Path to the directory where json files will be written.
       */
      destination_path: string
      /**
       * sftp-json
       * @constant
       * @enum {string}
       */
      destinationType: 'sftp-json'
    }
    /** Destination SFTP JSON */
    'destination-sftp-json-update': {
      /**
       * Host
       * @description Hostname of the SFTP server.
       */
      host: string
      /**
       * Port
       * @description Port of the SFTP server.
       * @default 22
       */
      port?: number
      /**
       * User
       * @description Username to use to access the SFTP server.
       */
      username: string
      /**
       * Password
       * @description Password associated with the username.
       */
      password: string
      /**
       * Destination path
       * @description Path to the directory where json files will be written.
       */
      destination_path: string
    }
    /** Kinesis Destination Spec */
    'destination-kinesis': {
      /**
       * Endpoint
       * @description AWS Kinesis endpoint.
       */
      endpoint: string
      /**
       * Region
       * @description AWS region. Your account determines the Regions that are available to you.
       */
      region: string
      /**
       * Shard Count
       * @description Number of shards to which the data should be streamed.
       * @default 5
       */
      shardCount: number
      /**
       * Access Key
       * @description Generate the AWS Access Key for current user.
       */
      accessKey: string
      /**
       * Private Key
       * @description The AWS Private Key - a string of numbers and letters that are unique for each account, also known as a "recovery phrase".
       */
      privateKey: string
      /**
       * Buffer Size
       * @description Buffer size for storing kinesis records before being batch streamed.
       * @default 100
       */
      bufferSize: number
      /**
       * kinesis
       * @constant
       * @enum {string}
       */
      destinationType: 'kinesis'
    }
    /** Kinesis Destination Spec */
    'destination-kinesis-update': {
      /**
       * Endpoint
       * @description AWS Kinesis endpoint.
       */
      endpoint: string
      /**
       * Region
       * @description AWS region. Your account determines the Regions that are available to you.
       */
      region: string
      /**
       * Shard Count
       * @description Number of shards to which the data should be streamed.
       * @default 5
       */
      shardCount: number
      /**
       * Access Key
       * @description Generate the AWS Access Key for current user.
       */
      accessKey: string
      /**
       * Private Key
       * @description The AWS Private Key - a string of numbers and letters that are unique for each account, also known as a "recovery phrase".
       */
      privateKey: string
      /**
       * Buffer Size
       * @description Buffer size for storing kinesis records before being batch streamed.
       * @default 100
       */
      bufferSize: number
    }
    /** S3 Destination Spec */
    'destination-s3': {
      /**
       * S3 Key ID
       * @description The access key ID to access the S3 bucket. Airbyte requires Read and Write permissions to the given bucket. Read more <a href="https://docs.aws.amazon.com/general/latest/gr/aws-sec-cred-types.html#access-keys-and-secret-access-keys">here</a>.
       */
      access_key_id?: string
      /**
       * S3 Access Key
       * @description The corresponding secret to the access key ID. Read more <a href="https://docs.aws.amazon.com/general/latest/gr/aws-sec-cred-types.html#access-keys-and-secret-access-keys">here</a>
       */
      secret_access_key?: string
      /**
       * S3 Bucket Name
       * @description The name of the S3 bucket. Read more <a href="https://docs.aws.amazon.com/AmazonS3/latest/userguide/create-bucket-overview.html">here</a>.
       */
      s3_bucket_name: string
      /**
       * S3 Bucket Path
       * @description Directory under the S3 bucket where data will be written. Read more <a href="https://docs.airbyte.com/integrations/destinations/s3#:~:text=to%20format%20the-,bucket%20path,-%3A">here</a>
       */
      s3_bucket_path: string
      /**
       * S3 Bucket Region
       * @description The region of the S3 bucket. See <a href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-regions-availability-zones.html#concepts-available-regions">here</a> for all region codes.
       * @default
       * @enum {string}
       */
      s3_bucket_region:
        | ''
        | 'af-south-1'
        | 'ap-east-1'
        | 'ap-northeast-1'
        | 'ap-northeast-2'
        | 'ap-northeast-3'
        | 'ap-south-1'
        | 'ap-south-2'
        | 'ap-southeast-1'
        | 'ap-southeast-2'
        | 'ap-southeast-3'
        | 'ap-southeast-4'
        | 'ca-central-1'
        | 'ca-west-1'
        | 'cn-north-1'
        | 'cn-northwest-1'
        | 'eu-central-1'
        | 'eu-central-2'
        | 'eu-north-1'
        | 'eu-south-1'
        | 'eu-south-2'
        | 'eu-west-1'
        | 'eu-west-2'
        | 'eu-west-3'
        | 'il-central-1'
        | 'me-central-1'
        | 'me-south-1'
        | 'sa-east-1'
        | 'us-east-1'
        | 'us-east-2'
        | 'us-gov-east-1'
        | 'us-gov-west-1'
        | 'us-west-1'
        | 'us-west-2'
      /**
       * Output Format
       * @description Format of the data output. See <a href="https://docs.airbyte.com/integrations/destinations/s3/#supported-output-schema">here</a> for more details
       */
      format: OneOf<
        [
          {
            /**
             * Format Type
             * @default CSV
             * @enum {string}
             */
            format_type: 'CSV'
            /**
             * Flattening
             * @description Whether the input json data should be normalized (flattened) in the output CSV. Please refer to docs for details.
             * @default No flattening
             * @enum {string}
             */
            flattening: 'No flattening' | 'Root level flattening'
            /**
             * Compression
             * @description Whether the output files should be compressed. If compression is selected, the output filename will have an extra extension (GZIP: ".csv.gz").
             */
            compression?: OneOf<
              [
                {
                  /**
                   * @default No Compression
                   * @enum {string}
                   */
                  compression_type?: 'No Compression'
                },
                {
                  /**
                   * @default GZIP
                   * @enum {string}
                   */
                  compression_type?: 'GZIP'
                },
              ]
            >
          },
          {
            /**
             * Format Type
             * @default JSONL
             * @enum {string}
             */
            format_type: 'JSONL'
            /**
             * Flattening
             * @description Whether the input json data should be normalized (flattened) in the output JSON Lines. Please refer to docs for details.
             * @default No flattening
             * @enum {string}
             */
            flattening?: 'No flattening' | 'Root level flattening'
            /**
             * Compression
             * @description Whether the output files should be compressed. If compression is selected, the output filename will have an extra extension (GZIP: ".jsonl.gz").
             */
            compression?: OneOf<
              [
                {
                  /**
                   * @default No Compression
                   * @enum {string}
                   */
                  compression_type?: 'No Compression'
                },
                {
                  /**
                   * @default GZIP
                   * @enum {string}
                   */
                  compression_type?: 'GZIP'
                },
              ]
            >
          },
          {
            /**
             * Format Type
             * @default Avro
             * @enum {string}
             */
            format_type: 'Avro'
            /**
             * Compression Codec
             * @description The compression algorithm used to compress data. Default to no compression.
             */
            compression_codec:
              | {
                  /**
                   * @default no compression
                   * @enum {string}
                   */
                  codec: 'no compression'
                }
              | {
                  /**
                   * @default Deflate
                   * @enum {string}
                   */
                  codec: 'Deflate'
                  /**
                   * Deflate Level
                   * @description 0: no compression & fastest, 9: best compression & slowest.
                   * @default 0
                   */
                  compression_level: number
                }
              | {
                  /**
                   * @default bzip2
                   * @enum {string}
                   */
                  codec: 'bzip2'
                }
              | {
                  /**
                   * @default xz
                   * @enum {string}
                   */
                  codec: 'xz'
                  /**
                   * Compression Level
                   * @description See <a href="https://commons.apache.org/proper/commons-compress/apidocs/org/apache/commons/compress/compressors/xz/XZCompressorOutputStream.html#XZCompressorOutputStream-java.io.OutputStream-int-">here</a> for details.
                   * @default 6
                   */
                  compression_level: number
                }
              | {
                  /**
                   * @default zstandard
                   * @enum {string}
                   */
                  codec: 'zstandard'
                  /**
                   * Compression Level
                   * @description Negative levels are 'fast' modes akin to lz4 or snappy, levels above 9 are generally for archival purposes, and levels above 18 use a lot of memory.
                   * @default 3
                   */
                  compression_level: number
                  /**
                   * Include Checksum
                   * @description If true, include a checksum with each data block.
                   * @default false
                   */
                  include_checksum?: boolean
                }
              | {
                  /**
                   * @default snappy
                   * @enum {string}
                   */
                  codec: 'snappy'
                }
          },
          {
            /**
             * Format Type
             * @default Parquet
             * @enum {string}
             */
            format_type: 'Parquet'
            /**
             * Compression Codec
             * @description The compression algorithm used to compress data pages.
             * @default UNCOMPRESSED
             * @enum {string}
             */
            compression_codec?:
              | 'UNCOMPRESSED'
              | 'SNAPPY'
              | 'GZIP'
              | 'LZO'
              | 'BROTLI'
              | 'LZ4'
              | 'ZSTD'
            /**
             * Block Size (Row Group Size) (MB)
             * @description This is the size of a row group being buffered in memory. It limits the memory usage when writing. Larger values will improve the IO when reading, but consume more memory when writing. Default: 128 MB.
             * @default 128
             */
            block_size_mb?: number
            /**
             * Max Padding Size (MB)
             * @description Maximum size allowed as padding to align row groups. This is also the minimum size of a row group. Default: 8 MB.
             * @default 8
             */
            max_padding_size_mb?: number
            /**
             * Page Size (KB)
             * @description The page size is for compression. A block is composed of pages. A page is the smallest unit that must be read fully to access a single record. If this value is too small, the compression will deteriorate. Default: 1024 KB.
             * @default 1024
             */
            page_size_kb?: number
            /**
             * Dictionary Page Size (KB)
             * @description There is one dictionary page per column per row group when dictionary encoding is used. The dictionary page size works like the page size but for dictionary. Default: 1024 KB.
             * @default 1024
             */
            dictionary_page_size_kb?: number
            /**
             * Dictionary Encoding
             * @description Default: true.
             * @default true
             */
            dictionary_encoding?: boolean
          },
        ]
      >
      /**
       * Endpoint
       * @description Your S3 endpoint url. Read more <a href="https://docs.aws.amazon.com/general/latest/gr/s3.html#:~:text=Service%20endpoints-,Amazon%20S3%20endpoints,-When%20you%20use">here</a>
       * @default
       */
      s3_endpoint?: string
      /**
       * S3 Path Format
       * @description Format string on how data will be organized inside the S3 bucket directory. Read more <a href="https://docs.airbyte.com/integrations/destinations/s3#:~:text=The%20full%20path%20of%20the%20output%20data%20with%20the%20default%20S3%20path%20format">here</a>
       */
      s3_path_format?: string
      /**
       * S3 Filename pattern
       * @description The pattern allows you to set the file-name format for the S3 staging file(s)
       */
      file_name_pattern?: string
      /**
       * s3
       * @constant
       * @enum {string}
       */
      destinationType: 's3'
    }
    /** S3 Destination Spec */
    'destination-s3-update': {
      /**
       * S3 Key ID
       * @description The access key ID to access the S3 bucket. Airbyte requires Read and Write permissions to the given bucket. Read more <a href="https://docs.aws.amazon.com/general/latest/gr/aws-sec-cred-types.html#access-keys-and-secret-access-keys">here</a>.
       */
      access_key_id?: string
      /**
       * S3 Access Key
       * @description The corresponding secret to the access key ID. Read more <a href="https://docs.aws.amazon.com/general/latest/gr/aws-sec-cred-types.html#access-keys-and-secret-access-keys">here</a>
       */
      secret_access_key?: string
      /**
       * S3 Bucket Name
       * @description The name of the S3 bucket. Read more <a href="https://docs.aws.amazon.com/AmazonS3/latest/userguide/create-bucket-overview.html">here</a>.
       */
      s3_bucket_name: string
      /**
       * S3 Bucket Path
       * @description Directory under the S3 bucket where data will be written. Read more <a href="https://docs.airbyte.com/integrations/destinations/s3#:~:text=to%20format%20the-,bucket%20path,-%3A">here</a>
       */
      s3_bucket_path: string
      /**
       * S3 Bucket Region
       * @description The region of the S3 bucket. See <a href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-regions-availability-zones.html#concepts-available-regions">here</a> for all region codes.
       * @default
       * @enum {string}
       */
      s3_bucket_region:
        | ''
        | 'af-south-1'
        | 'ap-east-1'
        | 'ap-northeast-1'
        | 'ap-northeast-2'
        | 'ap-northeast-3'
        | 'ap-south-1'
        | 'ap-south-2'
        | 'ap-southeast-1'
        | 'ap-southeast-2'
        | 'ap-southeast-3'
        | 'ap-southeast-4'
        | 'ca-central-1'
        | 'ca-west-1'
        | 'cn-north-1'
        | 'cn-northwest-1'
        | 'eu-central-1'
        | 'eu-central-2'
        | 'eu-north-1'
        | 'eu-south-1'
        | 'eu-south-2'
        | 'eu-west-1'
        | 'eu-west-2'
        | 'eu-west-3'
        | 'il-central-1'
        | 'me-central-1'
        | 'me-south-1'
        | 'sa-east-1'
        | 'us-east-1'
        | 'us-east-2'
        | 'us-gov-east-1'
        | 'us-gov-west-1'
        | 'us-west-1'
        | 'us-west-2'
      /**
       * Output Format
       * @description Format of the data output. See <a href="https://docs.airbyte.com/integrations/destinations/s3/#supported-output-schema">here</a> for more details
       */
      format: OneOf<
        [
          {
            /**
             * Format Type
             * @default CSV
             * @enum {string}
             */
            format_type: 'CSV'
            /**
             * Flattening
             * @description Whether the input json data should be normalized (flattened) in the output CSV. Please refer to docs for details.
             * @default No flattening
             * @enum {string}
             */
            flattening: 'No flattening' | 'Root level flattening'
            /**
             * Compression
             * @description Whether the output files should be compressed. If compression is selected, the output filename will have an extra extension (GZIP: ".csv.gz").
             */
            compression?: OneOf<
              [
                {
                  /**
                   * @default No Compression
                   * @enum {string}
                   */
                  compression_type?: 'No Compression'
                },
                {
                  /**
                   * @default GZIP
                   * @enum {string}
                   */
                  compression_type?: 'GZIP'
                },
              ]
            >
          },
          {
            /**
             * Format Type
             * @default JSONL
             * @enum {string}
             */
            format_type: 'JSONL'
            /**
             * Flattening
             * @description Whether the input json data should be normalized (flattened) in the output JSON Lines. Please refer to docs for details.
             * @default No flattening
             * @enum {string}
             */
            flattening?: 'No flattening' | 'Root level flattening'
            /**
             * Compression
             * @description Whether the output files should be compressed. If compression is selected, the output filename will have an extra extension (GZIP: ".jsonl.gz").
             */
            compression?: OneOf<
              [
                {
                  /**
                   * @default No Compression
                   * @enum {string}
                   */
                  compression_type?: 'No Compression'
                },
                {
                  /**
                   * @default GZIP
                   * @enum {string}
                   */
                  compression_type?: 'GZIP'
                },
              ]
            >
          },
          {
            /**
             * Format Type
             * @default Avro
             * @enum {string}
             */
            format_type: 'Avro'
            /**
             * Compression Codec
             * @description The compression algorithm used to compress data. Default to no compression.
             */
            compression_codec:
              | {
                  /**
                   * @default no compression
                   * @enum {string}
                   */
                  codec: 'no compression'
                }
              | {
                  /**
                   * @default Deflate
                   * @enum {string}
                   */
                  codec: 'Deflate'
                  /**
                   * Deflate Level
                   * @description 0: no compression & fastest, 9: best compression & slowest.
                   * @default 0
                   */
                  compression_level: number
                }
              | {
                  /**
                   * @default bzip2
                   * @enum {string}
                   */
                  codec: 'bzip2'
                }
              | {
                  /**
                   * @default xz
                   * @enum {string}
                   */
                  codec: 'xz'
                  /**
                   * Compression Level
                   * @description See <a href="https://commons.apache.org/proper/commons-compress/apidocs/org/apache/commons/compress/compressors/xz/XZCompressorOutputStream.html#XZCompressorOutputStream-java.io.OutputStream-int-">here</a> for details.
                   * @default 6
                   */
                  compression_level: number
                }
              | {
                  /**
                   * @default zstandard
                   * @enum {string}
                   */
                  codec: 'zstandard'
                  /**
                   * Compression Level
                   * @description Negative levels are 'fast' modes akin to lz4 or snappy, levels above 9 are generally for archival purposes, and levels above 18 use a lot of memory.
                   * @default 3
                   */
                  compression_level: number
                  /**
                   * Include Checksum
                   * @description If true, include a checksum with each data block.
                   * @default false
                   */
                  include_checksum?: boolean
                }
              | {
                  /**
                   * @default snappy
                   * @enum {string}
                   */
                  codec: 'snappy'
                }
          },
          {
            /**
             * Format Type
             * @default Parquet
             * @enum {string}
             */
            format_type: 'Parquet'
            /**
             * Compression Codec
             * @description The compression algorithm used to compress data pages.
             * @default UNCOMPRESSED
             * @enum {string}
             */
            compression_codec?:
              | 'UNCOMPRESSED'
              | 'SNAPPY'
              | 'GZIP'
              | 'LZO'
              | 'BROTLI'
              | 'LZ4'
              | 'ZSTD'
            /**
             * Block Size (Row Group Size) (MB)
             * @description This is the size of a row group being buffered in memory. It limits the memory usage when writing. Larger values will improve the IO when reading, but consume more memory when writing. Default: 128 MB.
             * @default 128
             */
            block_size_mb?: number
            /**
             * Max Padding Size (MB)
             * @description Maximum size allowed as padding to align row groups. This is also the minimum size of a row group. Default: 8 MB.
             * @default 8
             */
            max_padding_size_mb?: number
            /**
             * Page Size (KB)
             * @description The page size is for compression. A block is composed of pages. A page is the smallest unit that must be read fully to access a single record. If this value is too small, the compression will deteriorate. Default: 1024 KB.
             * @default 1024
             */
            page_size_kb?: number
            /**
             * Dictionary Page Size (KB)
             * @description There is one dictionary page per column per row group when dictionary encoding is used. The dictionary page size works like the page size but for dictionary. Default: 1024 KB.
             * @default 1024
             */
            dictionary_page_size_kb?: number
            /**
             * Dictionary Encoding
             * @description Default: true.
             * @default true
             */
            dictionary_encoding?: boolean
          },
        ]
      >
      /**
       * Endpoint
       * @description Your S3 endpoint url. Read more <a href="https://docs.aws.amazon.com/general/latest/gr/s3.html#:~:text=Service%20endpoints-,Amazon%20S3%20endpoints,-When%20you%20use">here</a>
       * @default
       */
      s3_endpoint?: string
      /**
       * S3 Path Format
       * @description Format string on how data will be organized inside the S3 bucket directory. Read more <a href="https://docs.airbyte.com/integrations/destinations/s3#:~:text=The%20full%20path%20of%20the%20output%20data%20with%20the%20default%20S3%20path%20format">here</a>
       */
      s3_path_format?: string
      /**
       * S3 Filename pattern
       * @description The pattern allows you to set the file-name format for the S3 staging file(s)
       */
      file_name_pattern?: string
    }
    /** Redis Destination Spec */
    'destination-redis': {
      /**
       * Host
       * @description Redis host to connect to.
       */
      host: string
      /**
       * Port
       * @description Port of Redis.
       * @default 6379
       */
      port: number
      /**
       * Username
       * @description Username associated with Redis.
       */
      username: string
      /**
       * Password
       * @description Password associated with Redis.
       */
      password?: string
      /**
       * SSL Connection
       * @description Indicates whether SSL encryption protocol will be used to connect to Redis. It is recommended to use SSL connection if possible.
       * @default false
       */
      ssl?: boolean
      /**
       * SSL Modes
       * @description SSL connection modes.
       *   <li><b>verify-full</b> - This is the most secure mode. Always require encryption and verifies the identity of the source database server
       */
      ssl_mode?: OneOf<
        [
          {
            /**
             * @default disable
             * @constant
             * @enum {string}
             */
            mode: 'disable'
          },
          {
            /**
             * @default verify-full
             * @constant
             * @enum {string}
             */
            mode: 'verify-full'
            /**
             * CA Certificate
             * @description CA certificate
             */
            ca_certificate: string
            /**
             * Client Certificate
             * @description Client certificate
             */
            client_certificate: string
            /**
             * Client Key
             * @description Client key
             */
            client_key: string
            /**
             * Client key password
             * @description Password for keystorage. If you do not add it - the password will be generated automatically.
             */
            client_key_password?: string
          },
        ]
      >
      /**
       * Cache type
       * @description Redis cache type to store data in.
       * @default hash
       * @enum {string}
       */
      cache_type: 'hash'
      /**
       * SSH Tunnel Method
       * @description Whether to initiate an SSH tunnel before connecting to the database, and if so, which kind of authentication to use.
       */
      tunnel_method?: OneOf<
        [
          {
            /**
             * @description No ssh tunnel needed to connect to database
             * @constant
             * @enum {string}
             */
            tunnel_method: 'NO_TUNNEL'
          },
          {
            /**
             * @description Connect through a jump server tunnel host using username and ssh key
             * @constant
             * @enum {string}
             */
            tunnel_method: 'SSH_KEY_AUTH'
            /**
             * SSH Tunnel Jump Server Host
             * @description Hostname of the jump server host that allows inbound ssh tunnel.
             */
            tunnel_host: string
            /**
             * SSH Connection Port
             * @description Port on the proxy/jump server that accepts inbound ssh connections.
             * @default 22
             */
            tunnel_port: number
            /**
             * SSH Login Username
             * @description OS-level username for logging into the jump server host.
             */
            tunnel_user: string
            /**
             * SSH Private Key
             * @description OS-level user account ssh key credentials in RSA PEM format ( created with ssh-keygen -t rsa -m PEM -f myuser_rsa )
             */
            ssh_key: string
          },
          {
            /**
             * @description Connect through a jump server tunnel host using username and password authentication
             * @constant
             * @enum {string}
             */
            tunnel_method: 'SSH_PASSWORD_AUTH'
            /**
             * SSH Tunnel Jump Server Host
             * @description Hostname of the jump server host that allows inbound ssh tunnel.
             */
            tunnel_host: string
            /**
             * SSH Connection Port
             * @description Port on the proxy/jump server that accepts inbound ssh connections.
             * @default 22
             */
            tunnel_port: number
            /**
             * SSH Login Username
             * @description OS-level username for logging into the jump server host
             */
            tunnel_user: string
            /**
             * Password
             * @description OS-level password for logging into the jump server host
             */
            tunnel_user_password: string
          },
        ]
      >
      /**
       * redis
       * @constant
       * @enum {string}
       */
      destinationType: 'redis'
    }
    /** Redis Destination Spec */
    'destination-redis-update': {
      /**
       * Host
       * @description Redis host to connect to.
       */
      host: string
      /**
       * Port
       * @description Port of Redis.
       * @default 6379
       */
      port: number
      /**
       * Username
       * @description Username associated with Redis.
       */
      username: string
      /**
       * Password
       * @description Password associated with Redis.
       */
      password?: string
      /**
       * SSL Connection
       * @description Indicates whether SSL encryption protocol will be used to connect to Redis. It is recommended to use SSL connection if possible.
       * @default false
       */
      ssl?: boolean
      /**
       * SSL Modes
       * @description SSL connection modes.
       *   <li><b>verify-full</b> - This is the most secure mode. Always require encryption and verifies the identity of the source database server
       */
      ssl_mode?: OneOf<
        [
          {
            /**
             * @default disable
             * @constant
             * @enum {string}
             */
            mode: 'disable'
          },
          {
            /**
             * @default verify-full
             * @constant
             * @enum {string}
             */
            mode: 'verify-full'
            /**
             * CA Certificate
             * @description CA certificate
             */
            ca_certificate: string
            /**
             * Client Certificate
             * @description Client certificate
             */
            client_certificate: string
            /**
             * Client Key
             * @description Client key
             */
            client_key: string
            /**
             * Client key password
             * @description Password for keystorage. If you do not add it - the password will be generated automatically.
             */
            client_key_password?: string
          },
        ]
      >
      /**
       * Cache type
       * @description Redis cache type to store data in.
       * @default hash
       * @enum {string}
       */
      cache_type: 'hash'
      /**
       * SSH Tunnel Method
       * @description Whether to initiate an SSH tunnel before connecting to the database, and if so, which kind of authentication to use.
       */
      tunnel_method?: OneOf<
        [
          {
            /**
             * @description No ssh tunnel needed to connect to database
             * @constant
             * @enum {string}
             */
            tunnel_method: 'NO_TUNNEL'
          },
          {
            /**
             * @description Connect through a jump server tunnel host using username and ssh key
             * @constant
             * @enum {string}
             */
            tunnel_method: 'SSH_KEY_AUTH'
            /**
             * SSH Tunnel Jump Server Host
             * @description Hostname of the jump server host that allows inbound ssh tunnel.
             */
            tunnel_host: string
            /**
             * SSH Connection Port
             * @description Port on the proxy/jump server that accepts inbound ssh connections.
             * @default 22
             */
            tunnel_port: number
            /**
             * SSH Login Username
             * @description OS-level username for logging into the jump server host.
             */
            tunnel_user: string
            /**
             * SSH Private Key
             * @description OS-level user account ssh key credentials in RSA PEM format ( created with ssh-keygen -t rsa -m PEM -f myuser_rsa )
             */
            ssh_key: string
          },
          {
            /**
             * @description Connect through a jump server tunnel host using username and password authentication
             * @constant
             * @enum {string}
             */
            tunnel_method: 'SSH_PASSWORD_AUTH'
            /**
             * SSH Tunnel Jump Server Host
             * @description Hostname of the jump server host that allows inbound ssh tunnel.
             */
            tunnel_host: string
            /**
             * SSH Connection Port
             * @description Port on the proxy/jump server that accepts inbound ssh connections.
             * @default 22
             */
            tunnel_port: number
            /**
             * SSH Login Username
             * @description OS-level username for logging into the jump server host
             */
            tunnel_user: string
            /**
             * Password
             * @description OS-level password for logging into the jump server host
             */
            tunnel_user_password: string
          },
        ]
      >
    }
    /** Destination Typesense */
    'destination-typesense': {
      /**
       * API Key
       * @description Typesense API Key
       */
      api_key: string
      /**
       * Host
       * @description Hostname of the Typesense instance without protocol.
       */
      host: string
      /**
       * Port
       * @description Port of the Typesense instance. Ex: 8108, 80, 443. Default is 443
       */
      port?: string
      /**
       * Protocol
       * @description Protocol of the Typesense instance. Ex: http or https. Default is https
       */
      protocol?: string
      /**
       * Batch size
       * @description How many documents should be imported together. Default 1000
       */
      batch_size?: number
      /**
       * typesense
       * @constant
       * @enum {string}
       */
      destinationType: 'typesense'
    }
    /** Destination Typesense */
    'destination-typesense-update': {
      /**
       * API Key
       * @description Typesense API Key
       */
      api_key: string
      /**
       * Host
       * @description Hostname of the Typesense instance without protocol.
       */
      host: string
      /**
       * Port
       * @description Port of the Typesense instance. Ex: 8108, 80, 443. Default is 443
       */
      port?: string
      /**
       * Protocol
       * @description Protocol of the Typesense instance. Ex: http or https. Default is https
       */
      protocol?: string
      /**
       * Batch size
       * @description How many documents should be imported together. Default 1000
       */
      batch_size?: number
    }
    /** BigQuery Destination Spec */
    'destination-bigquery': {
      /**
       * Project ID
       * @description The GCP project ID for the project containing the target BigQuery dataset. Read more <a href="https://cloud.google.com/resource-manager/docs/creating-managing-projects#identifying_projects">here</a>.
       */
      project_id: string
      /**
       * Dataset Location
       * @description The location of the dataset. Warning: Changes made after creation will not be applied. Read more <a href="https://cloud.google.com/bigquery/docs/locations">here</a>.
       * @enum {string}
       */
      dataset_location:
        | 'US'
        | 'EU'
        | 'asia-east1'
        | 'asia-east2'
        | 'asia-northeast1'
        | 'asia-northeast2'
        | 'asia-northeast3'
        | 'asia-south1'
        | 'asia-south2'
        | 'asia-southeast1'
        | 'asia-southeast2'
        | 'australia-southeast1'
        | 'australia-southeast2'
        | 'europe-central1'
        | 'europe-central2'
        | 'europe-north1'
        | 'europe-southwest1'
        | 'europe-west1'
        | 'europe-west2'
        | 'europe-west3'
        | 'europe-west4'
        | 'europe-west6'
        | 'europe-west7'
        | 'europe-west8'
        | 'europe-west9'
        | 'europe-west12'
        | 'me-central1'
        | 'me-central2'
        | 'me-west1'
        | 'northamerica-northeast1'
        | 'northamerica-northeast2'
        | 'southamerica-east1'
        | 'southamerica-west1'
        | 'us-central1'
        | 'us-east1'
        | 'us-east2'
        | 'us-east3'
        | 'us-east4'
        | 'us-east5'
        | 'us-south1'
        | 'us-west1'
        | 'us-west2'
        | 'us-west3'
        | 'us-west4'
      /**
       * Default Dataset ID
       * @description The default BigQuery Dataset ID that tables are replicated to if the source does not specify a namespace. Read more <a href="https://cloud.google.com/bigquery/docs/datasets#create-dataset">here</a>.
       */
      dataset_id: string
      /**
       * Loading Method
       * @description The way data will be uploaded to BigQuery.
       */
      loading_method?: OneOf<
        [
          {
            /**
             * @constant
             * @enum {string}
             */
            method: 'GCS Staging'
            /**
             * Credential
             * @description An HMAC key is a type of credential and can be associated with a service account or a user account in Cloud Storage. Read more <a href="https://cloud.google.com/storage/docs/authentication/hmackeys">here</a>.
             */
            credential: {
              /**
               * @constant
               * @enum {string}
               */
              credential_type: 'HMAC_KEY'
              /**
               * HMAC Key Access ID
               * @description HMAC key access ID. When linked to a service account, this ID is 61 characters long; when linked to a user account, it is 24 characters long.
               */
              hmac_key_access_id: string
              /**
               * HMAC Key Secret
               * @description The corresponding secret for the access ID. It is a 40-character base-64 encoded string.
               */
              hmac_key_secret: string
            }
            /**
             * GCS Bucket Name
             * @description The name of the GCS bucket. Read more <a href="https://cloud.google.com/storage/docs/naming-buckets">here</a>.
             */
            gcs_bucket_name: string
            /**
             * GCS Bucket Path
             * @description Directory under the GCS bucket where data will be written.
             */
            gcs_bucket_path: string
            /**
             * GCS Tmp Files Afterward Processing
             * @description This upload method is supposed to temporary store records in GCS bucket. By this select you can chose if these records should be removed from GCS when migration has finished. The default "Delete all tmp files from GCS" value is used if not set explicitly.
             * @default Delete all tmp files from GCS
             * @enum {string}
             */
            'keep_files_in_gcs-bucket'?:
              | 'Delete all tmp files from GCS'
              | 'Keep all tmp files in GCS'
          },
          {
            /**
             * @constant
             * @enum {string}
             */
            method: 'Standard'
          },
        ]
      >
      /**
       * Service Account Key JSON (Required for cloud, optional for open-source)
       * @description The contents of the JSON service account key. Check out the <a href="https://docs.airbyte.com/integrations/destinations/bigquery#service-account-key">docs</a> if you need help generating this key. Default credentials will be used if this field is left empty.
       */
      credentials_json?: string
      /**
       * Transformation Query Run Type
       * @description Interactive run type means that the query is executed as soon as possible, and these queries count towards concurrent rate limit and daily limit. Read more about interactive run type <a href="https://cloud.google.com/bigquery/docs/running-queries#queries">here</a>. Batch queries are queued and started as soon as idle resources are available in the BigQuery shared resource pool, which usually occurs within a few minutes. Batch queries don’t count towards your concurrent rate limit. Read more about batch queries <a href="https://cloud.google.com/bigquery/docs/running-queries#batch">here</a>. The default "interactive" value is used if not set explicitly.
       * @default interactive
       * @enum {string}
       */
      transformation_priority?: 'interactive' | 'batch'
      /**
       * Google BigQuery Client Chunk Size
       * @description Google BigQuery client's chunk (buffer) size (MIN=1, MAX = 15) for each table. The size that will be written by a single RPC. Written data will be buffered and only flushed upon reaching this size or closing the channel. The default 15MB value is used if not set explicitly. Read more <a href="https://googleapis.dev/python/bigquery/latest/generated/google.cloud.bigquery.client.Client.html">here</a>.
       * @default 15
       */
      big_query_client_buffer_size_mb?: number
      /**
       * Raw Table Dataset Name
       * @description The dataset to write raw tables into (default: airbyte_internal)
       */
      raw_data_dataset?: string
      /**
       * Disable Final Tables. (WARNING! Unstable option; Columns in raw table schema might change between versions)
       * @description Disable Writing Final Tables. WARNING! The data format in _airbyte_data is likely stable but there are no guarantees that other metadata columns will remain the same in future versions
       * @default false
       */
      disable_type_dedupe?: boolean
      /**
       * bigquery
       * @constant
       * @enum {string}
       */
      destinationType: 'bigquery'
    }
    /** BigQuery Destination Spec */
    'destination-bigquery-update': {
      /**
       * Project ID
       * @description The GCP project ID for the project containing the target BigQuery dataset. Read more <a href="https://cloud.google.com/resource-manager/docs/creating-managing-projects#identifying_projects">here</a>.
       */
      project_id: string
      /**
       * Dataset Location
       * @description The location of the dataset. Warning: Changes made after creation will not be applied. Read more <a href="https://cloud.google.com/bigquery/docs/locations">here</a>.
       * @enum {string}
       */
      dataset_location:
        | 'US'
        | 'EU'
        | 'asia-east1'
        | 'asia-east2'
        | 'asia-northeast1'
        | 'asia-northeast2'
        | 'asia-northeast3'
        | 'asia-south1'
        | 'asia-south2'
        | 'asia-southeast1'
        | 'asia-southeast2'
        | 'australia-southeast1'
        | 'australia-southeast2'
        | 'europe-central1'
        | 'europe-central2'
        | 'europe-north1'
        | 'europe-southwest1'
        | 'europe-west1'
        | 'europe-west2'
        | 'europe-west3'
        | 'europe-west4'
        | 'europe-west6'
        | 'europe-west7'
        | 'europe-west8'
        | 'europe-west9'
        | 'europe-west12'
        | 'me-central1'
        | 'me-central2'
        | 'me-west1'
        | 'northamerica-northeast1'
        | 'northamerica-northeast2'
        | 'southamerica-east1'
        | 'southamerica-west1'
        | 'us-central1'
        | 'us-east1'
        | 'us-east2'
        | 'us-east3'
        | 'us-east4'
        | 'us-east5'
        | 'us-south1'
        | 'us-west1'
        | 'us-west2'
        | 'us-west3'
        | 'us-west4'
      /**
       * Default Dataset ID
       * @description The default BigQuery Dataset ID that tables are replicated to if the source does not specify a namespace. Read more <a href="https://cloud.google.com/bigquery/docs/datasets#create-dataset">here</a>.
       */
      dataset_id: string
      /**
       * Loading Method
       * @description The way data will be uploaded to BigQuery.
       */
      loading_method?: OneOf<
        [
          {
            /**
             * @constant
             * @enum {string}
             */
            method: 'GCS Staging'
            /**
             * Credential
             * @description An HMAC key is a type of credential and can be associated with a service account or a user account in Cloud Storage. Read more <a href="https://cloud.google.com/storage/docs/authentication/hmackeys">here</a>.
             */
            credential: {
              /**
               * @constant
               * @enum {string}
               */
              credential_type: 'HMAC_KEY'
              /**
               * HMAC Key Access ID
               * @description HMAC key access ID. When linked to a service account, this ID is 61 characters long; when linked to a user account, it is 24 characters long.
               */
              hmac_key_access_id: string
              /**
               * HMAC Key Secret
               * @description The corresponding secret for the access ID. It is a 40-character base-64 encoded string.
               */
              hmac_key_secret: string
            }
            /**
             * GCS Bucket Name
             * @description The name of the GCS bucket. Read more <a href="https://cloud.google.com/storage/docs/naming-buckets">here</a>.
             */
            gcs_bucket_name: string
            /**
             * GCS Bucket Path
             * @description Directory under the GCS bucket where data will be written.
             */
            gcs_bucket_path: string
            /**
             * GCS Tmp Files Afterward Processing
             * @description This upload method is supposed to temporary store records in GCS bucket. By this select you can chose if these records should be removed from GCS when migration has finished. The default "Delete all tmp files from GCS" value is used if not set explicitly.
             * @default Delete all tmp files from GCS
             * @enum {string}
             */
            'keep_files_in_gcs-bucket'?:
              | 'Delete all tmp files from GCS'
              | 'Keep all tmp files in GCS'
          },
          {
            /**
             * @constant
             * @enum {string}
             */
            method: 'Standard'
          },
        ]
      >
      /**
       * Service Account Key JSON (Required for cloud, optional for open-source)
       * @description The contents of the JSON service account key. Check out the <a href="https://docs.airbyte.com/integrations/destinations/bigquery#service-account-key">docs</a> if you need help generating this key. Default credentials will be used if this field is left empty.
       */
      credentials_json?: string
      /**
       * Transformation Query Run Type
       * @description Interactive run type means that the query is executed as soon as possible, and these queries count towards concurrent rate limit and daily limit. Read more about interactive run type <a href="https://cloud.google.com/bigquery/docs/running-queries#queries">here</a>. Batch queries are queued and started as soon as idle resources are available in the BigQuery shared resource pool, which usually occurs within a few minutes. Batch queries don’t count towards your concurrent rate limit. Read more about batch queries <a href="https://cloud.google.com/bigquery/docs/running-queries#batch">here</a>. The default "interactive" value is used if not set explicitly.
       * @default interactive
       * @enum {string}
       */
      transformation_priority?: 'interactive' | 'batch'
      /**
       * Google BigQuery Client Chunk Size
       * @description Google BigQuery client's chunk (buffer) size (MIN=1, MAX = 15) for each table. The size that will be written by a single RPC. Written data will be buffered and only flushed upon reaching this size or closing the channel. The default 15MB value is used if not set explicitly. Read more <a href="https://googleapis.dev/python/bigquery/latest/generated/google.cloud.bigquery.client.Client.html">here</a>.
       * @default 15
       */
      big_query_client_buffer_size_mb?: number
      /**
       * Raw Table Dataset Name
       * @description The dataset to write raw tables into (default: airbyte_internal)
       */
      raw_data_dataset?: string
      /**
       * Disable Final Tables. (WARNING! Unstable option; Columns in raw table schema might change between versions)
       * @description Disable Writing Final Tables. WARNING! The data format in _airbyte_data is likely stable but there are no guarantees that other metadata columns will remain the same in future versions
       * @default false
       */
      disable_type_dedupe?: boolean
    }
    /** Vertica Destination Spec */
    'destination-vertica': {
      /**
       * Host
       * @description Hostname of the database.
       */
      host: string
      /**
       * Port
       * @description Port of the database.
       * @default 5433
       */
      port: number
      /**
       * DB Name
       * @description Name of the database.
       */
      database: string
      /**
       * User
       * @description Username to use to access the database.
       */
      username: string
      /**
       * Password
       * @description Password associated with the username.
       */
      password?: string
      /**
       * JDBC URL Params
       * @description Additional properties to pass to the JDBC URL string when connecting to the database formatted as 'key=value' pairs separated by the symbol '&'. (example: key1=value1&key2=value2&key3=value3).
       */
      jdbc_url_params?: string
      /**
       * Schema
       * @description Schema for vertica destination
       */
      schema: string
      /**
       * SSH Tunnel Method
       * @description Whether to initiate an SSH tunnel before connecting to the database, and if so, which kind of authentication to use.
       */
      tunnel_method?: OneOf<
        [
          {
            /**
             * @description No ssh tunnel needed to connect to database
             * @constant
             * @enum {string}
             */
            tunnel_method: 'NO_TUNNEL'
          },
          {
            /**
             * @description Connect through a jump server tunnel host using username and ssh key
             * @constant
             * @enum {string}
             */
            tunnel_method: 'SSH_KEY_AUTH'
            /**
             * SSH Tunnel Jump Server Host
             * @description Hostname of the jump server host that allows inbound ssh tunnel.
             */
            tunnel_host: string
            /**
             * SSH Connection Port
             * @description Port on the proxy/jump server that accepts inbound ssh connections.
             * @default 22
             */
            tunnel_port: number
            /**
             * SSH Login Username
             * @description OS-level username for logging into the jump server host.
             */
            tunnel_user: string
            /**
             * SSH Private Key
             * @description OS-level user account ssh key credentials in RSA PEM format ( created with ssh-keygen -t rsa -m PEM -f myuser_rsa )
             */
            ssh_key: string
          },
          {
            /**
             * @description Connect through a jump server tunnel host using username and password authentication
             * @constant
             * @enum {string}
             */
            tunnel_method: 'SSH_PASSWORD_AUTH'
            /**
             * SSH Tunnel Jump Server Host
             * @description Hostname of the jump server host that allows inbound ssh tunnel.
             */
            tunnel_host: string
            /**
             * SSH Connection Port
             * @description Port on the proxy/jump server that accepts inbound ssh connections.
             * @default 22
             */
            tunnel_port: number
            /**
             * SSH Login Username
             * @description OS-level username for logging into the jump server host
             */
            tunnel_user: string
            /**
             * Password
             * @description OS-level password for logging into the jump server host
             */
            tunnel_user_password: string
          },
        ]
      >
      /**
       * vertica
       * @constant
       * @enum {string}
       */
      destinationType: 'vertica'
    }
    /** Vertica Destination Spec */
    'destination-vertica-update': {
      /**
       * Host
       * @description Hostname of the database.
       */
      host: string
      /**
       * Port
       * @description Port of the database.
       * @default 5433
       */
      port: number
      /**
       * DB Name
       * @description Name of the database.
       */
      database: string
      /**
       * User
       * @description Username to use to access the database.
       */
      username: string
      /**
       * Password
       * @description Password associated with the username.
       */
      password?: string
      /**
       * JDBC URL Params
       * @description Additional properties to pass to the JDBC URL string when connecting to the database formatted as 'key=value' pairs separated by the symbol '&'. (example: key1=value1&key2=value2&key3=value3).
       */
      jdbc_url_params?: string
      /**
       * Schema
       * @description Schema for vertica destination
       */
      schema: string
      /**
       * SSH Tunnel Method
       * @description Whether to initiate an SSH tunnel before connecting to the database, and if so, which kind of authentication to use.
       */
      tunnel_method?: OneOf<
        [
          {
            /**
             * @description No ssh tunnel needed to connect to database
             * @constant
             * @enum {string}
             */
            tunnel_method: 'NO_TUNNEL'
          },
          {
            /**
             * @description Connect through a jump server tunnel host using username and ssh key
             * @constant
             * @enum {string}
             */
            tunnel_method: 'SSH_KEY_AUTH'
            /**
             * SSH Tunnel Jump Server Host
             * @description Hostname of the jump server host that allows inbound ssh tunnel.
             */
            tunnel_host: string
            /**
             * SSH Connection Port
             * @description Port on the proxy/jump server that accepts inbound ssh connections.
             * @default 22
             */
            tunnel_port: number
            /**
             * SSH Login Username
             * @description OS-level username for logging into the jump server host.
             */
            tunnel_user: string
            /**
             * SSH Private Key
             * @description OS-level user account ssh key credentials in RSA PEM format ( created with ssh-keygen -t rsa -m PEM -f myuser_rsa )
             */
            ssh_key: string
          },
          {
            /**
             * @description Connect through a jump server tunnel host using username and password authentication
             * @constant
             * @enum {string}
             */
            tunnel_method: 'SSH_PASSWORD_AUTH'
            /**
             * SSH Tunnel Jump Server Host
             * @description Hostname of the jump server host that allows inbound ssh tunnel.
             */
            tunnel_host: string
            /**
             * SSH Connection Port
             * @description Port on the proxy/jump server that accepts inbound ssh connections.
             * @default 22
             */
            tunnel_port: number
            /**
             * SSH Login Username
             * @description OS-level username for logging into the jump server host
             */
            tunnel_user: string
            /**
             * Password
             * @description OS-level password for logging into the jump server host
             */
            tunnel_user_password: string
          },
        ]
      >
    }
    /** Elasticsearch Connection Configuration */
    'destination-elasticsearch': {
      /**
       * Server Endpoint
       * @description The full url of the Elasticsearch server
       */
      endpoint: string
      /**
       * Upsert Records
       * @description If a primary key identifier is defined in the source, an upsert will be performed using the primary key value as the elasticsearch doc id. Does not support composite primary keys.
       * @default true
       */
      upsert?: boolean
      /**
       * CA certificate
       * @description CA certificate
       */
      ca_certificate?: string
      /**
       * Authentication Method
       * @description The type of authentication to be used
       */
      authenticationMethod?: OneOf<
        [
          {
            /**
             * @constant
             * @enum {string}
             */
            method: 'secret'
            /**
             * API Key ID
             * @description The Key ID to used when accessing an enterprise Elasticsearch instance.
             */
            apiKeyId: string
            /**
             * API Key Secret
             * @description The secret associated with the API Key ID.
             */
            apiKeySecret: string
          },
          {
            /**
             * @constant
             * @enum {string}
             */
            method: 'basic'
            /**
             * Username
             * @description Basic auth username to access a secure Elasticsearch server
             */
            username: string
            /**
             * Password
             * @description Basic auth password to access a secure Elasticsearch server
             */
            password: string
          },
        ]
      >
      /**
       * elasticsearch
       * @constant
       * @enum {string}
       */
      destinationType: 'elasticsearch'
    }
    /** Elasticsearch Connection Configuration */
    'destination-elasticsearch-update': {
      /**
       * Server Endpoint
       * @description The full url of the Elasticsearch server
       */
      endpoint: string
      /**
       * Upsert Records
       * @description If a primary key identifier is defined in the source, an upsert will be performed using the primary key value as the elasticsearch doc id. Does not support composite primary keys.
       * @default true
       */
      upsert?: boolean
      /**
       * CA certificate
       * @description CA certificate
       */
      ca_certificate?: string
      /**
       * Authentication Method
       * @description The type of authentication to be used
       */
      authenticationMethod?: OneOf<
        [
          {
            /**
             * @constant
             * @enum {string}
             */
            method: 'secret'
            /**
             * API Key ID
             * @description The Key ID to used when accessing an enterprise Elasticsearch instance.
             */
            apiKeyId: string
            /**
             * API Key Secret
             * @description The secret associated with the API Key ID.
             */
            apiKeySecret: string
          },
          {
            /**
             * @constant
             * @enum {string}
             */
            method: 'basic'
            /**
             * Username
             * @description Basic auth username to access a secure Elasticsearch server
             */
            username: string
            /**
             * Password
             * @description Basic auth password to access a secure Elasticsearch server
             */
            password: string
          },
        ]
      >
    }
    /** AzureBlobStorage Destination Spec */
    'destination-azure-blob-storage': {
      /**
       * Endpoint Domain Name
       * @description This is Azure Blob Storage endpoint domain name. Leave default value (or leave it empty if run container from command line) to use Microsoft native from example.
       * @default blob.core.windows.net
       */
      azure_blob_storage_endpoint_domain_name?: string
      /**
       * Azure blob storage container (Bucket) Name
       * @description The name of the Azure blob storage container. If not exists - will be created automatically. May be empty, then will be created automatically airbytecontainer+timestamp
       */
      azure_blob_storage_container_name?: string
      /**
       * Azure Blob Storage account name
       * @description The account's name of the Azure Blob Storage.
       */
      azure_blob_storage_account_name: string
      /**
       * Azure Blob Storage account key
       * @description The Azure blob storage account key.
       */
      azure_blob_storage_account_key: string
      /**
       * Azure Blob Storage output buffer size (Megabytes)
       * @description The amount of megabytes to buffer for the output stream to Azure. This will impact memory footprint on workers, but may need adjustment for performance and appropriate block size in Azure.
       * @default 5
       */
      azure_blob_storage_output_buffer_size?: number
      /**
       * Azure Blob Storage file spill size
       * @description The amount of megabytes after which the connector should spill the records in a new blob object. Make sure to configure size greater than individual records. Enter 0 if not applicable
       * @default 500
       */
      azure_blob_storage_spill_size?: number
      /**
       * Output Format
       * @description Output data format
       */
      format: OneOf<
        [
          {
            /**
             * @constant
             * @enum {string}
             */
            format_type: 'CSV'
            /**
             * Normalization (Flattening)
             * @description Whether the input json data should be normalized (flattened) in the output CSV. Please refer to docs for details.
             * @default No flattening
             * @enum {string}
             */
            flattening: 'No flattening' | 'Root level flattening'
          },
          {
            /**
             * @constant
             * @enum {string}
             */
            format_type: 'JSONL'
          },
        ]
      >
      /**
       * azure-blob-storage
       * @constant
       * @enum {string}
       */
      destinationType: 'azure-blob-storage'
    }
    /** AzureBlobStorage Destination Spec */
    'destination-azure-blob-storage-update': {
      /**
       * Endpoint Domain Name
       * @description This is Azure Blob Storage endpoint domain name. Leave default value (or leave it empty if run container from command line) to use Microsoft native from example.
       * @default blob.core.windows.net
       */
      azure_blob_storage_endpoint_domain_name?: string
      /**
       * Azure blob storage container (Bucket) Name
       * @description The name of the Azure blob storage container. If not exists - will be created automatically. May be empty, then will be created automatically airbytecontainer+timestamp
       */
      azure_blob_storage_container_name?: string
      /**
       * Azure Blob Storage account name
       * @description The account's name of the Azure Blob Storage.
       */
      azure_blob_storage_account_name: string
      /**
       * Azure Blob Storage account key
       * @description The Azure blob storage account key.
       */
      azure_blob_storage_account_key: string
      /**
       * Azure Blob Storage output buffer size (Megabytes)
       * @description The amount of megabytes to buffer for the output stream to Azure. This will impact memory footprint on workers, but may need adjustment for performance and appropriate block size in Azure.
       * @default 5
       */
      azure_blob_storage_output_buffer_size?: number
      /**
       * Azure Blob Storage file spill size
       * @description The amount of megabytes after which the connector should spill the records in a new blob object. Make sure to configure size greater than individual records. Enter 0 if not applicable
       * @default 500
       */
      azure_blob_storage_spill_size?: number
      /**
       * Output Format
       * @description Output data format
       */
      format: OneOf<
        [
          {
            /**
             * @constant
             * @enum {string}
             */
            format_type: 'CSV'
            /**
             * Normalization (Flattening)
             * @description Whether the input json data should be normalized (flattened) in the output CSV. Please refer to docs for details.
             * @default No flattening
             * @enum {string}
             */
            flattening: 'No flattening' | 'Root level flattening'
          },
          {
            /**
             * @constant
             * @enum {string}
             */
            format_type: 'JSONL'
          },
        ]
      >
    }
    /** Langchain Destination Config */
    'destination-langchain': {
      /** ProcessingConfigModel */
      processing: {
        /**
         * Chunk size
         * @description Size of chunks in tokens to store in vector store (make sure it is not too big for the context if your LLM)
         */
        chunk_size: number
        /**
         * Chunk overlap
         * @description Size of overlap between chunks in tokens to store in vector store to better capture relevant context
         * @default 0
         */
        chunk_overlap?: number
        /**
         * Text fields to embed
         * @description List of fields in the record that should be used to calculate the embedding. All other fields are passed along as meta fields. The field list is applied to all streams in the same way and non-existing fields are ignored. If none are defined, all fields are considered text fields. When specifying text fields, you can access nested fields in the record by using dot notation, e.g. `user.name` will access the `name` field in the `user` object. It's also possible to use wildcards to access all fields in an object, e.g. `users.*.name` will access all `names` fields in all entries of the `users` array.
         */
        text_fields: string[]
      }
      /**
       * Embedding
       * @description Embedding configuration
       */
      embedding: OneOf<
        [
          {
            /**
             * Mode
             * @default openai
             * @constant
             * @enum {string}
             */
            mode?: 'openai'
            /** OpenAI API key */
            openai_key: string
          },
          {
            /**
             * Mode
             * @default fake
             * @constant
             * @enum {string}
             */
            mode?: 'fake'
          },
        ]
      >
      /**
       * Indexing
       * @description Indexing configuration
       */
      indexing: OneOf<
        [
          {
            /**
             * Mode
             * @default pinecone
             * @constant
             * @enum {string}
             */
            mode?: 'pinecone'
            /** Pinecone API key */
            pinecone_key: string
            /**
             * Pinecone environment
             * @description Pinecone environment to use
             */
            pinecone_environment: string
            /**
             * Index
             * @description Pinecone index to use
             */
            index: string
          },
          {
            /**
             * Mode
             * @default DocArrayHnswSearch
             * @constant
             * @enum {string}
             */
            mode?: 'DocArrayHnswSearch'
            /**
             * Destination Path
             * @description Path to the directory where hnswlib and meta data files will be written. The files will be placed inside that local mount. All files in the specified destination directory will be deleted on each run.
             */
            destination_path: string
          },
          {
            /**
             * Mode
             * @default chroma_local
             * @constant
             * @enum {string}
             */
            mode?: 'chroma_local'
            /**
             * Destination Path
             * @description Path to the directory where chroma files will be written. The files will be placed inside that local mount.
             */
            destination_path: string
            /**
             * Collection Name
             * @description Name of the collection to use.
             * @default langchain
             */
            collection_name?: string
          },
        ]
      >
      /**
       * langchain
       * @constant
       * @enum {string}
       */
      destinationType: 'langchain'
    }
    /** Langchain Destination Config */
    'destination-langchain-update': {
      /** ProcessingConfigModel */
      processing: {
        /**
         * Chunk size
         * @description Size of chunks in tokens to store in vector store (make sure it is not too big for the context if your LLM)
         */
        chunk_size: number
        /**
         * Chunk overlap
         * @description Size of overlap between chunks in tokens to store in vector store to better capture relevant context
         * @default 0
         */
        chunk_overlap?: number
        /**
         * Text fields to embed
         * @description List of fields in the record that should be used to calculate the embedding. All other fields are passed along as meta fields. The field list is applied to all streams in the same way and non-existing fields are ignored. If none are defined, all fields are considered text fields. When specifying text fields, you can access nested fields in the record by using dot notation, e.g. `user.name` will access the `name` field in the `user` object. It's also possible to use wildcards to access all fields in an object, e.g. `users.*.name` will access all `names` fields in all entries of the `users` array.
         */
        text_fields: string[]
      }
      /**
       * Embedding
       * @description Embedding configuration
       */
      embedding: OneOf<
        [
          {
            /**
             * Mode
             * @default openai
             * @constant
             * @enum {string}
             */
            mode?: 'openai'
            /** OpenAI API key */
            openai_key: string
          },
          {
            /**
             * Mode
             * @default fake
             * @constant
             * @enum {string}
             */
            mode?: 'fake'
          },
        ]
      >
      /**
       * Indexing
       * @description Indexing configuration
       */
      indexing: OneOf<
        [
          {
            /**
             * Mode
             * @default pinecone
             * @constant
             * @enum {string}
             */
            mode?: 'pinecone'
            /** Pinecone API key */
            pinecone_key: string
            /**
             * Pinecone environment
             * @description Pinecone environment to use
             */
            pinecone_environment: string
            /**
             * Index
             * @description Pinecone index to use
             */
            index: string
          },
          {
            /**
             * Mode
             * @default DocArrayHnswSearch
             * @constant
             * @enum {string}
             */
            mode?: 'DocArrayHnswSearch'
            /**
             * Destination Path
             * @description Path to the directory where hnswlib and meta data files will be written. The files will be placed inside that local mount. All files in the specified destination directory will be deleted on each run.
             */
            destination_path: string
          },
          {
            /**
             * Mode
             * @default chroma_local
             * @constant
             * @enum {string}
             */
            mode?: 'chroma_local'
            /**
             * Destination Path
             * @description Path to the directory where chroma files will be written. The files will be placed inside that local mount.
             */
            destination_path: string
            /**
             * Collection Name
             * @description Name of the collection to use.
             * @default langchain
             */
            collection_name?: string
          },
        ]
      >
    }
    /** Destination Cumulio */
    'destination-cumulio': {
      /**
       * Cumul.io API Host URL
       * @description URL of the Cumul.io API (e.g. 'https://api.cumul.io', 'https://api.us.cumul.io', or VPC-specific API url). Defaults to 'https://api.cumul.io'.
       * @default https://api.cumul.io
       */
      api_host: string
      /**
       * Cumul.io API Key
       * @description An API key generated in Cumul.io's platform (can be generated here: https://app.cumul.io/start/profile/integration).
       */
      api_key: string
      /**
       * Cumul.io API Token
       * @description The corresponding API token generated in Cumul.io's platform (can be generated here: https://app.cumul.io/start/profile/integration).
       */
      api_token: string
      /**
       * cumulio
       * @constant
       * @enum {string}
       */
      destinationType: 'cumulio'
    }
    /** Destination Cumulio */
    'destination-cumulio-update': {
      /**
       * Cumul.io API Host URL
       * @description URL of the Cumul.io API (e.g. 'https://api.cumul.io', 'https://api.us.cumul.io', or VPC-specific API url). Defaults to 'https://api.cumul.io'.
       * @default https://api.cumul.io
       */
      api_host: string
      /**
       * Cumul.io API Key
       * @description An API key generated in Cumul.io's platform (can be generated here: https://app.cumul.io/start/profile/integration).
       */
      api_key: string
      /**
       * Cumul.io API Token
       * @description The corresponding API token generated in Cumul.io's platform (can be generated here: https://app.cumul.io/start/profile/integration).
       */
      api_token: string
    }
    /** Postgres Destination Spec */
    'destination-postgres': {
      /**
       * Host
       * @description Hostname of the database.
       */
      host: string
      /**
       * Port
       * @description Port of the database.
       * @default 5432
       */
      port: number
      /**
       * DB Name
       * @description Name of the database.
       */
      database: string
      /**
       * Default Schema
       * @description The default schema tables are written to if the source does not specify a namespace. The usual value for this field is "public".
       * @default public
       */
      schema: string
      /**
       * User
       * @description Username to use to access the database.
       */
      username: string
      /**
       * Password
       * @description Password associated with the username.
       */
      password?: string
      /**
       * SSL modes
       * @description SSL connection modes.
       *  <b>disable</b> - Chose this mode to disable encryption of communication between Airbyte and destination database
       *  <b>allow</b> - Chose this mode to enable encryption only when required by the source database
       *  <b>prefer</b> - Chose this mode to allow unencrypted connection only if the source database does not support encryption
       *  <b>require</b> - Chose this mode to always require encryption. If the source database server does not support encryption, connection will fail
       *   <b>verify-ca</b> - Chose this mode to always require encryption and to verify that the source database server has a valid SSL certificate
       *   <b>verify-full</b> - This is the most secure mode. Chose this mode to always require encryption and to verify the identity of the source database server
       *  See more information - <a href="https://jdbc.postgresql.org/documentation/head/ssl-client.html"> in the docs</a>.
       */
      ssl_mode?:
        | {
            /**
             * @default disable
             * @constant
             * @enum {string}
             */
            mode: 'disable'
          }
        | {
            /**
             * @default allow
             * @constant
             * @enum {string}
             */
            mode: 'allow'
          }
        | {
            /**
             * @default prefer
             * @constant
             * @enum {string}
             */
            mode: 'prefer'
          }
        | {
            /**
             * @default require
             * @constant
             * @enum {string}
             */
            mode: 'require'
          }
        | {
            /**
             * @default verify-ca
             * @constant
             * @enum {string}
             */
            mode: 'verify-ca'
            /**
             * CA certificate
             * @description CA certificate
             */
            ca_certificate: string
            /**
             * Client key password
             * @description Password for keystorage. This field is optional. If you do not add it - the password will be generated automatically.
             */
            client_key_password?: string
          }
        | {
            /**
             * @default verify-full
             * @constant
             * @enum {string}
             */
            mode: 'verify-full'
            /**
             * CA certificate
             * @description CA certificate
             */
            ca_certificate: string
            /**
             * Client certificate
             * @description Client certificate
             */
            client_certificate: string
            /**
             * Client key
             * @description Client key
             */
            client_key: string
            /**
             * Client key password
             * @description Password for keystorage. This field is optional. If you do not add it - the password will be generated automatically.
             */
            client_key_password?: string
          }
      /**
       * JDBC URL Params
       * @description Additional properties to pass to the JDBC URL string when connecting to the database formatted as 'key=value' pairs separated by the symbol '&'. (example: key1=value1&key2=value2&key3=value3).
       */
      jdbc_url_params?: string
      /**
       * Raw table schema (defaults to airbyte_internal)
       * @description The schema to write raw tables into
       */
      raw_data_schema?: string
      /**
       * Disable Final Tables. (WARNING! Unstable option; Columns in raw table schema might change between versions)
       * @description Disable Writing Final Tables. WARNING! The data format in _airbyte_data is likely stable but there are no guarantees that other metadata columns will remain the same in future versions
       * @default false
       */
      disable_type_dedupe?: boolean
      /**
       * SSH Tunnel Method
       * @description Whether to initiate an SSH tunnel before connecting to the database, and if so, which kind of authentication to use.
       */
      tunnel_method?: OneOf<
        [
          {
            /**
             * @description No ssh tunnel needed to connect to database
             * @constant
             * @enum {string}
             */
            tunnel_method: 'NO_TUNNEL'
          },
          {
            /**
             * @description Connect through a jump server tunnel host using username and ssh key
             * @constant
             * @enum {string}
             */
            tunnel_method: 'SSH_KEY_AUTH'
            /**
             * SSH Tunnel Jump Server Host
             * @description Hostname of the jump server host that allows inbound ssh tunnel.
             */
            tunnel_host: string
            /**
             * SSH Connection Port
             * @description Port on the proxy/jump server that accepts inbound ssh connections.
             * @default 22
             */
            tunnel_port: number
            /**
             * SSH Login Username
             * @description OS-level username for logging into the jump server host.
             */
            tunnel_user: string
            /**
             * SSH Private Key
             * @description OS-level user account ssh key credentials in RSA PEM format ( created with ssh-keygen -t rsa -m PEM -f myuser_rsa )
             */
            ssh_key: string
          },
          {
            /**
             * @description Connect through a jump server tunnel host using username and password authentication
             * @constant
             * @enum {string}
             */
            tunnel_method: 'SSH_PASSWORD_AUTH'
            /**
             * SSH Tunnel Jump Server Host
             * @description Hostname of the jump server host that allows inbound ssh tunnel.
             */
            tunnel_host: string
            /**
             * SSH Connection Port
             * @description Port on the proxy/jump server that accepts inbound ssh connections.
             * @default 22
             */
            tunnel_port: number
            /**
             * SSH Login Username
             * @description OS-level username for logging into the jump server host
             */
            tunnel_user: string
            /**
             * Password
             * @description OS-level password for logging into the jump server host
             */
            tunnel_user_password: string
          },
        ]
      >
      /**
       * postgres
       * @constant
       * @enum {string}
       */
      destinationType: 'postgres'
    }
    /** Postgres Destination Spec */
    'destination-postgres-update': {
      /**
       * Host
       * @description Hostname of the database.
       */
      host: string
      /**
       * Port
       * @description Port of the database.
       * @default 5432
       */
      port: number
      /**
       * DB Name
       * @description Name of the database.
       */
      database: string
      /**
       * Default Schema
       * @description The default schema tables are written to if the source does not specify a namespace. The usual value for this field is "public".
       * @default public
       */
      schema: string
      /**
       * User
       * @description Username to use to access the database.
       */
      username: string
      /**
       * Password
       * @description Password associated with the username.
       */
      password?: string
      /**
       * SSL modes
       * @description SSL connection modes.
       *  <b>disable</b> - Chose this mode to disable encryption of communication between Airbyte and destination database
       *  <b>allow</b> - Chose this mode to enable encryption only when required by the source database
       *  <b>prefer</b> - Chose this mode to allow unencrypted connection only if the source database does not support encryption
       *  <b>require</b> - Chose this mode to always require encryption. If the source database server does not support encryption, connection will fail
       *   <b>verify-ca</b> - Chose this mode to always require encryption and to verify that the source database server has a valid SSL certificate
       *   <b>verify-full</b> - This is the most secure mode. Chose this mode to always require encryption and to verify the identity of the source database server
       *  See more information - <a href="https://jdbc.postgresql.org/documentation/head/ssl-client.html"> in the docs</a>.
       */
      ssl_mode?:
        | {
            /**
             * @default disable
             * @constant
             * @enum {string}
             */
            mode: 'disable'
          }
        | {
            /**
             * @default allow
             * @constant
             * @enum {string}
             */
            mode: 'allow'
          }
        | {
            /**
             * @default prefer
             * @constant
             * @enum {string}
             */
            mode: 'prefer'
          }
        | {
            /**
             * @default require
             * @constant
             * @enum {string}
             */
            mode: 'require'
          }
        | {
            /**
             * @default verify-ca
             * @constant
             * @enum {string}
             */
            mode: 'verify-ca'
            /**
             * CA certificate
             * @description CA certificate
             */
            ca_certificate: string
            /**
             * Client key password
             * @description Password for keystorage. This field is optional. If you do not add it - the password will be generated automatically.
             */
            client_key_password?: string
          }
        | {
            /**
             * @default verify-full
             * @constant
             * @enum {string}
             */
            mode: 'verify-full'
            /**
             * CA certificate
             * @description CA certificate
             */
            ca_certificate: string
            /**
             * Client certificate
             * @description Client certificate
             */
            client_certificate: string
            /**
             * Client key
             * @description Client key
             */
            client_key: string
            /**
             * Client key password
             * @description Password for keystorage. This field is optional. If you do not add it - the password will be generated automatically.
             */
            client_key_password?: string
          }
      /**
       * JDBC URL Params
       * @description Additional properties to pass to the JDBC URL string when connecting to the database formatted as 'key=value' pairs separated by the symbol '&'. (example: key1=value1&key2=value2&key3=value3).
       */
      jdbc_url_params?: string
      /**
       * Raw table schema (defaults to airbyte_internal)
       * @description The schema to write raw tables into
       */
      raw_data_schema?: string
      /**
       * Disable Final Tables. (WARNING! Unstable option; Columns in raw table schema might change between versions)
       * @description Disable Writing Final Tables. WARNING! The data format in _airbyte_data is likely stable but there are no guarantees that other metadata columns will remain the same in future versions
       * @default false
       */
      disable_type_dedupe?: boolean
      /**
       * SSH Tunnel Method
       * @description Whether to initiate an SSH tunnel before connecting to the database, and if so, which kind of authentication to use.
       */
      tunnel_method?: OneOf<
        [
          {
            /**
             * @description No ssh tunnel needed to connect to database
             * @constant
             * @enum {string}
             */
            tunnel_method: 'NO_TUNNEL'
          },
          {
            /**
             * @description Connect through a jump server tunnel host using username and ssh key
             * @constant
             * @enum {string}
             */
            tunnel_method: 'SSH_KEY_AUTH'
            /**
             * SSH Tunnel Jump Server Host
             * @description Hostname of the jump server host that allows inbound ssh tunnel.
             */
            tunnel_host: string
            /**
             * SSH Connection Port
             * @description Port on the proxy/jump server that accepts inbound ssh connections.
             * @default 22
             */
            tunnel_port: number
            /**
             * SSH Login Username
             * @description OS-level username for logging into the jump server host.
             */
            tunnel_user: string
            /**
             * SSH Private Key
             * @description OS-level user account ssh key credentials in RSA PEM format ( created with ssh-keygen -t rsa -m PEM -f myuser_rsa )
             */
            ssh_key: string
          },
          {
            /**
             * @description Connect through a jump server tunnel host using username and password authentication
             * @constant
             * @enum {string}
             */
            tunnel_method: 'SSH_PASSWORD_AUTH'
            /**
             * SSH Tunnel Jump Server Host
             * @description Hostname of the jump server host that allows inbound ssh tunnel.
             */
            tunnel_host: string
            /**
             * SSH Connection Port
             * @description Port on the proxy/jump server that accepts inbound ssh connections.
             * @default 22
             */
            tunnel_port: number
            /**
             * SSH Login Username
             * @description OS-level username for logging into the jump server host
             */
            tunnel_user: string
            /**
             * Password
             * @description OS-level password for logging into the jump server host
             */
            tunnel_user_password: string
          },
        ]
      >
    }
    /**
     * @description The values required to configure the destination.
     * @example {
     *   "user": "charles"
     * }
     */
    DestinationConfiguration:
      | components['schemas']['destination-google-sheets']
      | components['schemas']['destination-astra']
      | components['schemas']['destination-aws-datalake']
      | components['schemas']['destination-azure-blob-storage']
      | components['schemas']['destination-bigquery']
      | components['schemas']['destination-clickhouse']
      | components['schemas']['destination-convex']
      | components['schemas']['destination-cumulio']
      | components['schemas']['destination-databend']
      | components['schemas']['destination-databricks']
      | components['schemas']['destination-dev-null']
      | components['schemas']['destination-duckdb']
      | components['schemas']['destination-dynamodb']
      | components['schemas']['destination-elasticsearch']
      | components['schemas']['destination-firebolt']
      | components['schemas']['destination-firestore']
      | components['schemas']['destination-gcs']
      | components['schemas']['destination-keen']
      | components['schemas']['destination-kinesis']
      | components['schemas']['destination-langchain']
      | components['schemas']['destination-milvus']
      | components['schemas']['destination-mongodb']
      | components['schemas']['destination-mssql']
      | components['schemas']['destination-mysql']
      | components['schemas']['destination-oracle']
      | components['schemas']['destination-pinecone']
      | components['schemas']['destination-postgres']
      | components['schemas']['destination-pubsub']
      | components['schemas']['destination-qdrant']
      | components['schemas']['destination-redis']
      | components['schemas']['destination-redshift']
      | components['schemas']['destination-s3']
      | components['schemas']['destination-s3-glue']
      | components['schemas']['destination-sftp-json']
      | components['schemas']['destination-snowflake']
      | components['schemas']['destination-teradata']
      | components['schemas']['destination-timeplus']
      | components['schemas']['destination-typesense']
      | components['schemas']['destination-vectara']
      | components['schemas']['destination-vertica']
      | components['schemas']['destination-weaviate']
      | components['schemas']['destination-xata']
    /**
     * @description The values required to configure the source.
     * @example {
     *   "user": "charles"
     * }
     */
    SourceConfiguration: unknown
  }
  responses: {
    /** @description Response from the initiate OAuth call should be an object with a single property which will be the `redirect_url`. If a user is redirected to this URL, they'll be prompted by the identity provider to authenticate. */
    InitiateOauthResponse: {
      content: {
        'application/json': unknown
      }
    }
  }
  parameters: never
  requestBodies: never
  headers: never
  pathItems: never
}

export type $defs = Record<string, never>

export type external = Record<string, never>

export interface operations {
  /** List destinations */
  listDestinations: {
    parameters: {
      query?: {
        /** @description The UUIDs of the workspaces you wish to list destinations for. Empty list will retrieve all allowed workspaces. */
        workspaceIds?: string[]
        /** @description Include deleted destinations in the returned results. */
        includeDeleted?: boolean
        /** @description Set the limit on the number of destinations returned. The default is 20. */
        limit?: number
        /** @description Set the offset to start at when returning destinations. The default is 0 */
        offset?: number
      }
    }
    responses: {
      /** @description Successful operation */
      200: {
        content: {
          'application/json': components['schemas']['DestinationsResponse']
        }
      }
      /** @description Not allowed */
      403: {
        content: never
      }
      /** @description Not found */
      404: {
        content: never
      }
    }
  }
  /**
   * Create a destination
   * @description Creates a destination given a name, workspace id, and a json blob containing the configuration for the source.
   */
  createDestination: {
    requestBody?: {
      content: {
        'application/json': components['schemas']['DestinationCreateRequest']
      }
    }
    responses: {
      /** @description Successful operation */
      200: {
        content: {
          'application/json': components['schemas']['DestinationResponse']
        }
      }
      /** @description Invalid data */
      400: {
        content: never
      }
      /** @description Not allowed */
      403: {
        content: never
      }
      /** @description Not found */
      404: {
        content: never
      }
    }
  }
  /** Get Destination details */
  getDestination: {
    parameters: {
      path: {
        destinationId: string
      }
    }
    responses: {
      /** @description Get a Destination by the id in the path. */
      200: {
        content: {
          'application/json': components['schemas']['DestinationResponse']
        }
      }
      /** @description Not allowed */
      403: {
        content: never
      }
      /** @description Not found */
      404: {
        content: never
      }
    }
  }
  /** Update a Destination and fully overwrite it */
  putDestination: {
    parameters: {
      path: {
        destinationId: string
      }
    }
    requestBody?: {
      content: {
        'application/json': components['schemas']['DestinationPutRequest']
      }
    }
    responses: {
      /** @description Update a Destination and fully overwrite it */
      200: {
        content: {
          'application/json': components['schemas']['DestinationResponse']
        }
      }
      /** @description Not allowed */
      403: {
        content: never
      }
      /** @description Not found */
      404: {
        content: never
      }
    }
  }
  /** Delete a Destination */
  deleteDestination: {
    parameters: {
      path: {
        destinationId: string
      }
    }
    responses: {
      /** @description The resource was deleted successfully */
      204: {
        content: never
      }
      /** @description Not allowed */
      403: {
        content: never
      }
      /** @description Not found */
      404: {
        content: never
      }
    }
  }
  /** Update a Destination */
  patchDestination: {
    parameters: {
      path: {
        destinationId: string
      }
    }
    requestBody?: {
      content: {
        'application/json': components['schemas']['DestinationPatchRequest']
      }
    }
    responses: {
      /** @description Update a Destination */
      200: {
        content: {
          'application/json': components['schemas']['DestinationResponse']
        }
      }
      /** @description Not allowed */
      403: {
        content: never
      }
      /** @description Not found */
      404: {
        content: never
      }
    }
  }
}

export interface oasTypes {
  components: components
  external: external
  operations: operations
  paths: paths
  webhooks: webhooks
}

export default oasTypes
